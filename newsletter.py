#!/usr/bin/env python
# coding: utf-8

# # **ë‰´ìŠ¤ë ˆí„° ì‹¤í–‰ ë°©ë²•**
# **ì¢Œì¸¡ìƒë‹¨ "ëª¨ë‘ ì‹¤í–‰" í´ë¦­**
# <br>
# ![image.png](data:image/png;base64,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)
# 

# # **01-1 ì„¤ì¹˜ & import**

# In[11]:


# ============================
# ğŸ”— GitHub ì—°ë™ ì„¤ì • (Colab ì „ìš©)
# ============================
try:
    from google.colab import userdata
    IN_COLAB = True
    print("âœ… Google Colab í™˜ê²½ ê°ì§€ë¨")
except:
    IN_COLAB = False
    print("âš ï¸ ë¡œì»¬/GitHub Actions í™˜ê²½")

if IN_COLAB:
    import os

    # Colab Secretsì—ì„œ API í‚¤ ì½ê¸°
    os.environ["GITHUB_TOKEN"] = userdata.get('GITHUB_TOKEN')
    os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
    os.environ["NEWSAPI_KEY"] = userdata.get('NEWSAPI_KEY')
    os.environ["NEWSDATA_API_KEY"] = userdata.get('NEWSDATA_API_KEY')
    os.environ["GNEWS_API_KEY"] = userdata.get('GNEWS_API_KEY')
    os.environ["MEDIASTACK_API_KEY"] = userdata.get('MEDIASTACK_API_KEY')
    os.environ["SERPAPI_KEY"] = userdata.get('SERPAPI_KEY')
    os.environ["CURRENTS_API_KEY"] = userdata.get('CURRENTS_API_KEY')
    os.environ["GMAIL_USER"] = userdata.get('GMAIL_USER')
    os.environ["GMAIL_APP_PASSWORD"] = userdata.get('GMAIL_APP_PASSWORD')
    os.environ["TO_EMAIL"] = userdata.get('TO_EMAIL')
    os.environ["TO_EMAIL_TEST"] = userdata.get('TO_EMAIL_TEST')  # ğŸ”¹ ìƒˆë¡œ ì¶”ê°€

    print("âœ… Colab Secretsì—ì„œ API í‚¤ ë¡œë“œ ì™„ë£Œ")


# # **01-2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**

# In[12]:


# ============================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# ============================
if IN_COLAB:
    get_ipython().system('pip install -q requests pandas python-dateutil openai beautifulsoup4 feedparser')

import os
import time
import json
import requests
import pandas as pd

from datetime import datetime, timedelta, timezone

import feedparser
from dateutil import parser as dateparser  # ì´ë¯¸ ìˆë‹¤ë©´ ìƒëµ

from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from html import escape as h
# IPythonì€ Colabì—ì„œë§Œ í•„ìš”
try:
    from IPython.display import HTML, display
except ImportError:
    # GitHub Actions í™˜ê²½ìš© ë”ë¯¸ í•¨ìˆ˜
    def display(obj):
        pass
    def HTML(text):
        return text
    pass
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from concurrent.futures import ThreadPoolExecutor, as_completed


# # **02-1 ì„¤ì • (API í‚¤)**

# In[13]:


# ============================================================
# 2. API í‚¤ & ê¸°ë³¸ ì„¤ì •
# ============================================================

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
NEWSAPI_KEY = os.environ.get("NEWSAPI_KEY")
NEWSDATA_API_KEY = os.environ.get("NEWSDATA_API_KEY")
GNEWS_API_KEY = os.environ.get("GNEWS_API_KEY")
MEDIASTACK_API_KEY = os.environ.get("MEDIASTACK_API_KEY")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")
CURRENTS_API_KEY = os.environ.get("CURRENTS_API_KEY")


NEWSDATA_BASE_URL_LATEST = "https://newsdata.io/api/1/latest"



# # **02-2 ì„¤ì • (ë‚ ì§œ, ì£¼ì œ, í‚¤ì›Œë“œ, ìƒìˆ˜)**

# In[14]:


# ì‚¬ìš©í•  GPT mini ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4.1-mini", ë‚˜ì¤‘ì— "gpt-5.1-mini"ë¡œ êµì²´ ê°€ëŠ¥)
MODEL_NAME = "gpt-4.1-mini"

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
client = OpenAI()

# ============================================================
# GitHub Pages ì„¤ì •
# ============================================================
# GitHub ê³„ì • ì•„ì´ë””ì™€ ë ˆí¬ ì´ë¦„
GITHUB_OWNER = "hancom-inspace"              # ì˜ˆ: "junwoo0920"
GITHUB_REPO  = "Weekly-Newsletter"    # GitHubì—ì„œ ë§Œë“  ë ˆí¬ ì´ë¦„

# GitHub Pages ìµœì¢… URL (Settings â†’ Pagesì— í‘œì‹œë˜ëŠ” ì£¼ì†Œ)
# ë³´í†µ í˜•íƒœ: https://{owner}.github.io/{repo}
BASE_URL = f"https://{GITHUB_OWNER}.github.io/{GITHUB_REPO}"

# GitHub Personal Access Token (Colab ì…€ì—ì„œ os.environ["GITHUB_TOKEN"] ë¡œ ë¯¸ë¦¬ ë„£ì–´ë‘” ê°’)
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")


# í—¤ë” ì´ë¯¸ì§€ / ë¡œê³  / ê¸°ë³¸ ì¸ë„¤ì¼
HEADER_BACKGROUND = "https://static.wixstatic.com/media/b749e1_60701dc82eb542149d95637ce7a34802~mv2_d_5568_3712_s_4_2.jpg/v1/fill/w_1553,h_1071,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/b749e1_60701dc82eb542149d95637ce7a34802~mv2_d_5568_3712_s_4_2.jpg"
LOGO_URL = "https://static.wixstatic.com/media/0c4f16_c8132c52e3ef485286a1f0ec457a5cdb~mv2.png/v1/fill/w_111,h_44,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/logotype_vert_rgb_v2__w__w500.png%201x,%20https://static.wixstatic.com/media/0c4f16_c8132c52e3ef485286a1f0ec457a5cdb~mv2.png/v1/fill/w_222,h_88,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/logotype_vert_rgb_v2__w__w500.png%202x"
DEFAULT_THUMB = "https://e0.pxfuel.com/wallpapers/597/629/desktop-wallpaper-fun-funny-funny-funny-space.jpg"

CONTENT_WIDTH = 700

# ============================
# 2. ë‚ ì§œ ìë™ ì„¤ì • (UTC ê¸°ì¤€ 7ì¼ ì „ ~ ì˜¤ëŠ˜)
# ============================
# KST ì‹œê°„ëŒ€ ì •ì˜
KST = timezone(timedelta(hours=9))

# í˜„ì¬ KST ì‹œê°„ ê¸°ì¤€
now_kst = datetime.now(KST)
today_kst = now_kst.date()

# ê²€ìƒ‰ ë²”ìœ„: KST ê¸°ì¤€ 7ì¼ ì „ 00:00:00 ~ ì˜¤ëŠ˜ 23:59:59
seven_days_ago_kst = today_kst - timedelta(days=7)

# KST ë‚ ì§œë¥¼ UTC datetimeìœ¼ë¡œ ë³€í™˜ (APIëŠ” UTC í•„ìš”)
date_from_utc = datetime.combine(seven_days_ago_kst, datetime.min.time()).replace(tzinfo=KST).astimezone(timezone.utc)
date_to_utc = datetime.combine(today_kst, datetime.max.time()).replace(tzinfo=KST).astimezone(timezone.utc)

DATE_FROM = date_from_utc.strftime("%Y-%m-%d")
DATE_TO = date_to_utc.strftime("%Y-%m-%d")

print("=" * 60)
print(f"ğŸ• í˜„ì¬ KST ì‹œê°„: {now_kst.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ“… ê²€ìƒ‰ ë²”ìœ„ (KST): {seven_days_ago_kst} ~ {today_kst}")
print(f"ğŸ“… ê²€ìƒ‰ ë²”ìœ„ (UTC): {DATE_FROM} ~ {DATE_TO}")
print("=" * 60)


# ì˜ì–´ + í•œêµ­ì–´ ê¸°ì‚¬ ê°™ì´ ì‚¬ìš©
LANGUAGES = ["en", "ko"]

# ============================
# 2. ì£¼ì œ ì •ì˜ & í‚¤ì›Œë“œ
# ============================
TOPIC_DESC = {
    1: "GeoINT / ì§€ë¦¬ê³µê°„ ì •ë³´, ìœ„ì„±Â·ì§€ë„ ê¸°ë°˜ ì •ë³´ ë¶„ì„",
    2: "í•­ê³µ / ë“œë¡ Â·ë¬´ì¸ê¸°(UAV/UAS) ë™í–¥",
    3: "AI ë°ì´í„°Â·ë¶„ì„ í”Œë«í¼ / MLOpsÂ·ìë™í™”",
    4: "ìœ„ì„± ì˜ìƒ / SARÂ·ê´‘í•™Â·í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´ ì´ë¯¸ì§€ ë¶„ì„"
}

TOPIC_ICON = {
    1: "ğŸŒ",   # GEOINT
    2: "âœˆï¸",   # í•­ê³µ
    3: "ğŸ¤–",   # AI í”Œë«í¼
    4: "ğŸ›°ï¸",   # ìœ„ì„±ì˜ìƒ
}

# ì˜ì–´ + í•œê¸€ í‚¤ì›Œë“œ í˜¼í•©
TOPIC_KEYWORDS = {
    1: [
        # ê¸°ì¡´ í‚¤ì›Œë“œ
        "geospatial intelligence", "GeoINT",
        "geospatial analytics security",
        "geospatial data national security",
        "ì§€ë¦¬ê³µê°„ ì •ë³´",
        "ìœ„ì„± ì´ë¯¸ì§€ ë¶„ì„",
        "OSINT ìœ„ì„±",
        "ìœ„ì„± ê¸°ë°˜ ì •ë³´ì „",

        # ğŸ”¹ ë²”ìœ„ë¥¼ ë„“íˆëŠ” ì¶”ê°€ í‚¤ì›Œë“œë“¤
        "geospatial analytics",
        "geospatial data",
        "geospatial mapping",
        "location intelligence",
        "mapping platform",
        "map intelligence",

        "geospatial startup",
        "geospatial company",
        "geospatial SaaS",

        # âœ… í•œê¸€ í‚¤ì›Œë“œ ëŒ€í­ í™•ì¥
        "ì§€ë¦¬ê³µê°„ ì •ë³´", "ìœ„ì„± ì´ë¯¸ì§€ ë¶„ì„", "OSINT ìœ„ì„±",
        "ê³µê°„ì •ë³´", "ì§€ë¦¬ì •ë³´ì‹œìŠ¤í…œ", "GIS í”Œë«í¼",
        "ìœ„ì¹˜ ê¸°ë°˜ ì„œë¹„ìŠ¤", "ìœ„ì„±ì˜ìƒ ë¶„ì„", "ê³µê°„ ë°ì´í„°",
        "ì§€ë„ ì„œë¹„ìŠ¤", "ìœ„ì¹˜ì •ë³´ ê¸°ìˆ ", "ê³µê°„ì •ë³´ ì‚°ì—…",
        "êµ­í† ì •ë³´", "ì§€ì ì •ë³´", "ì¸¡ëŸ‰ ê¸°ìˆ ",  # ğŸ†• ì¶”ê°€
    ],
    2: [
        "drone industry",
        "UAV UAS technology",
        "urban air mobility",
        "drone regulation",
        "ë“œë¡  ì‚°ì—…",
        "ë¬´ì¸ê¸°",
        "ë„ì‹¬ í•­ê³µ ëª¨ë¹Œë¦¬í‹°",
        "UAM ê·œì œ",
        "ë“œë¡  ë°©ì‚°",

        # ì‚°ì—…ë³„ ë“œë¡  í™œìš©
        "commercial drone",
        "enterprise drone",
        "industrial drone",
        "drone services",
        "drone inspection",
        "drone surveillance",
        "drone monitoring",

        "drone in logistics",
        "drone delivery",
        "warehouse drone",
        "last mile delivery drone",

        "drone in agriculture",
        "agricultural drone",
        "precision agriculture drone",

        "drone energy inspection",
        "power line inspection drone",
        "oil gas drone inspection",

        # âœ… í•œê¸€ ë“œë¡  í‚¤ì›Œë“œ í™•ì¥
        "ë“œë¡  ì‚°ì—…", "ë¬´ì¸ê¸°", "ë„ì‹¬ í•­ê³µ ëª¨ë¹Œë¦¬í‹°",
        "ë“œë¡  ë°°ì†¡", "ë¬¼ë¥˜ ë“œë¡ ", "ë†ì—… ë“œë¡ ",
        "ë“œë¡  ê·œì œ", "ë“œë¡  ì•ˆì „", "ë“œë¡  ê¸°ìˆ ",
        "UAM ì„œë¹„ìŠ¤", "ì—ì–´íƒì‹œ", "í•˜ëŠ˜ê¸¸",  # ğŸ†• ì¶”ê°€
        "ë“œë¡  ìŠ¤íƒ€íŠ¸ì—…", "ë“œë¡  ì‹œì¥", "ë“œë¡  ì •ì±…",  # ğŸ†• ì¶”ê°€

        # ë“œë¡  í”Œë«í¼ / ì†Œí”„íŠ¸ì›¨ì–´
        "drone software platform",
        "drone data platform",
        "UAS management",
        "fleet management drone",
        "drone analytics",

        "BVLOS drone",
        "drone autonomy",
        "autonomous drone",
        "drone AI navigation",

        "drone startup",
        "drone company",
        "UAS startup"
    ],
    3: [
        # ê¸°ì¡´ í‚¤ì›Œë“œ
        "AI data platform",
        "AI analytics platform",
        "MLOps platform",
        "data automation platform",
        "AI ë°ì´í„° í”Œë«í¼",
        "MLOps",
        "ë°ì´í„° ë¶„ì„ í”Œë«í¼",
        "AI ë¶„ì„ í”Œë«í¼",

        # ìƒˆë¡œ ì¶”ê°€í•˜ëŠ” í‚¤ì›Œë“œë“¤ (AI ê¸°ì‚¬ í­ ë„“íˆê¸°)
        "AI infrastructure",
        "AI cloud platform",
        "LLM platform",
        "large language model platform",
        "enterprise AI platform",
        "enterprise AI",
        "gen AI platform",
        "generative AI platform",
        "generative AI for business",
        "RAG platform",
        "retrieval augmented generation",
        "vector database",
        "feature store",
        "model deployment",
        "model serving",
        "LLMOps",

        # í•œêµ­ì–´ í‚¤ì›Œë“œ (êµ­ë‚´ ê¸°ì‚¬ ëŒ€ë¹„)
        "AI ì¸í”„ë¼",
        "AI í´ë¼ìš°ë“œ í”Œë«í¼",
        "LLM í”Œë«í¼",
        "ëŒ€ê·œëª¨ ì–¸ì–´ëª¨ë¸",
        "ì—”í„°í”„ë¼ì´ì¦ˆ AI",
        "ê¸°ì—…ìš© AI",
        "ìƒì„±í˜• AI í”Œë«í¼",
        "ìƒì„±í˜• AI ì¸í”„ë¼",
        "RAG í”Œë«í¼",
        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤",
        "ëª¨ë¸ ë°°í¬",
        "ëª¨ë¸ ì„œë¹™",
        "LLM ìš´ì˜",

         # ì¢€ ë” ë²”ìš©ì ì¸ AI ë¹„ì¦ˆë‹ˆìŠ¤/ì—”í„°í”„ë¼ì´ì¦ˆ í‚¤ì›Œë“œ ì¶”ê°€
        "enterprise AI",
        "AI strategy",
        "AI adoption",
        "AI transformation",
        "AI in business",
        "AI for enterprises",
        "AI for industry",
        "AI automation",

        "ì¸ê³µì§€ëŠ¥ ì „ëµ",
        "ê¸°ì—… AI ë„ì…",
        "AI ì—…ë¬´ ìë™í™”",
        "ì—”í„°í”„ë¼ì´ì¦ˆ AI ë„ì…"

        # âœ… í•œê¸€ AI í‚¤ì›Œë“œ í™•ì¥
        "AI ë°ì´í„° í”Œë«í¼", "MLOps", "ì¸ê³µì§€ëŠ¥ ì¸í”„ë¼",
        "ê¸°ì—… AI", "ìƒì„±í˜• AI", "AI ë„ì…",
        "AI í´ë¼ìš°ë“œ", "LLM í”Œë«í¼", "AI ì „ëµ",
        "AI í˜ì‹ ", "ë””ì§€í„¸ ì „í™˜", "AI ìë™í™”",  # ğŸ†• ì¶”ê°€
        "ê¸°ì—…ìš© AI", "ì‚°ì—… AI", "AI ì„œë¹„ìŠ¤",  # ğŸ†• ì¶”ê°€
    ],

    4: [
        "satellite imagery processing",
        "SAR satellite analytics",
        "optical satellite imagery analysis",
        "hyperspectral satellite data",
        "ìœ„ì„± ì˜ìƒ",
        "SAR ìœ„ì„±",
        "ê´‘í•™ ìœ„ì„± ì˜ìƒ",
        "í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´ ì˜ìƒ",
        "ìœ„ì„± ì´ë¯¸ì§€ ë¶„ì„",

        # ìœ„ì„± ë°ì´í„° ì‚°ì—… / ì„œë¹„ìŠ¤
        "satellite data platform",
        "satellite data analytics",
        "earth observation data",
        "EO data platform",
        "space data analytics",

        "satellite imagery services",
        "commercial satellite imagery",
        "satellite data services",


        # ì‚°ì—…ë³„ ìœ„ì„± í™œìš©
        "satellite data for agriculture",
        "satellite data for insurance",
        "satellite data for climate",
        "satellite data for energy",
        "satellite data for supply chain",

        # âœ… í•œê¸€ ìœ„ì„± í‚¤ì›Œë“œ í™•ì¥
        "ìœ„ì„± ì˜ìƒ", "SAR ìœ„ì„±", "ê´‘í•™ ìœ„ì„± ì˜ìƒ",
        "ìœ„ì„± ë°ì´í„°", "ì§€êµ¬ê´€ì¸¡", "ìœ„ì„±ì •ë³´",
        "ìœ„ì„±ì˜ìƒ ë¶„ì„", "ì›ê²©íƒì‚¬", "ìš°ì£¼ ì‚°ì—…",  # ğŸ†• ì¶”ê°€
        "ìœ„ì„± ì„œë¹„ìŠ¤", "ìœ„ì„± ê¸°ìˆ ", "ìœ„ì„± í™œìš©",  # ğŸ†• ì¶”ê°€
    ]
}

# 1ì°¨ í›„ë³´ ê°œìˆ˜ (í† í”½ë‹¹ NewsAPIì—ì„œ ë„‰ë„‰íˆ ê°€ì ¸ì˜¤ê¸°)
ARTICLES_PER_TOPIC_RAW = 100

# ì–¸ì–´ë³„ ëª©í‘œ ê°œìˆ˜ (í•œê¸€ 30% : ì˜ì–´ 70%)
ARTICLES_PER_LANG_KO = 30   # í•œê¸€: 30%
ARTICLES_PER_LANG_EN = 70   # ì˜ì–´: 70%

# ìµœì¢… ë‰´ìŠ¤ë ˆí„°ì— ë°˜ë“œì‹œ ë³´ì—¬ì¤„ ê°œìˆ˜
ARTICLES_PER_TOPIC_FINAL = 3

# [ì¶”ê°€] í•œ í† í”½ì—ì„œ "ë©”ì¸ 3 + ì¶”ê°€ ìµœì†Œ 6"ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì „ì²´ ìµœì†Œ ê°œìˆ˜
MIN_TOTAL_PER_TOPIC = ARTICLES_PER_TOPIC_FINAL + 6  # 3 + 6 = 9


# # **03 NewsAPIë¡œ ê¸°ì‚¬ ìˆ˜ì§‘**

# In[15]:


# ============================
# 3. NewsAPIì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
# ============================
def search_news_newsapi(query, from_date, to_date, language=None, page_size=50):
    """
    NewsAPI (2ìˆœìœ„ ë³´ì¡° ì†ŒìŠ¤ 1)
    """
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "from": from_date,
        "to": to_date,
        "sortBy": "publishedAt",
        "pageSize": page_size,
        "apiKey": NEWSAPI_KEY,
    }
    if language:
        params["language"] = language

    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    return r.json().get("articles", [])


def search_news_mediastack(query, from_date, to_date, language=None, page_size=30):
    """
    MediaStack (2ìˆœìœ„ ë³´ì¡° ì†ŒìŠ¤ 2)
    - ë¬´ë£Œ í”Œëœì´ë©´ HTTPSê°€ ì•ˆë  ìˆ˜ë„ ìˆì–´ì„œ httpë¡œ ì¨ì•¼ í•  ë•Œë„ ìˆìŒ. (ë¬¸ì„œ ì°¸ê³ )
    """
    url = "http://api.mediastack.com/v1/news"
    limit = min(page_size, 50)

    params = {
        "access_key": MEDIASTACK_API_KEY,
        "keywords": query,
        "date_from": from_date,
        "date_to": to_date,
        "sort": "published_desc",
        "limit": limit,
    }
    if language:
        # mediastackëŠ” ì–¸ì–´ ì½”ë“œê°€ en, ko ë“± (ê³µì‹ ë¬¸ì„œ í™•ì¸)
        params["languages"] = language

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] MediaStack error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    articles = []
    for item in data.get("data", []):
        articles.append({
            "source": {"name": item.get("source")},
            "author": item.get("author"),
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("description"),
            "url": item.get("url"),
            "publishedAt": item.get("published_at"),  # ë˜ëŠ” published_at/created_at í™•ì¸ í•„ìš”
        })
    return articles

def search_news_serpapi(query, from_date, to_date, language=None, page_size=30):
    """
    SerpAPI Google News (3ìˆœìœ„ ë°±ì—… ì†ŒìŠ¤)
    """
    url = "https://serpapi.com/search"
    num = min(page_size, 20)

    params = {
        "engine": "google_news",
        "q": query,
        "api_key": SERPAPI_KEY,
        "num": num,
    }
    if language:
        params["hl"] = language  # hl=ko, hl=en ë“±

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] SerpAPI error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    results = data.get("news_results") or []
    articles = []
    for item in results:
        # published_dateëŠ” ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ ("3 hours ago" ì´ëŸ° ì‹ì´ë©´ ë‚˜ì¤‘ì— í•„í„°ì—ì„œ ê±¸ëŸ¬ì§ˆ ìˆ˜ ìˆìŒ)
        articles.append({
            "source": {"name": item.get("source")},
            "author": None,
            "title": item.get("title"),
            "description": item.get("snippet"),
            "content": item.get("snippet"),
            "url": item.get("link"),
            "publishedAt": item.get("date") or item.get("published_date"),
        })
    return articles


def search_news_currents(query, from_date, to_date, language=None, page_size=30):
    """
    Currents API (3ìˆœìœ„ ë°±ì—… ì†ŒìŠ¤)
    """
    url = "https://api.currentsapi.services/v1/search"
    limit = min(page_size, 50)

    params = {
        "apiKey": CURRENTS_API_KEY,
        "keywords": query,
        "limit": limit,
    }
    if language:
        params["language"] = language

    # ë‚ ì§œ í•„í„°ëŠ” í”Œëœì— ë”°ë¼ ë™ì‘ ë°©ì‹ì´ ë‹¬ë¼ì„œ, í•„ìš”ì‹œ ë¬¸ì„œ ë³´ê³  ë§ê²Œ ì¡°ì •
    params["start_date"] = from_date
    params["end_date"] = to_date

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] Currents error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    results = data.get("news") or []
    articles = []
    for item in results:
        articles.append({
            "source": {"name": item.get("author")},
            "author": item.get("author"),
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("description"),
            "url": item.get("url"),
            "publishedAt": item.get("published"),  # ISO8601ì¸ì§€ í™•ì¸ í•„ìš”
        })
    return articles


def search_news_gnews(query, from_date, to_date, language=None, page_size=30):
    """
    GNews API (1ìˆœìœ„ ë©”ì¸ ì†ŒìŠ¤)
    - ê³µì‹ ë¬¸ì„œ ë³´ê³  íŒŒë¼ë¯¸í„° ì´ë¦„/ì œí•œì€ í•„ìš”ì‹œ ì¡°ì •
    """
    url = "https://gnews.io/api/v4/search"
    max_size = min(page_size, 50)  # ë¬´ë£Œ í”Œëœì€ ë³´í†µ 10~50 ì œí•œ

    params = {
        "q": query,
        "token": GNEWS_API_KEY,
        "max": max_size,
        "from": from_date,
        "to": to_date,
        "sortby": "publishedAt",
    }
    if language:
        # GNewsëŠ” ì–¸ì–´ì½”ë“œê°€ en, ko ë“±ì„ ì§€ì› (ë¬¸ì„œ í™•ì¸í•´ì„œ ë§ì¶°ì•¼ í•¨)
        params["lang"] = language

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] GNews error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    articles = []
    for item in data.get("articles", []):
        articles.append({
            "source": {"name": item.get("source", {}).get("name")},
            "author": item.get("author"),
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("content"),
            "url": item.get("url"),
            # GNewsëŠ” publishedAtì´ ISO8601 í˜•ì‹
            "publishedAt": item.get("publishedAt"),
        })
    return articles


def search_news_newsdata(query, from_date, to_date, language=None, page_size=30):
    """
    NewsData.io 'latest' ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ë²„ì „
    - ë¬´ë£Œ í”Œëœì—ì„œë„ ë™ì‘
    - latestëŠ” ê³¼ê±° nì¼ì´ ì•„ë‹ˆë¼, ìµœê·¼ ~48ì‹œê°„ ê¸°ì¤€ì´ì§€ë§Œ,
      ìš°ë¦¬ëŠ” ì–´ì°¨í”¼ NewsAPIì—ì„œ ì¼ì£¼ì¼ì¹˜ ì»¤ë²„í•˜ê³ ,
      NewsDataëŠ” "ì¶”ê°€ ì†ŒìŠ¤" ëŠë‚Œìœ¼ë¡œë§Œ ì“°ë©´ ë¨.
    """
    url = NEWSDATA_BASE_URL_LATEST

    # free í”Œëœì—ì„œ í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ sizeëŠ” 10ê°œ
    # (ê·¸ ì´ìƒ ë„£ìœ¼ë©´ ì—ëŸ¬) :contentReference[oaicite:1]{index=1}
    size = min(page_size, 10)

    params = {
        "apikey": NEWSDATA_API_KEY,
        "q": query,
        "size": size,
    }

    if language:
        params["language"] = language  # "en", "ko" ë“±

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except requests.exceptions.HTTPError as e:
        status = getattr(e.response, "status_code", None)
        print(f"[WARN] NewsData.io HTTP error (status={status}, q={query}, lang={language}): {e}")
        return []
    except Exception as e:
        print(f"[WARN] NewsData.io error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    results = data.get("results") or []

    articles = []

    for item in results:
        creator = item.get("creator")
        if isinstance(creator, list):
            author = creator[0] if creator else None
        else:
            author = creator

        articles.append({
            # ê¸°ì¡´ NewsAPI article êµ¬ì¡°ì— ë§ì¶° ë³€í™˜
            "source": {"name": item.get("source_id")},
            "author": author,
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("content"),
            "url": item.get("link"),
            "publishedAt": item.get("pubDate"),  # ì˜ˆ: "2025-12-08 10:30:00"
        })

    return articles


def search_news_topheadlines_kr(page_size=50):
    """
    NewsAPI 'top-headlines' ì—”ë“œí¬ì¸íŠ¸ë¡œ
    í•œêµ­(country=kr) ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    url = "https://newsapi.org/v2/top-headlines"
    params = {
        "country": "kr",          # í•œêµ­
        "pageSize": page_size,    # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜
        "apiKey": NEWSAPI_KEY,
    }

    r = requests.get(url, params=params)
    r.raise_for_status()
    return r.json().get("articles", [])


EXCLUDE_KEYWORDS = [
    "tutorial", "how to", "explained", "explainer", "what is", "history of",
    "advertisement", "sponsored", "review", "buy now", "product page"
]

def is_basic_newsworthy(article):
    """
    ë…¸ê³¨ì ì¸ íŠœí† ë¦¬ì–¼/ê´‘ê³ /ìƒí’ˆ í˜ì´ì§€ ë“± 1ì°¨ í•„í„°
    """
    title = (article.get("title") or "").lower()
    description = (article.get("description") or "").lower()
    content = (article.get("content") or "").lower()
    text = " ".join([title, description, content])
    for bad in EXCLUDE_KEYWORDS:
        if bad in text:
            return False
    return True


def collect_articles_for_topic(topic_id, keywords):
    collected_ko = []  # í•œê¸€ ê¸°ì‚¬ ì €ì¥
    collected_en = []  # ì˜ì–´ ê¸°ì‚¬ ì €ì¥
    seen_urls = set()

    for kw in keywords:
        for lang in LANGUAGES:
            # ì–¸ì–´ë³„ ëª©í‘œ ê°œìˆ˜ í™•ì¸
            if lang == "ko":
                if len(collected_ko) >= ARTICLES_PER_LANG_KO:
                    continue
                remaining = ARTICLES_PER_LANG_KO - len(collected_ko)
                target_list = collected_ko
            else:  # "en"
                if len(collected_en) >= ARTICLES_PER_LANG_EN:
                    continue
                remaining = ARTICLES_PER_LANG_EN - len(collected_en)
                target_list = collected_en

            tier_articles = []

            # -----------------------------
            # 1ï¸âƒ£ 1ìˆœìœ„: GNews
            # -----------------------------
            try:
                gnews_list = search_news_gnews(
                    kw,
                    DATE_FROM,
                    DATE_TO,
                    language=lang,
                    page_size=remaining,
                )
                tier_articles.extend(gnews_list or [])
            except Exception as e:
                print(f"[WARN] GNews error (kw={kw}, lang={lang}): {e}")

            # -----------------------------
            # 2ï¸âƒ£ 2ìˆœìœ„: NewsAPI + MediaStack
            #    (GNewsì—ì„œ ë¶€ì¡±í•˜ë©´)
            # -----------------------------
            if len(tier_articles) < remaining:
                rem2 = remaining - len(tier_articles)

                # 2-1) NewsAPI
                try:
                    newsapi_list = search_news_newsapi(
                        kw,
                        DATE_FROM,
                        DATE_TO,
                        language=lang,
                        page_size=rem2,
                    )
                    tier_articles.extend(newsapi_list or [])
                except Exception as e:
                    print(f"[WARN] NewsAPI error (kw={kw}, lang={lang}): {e}")

                # 2-2) MediaStack
                if len(tier_articles) < remaining:
                    rem3 = remaining - len(tier_articles)
                    try:
                        mediastack_list = search_news_mediastack(
                            kw,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=rem3,
                        )
                        tier_articles.extend(mediastack_list or [])
                    except Exception as e:
                        print(f"[WARN] MediaStack error (kw={kw}, lang={lang}): {e}")

            # -----------------------------
            # 3ï¸âƒ£ 3ìˆœìœ„: SerpAPI + Currents + NewsData.io
            #    (ê·¸ë˜ë„ ë¶€ì¡±í•  ë•Œ)
            # -----------------------------
            if len(tier_articles) < remaining:
                rem4 = remaining - len(tier_articles)

                # SerpAPI
                try:
                    serp_list = search_news_serpapi(
                        kw,
                        DATE_FROM,
                        DATE_TO,
                        language=lang,
                        page_size=min(rem4, 10),
                    )
                    tier_articles.extend(serp_list or [])
                except Exception as e:
                    print(f"[WARN] SerpAPI error (kw={kw}, lang={lang}): {e}")

                # Currents
                if len(tier_articles) < remaining:
                    rem5 = remaining - len(tier_articles)
                    try:
                        curr_list = search_news_currents(
                            kw,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem5, 50),
                        )
                        tier_articles.extend(curr_list or [])
                    except Exception as e:
                        print(f"[WARN] Currents error (kw={kw}, lang={lang}): {e}")

                # NewsData.io (ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©)
                if len(tier_articles) < remaining:
                    rem6 = remaining - len(tier_articles)
                    try:
                        newsdata_list = search_news_newsdata(
                            kw,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=rem6,
                        )
                        tier_articles.extend(newsdata_list or [])
                    except Exception as e:
                        print(f"[WARN] NewsData.io error (kw={kw}, lang={lang}): {e}")

            # ğŸ†• í•œê¸€ í‚¤ì›Œë“œì¼ ë•Œ top-headlines ì¶”ê°€ ìˆ˜ì§‘
            if lang == "ko" and any('\uac00' <= ch <= '\ud7a3' for ch in kw):
                try:
                    extra_kr = search_news_topheadlines_kr(page_size=10)
                    # í‚¤ì›Œë“œ í•„í„°ë§ (ê´€ë ¨ì„± ì²´í¬)
                    for art in extra_kr:
                        title_text = str(art.get('title', '')).lower()
                        if any(k.lower() in title_text for k in kw.split()):
                            tier_articles.append(art)
                except Exception as e:
                    print(f"[WARN] top-headlines ë³´ê°• ì‹¤íŒ¨: {e}")


            # -----------------------------
            # ê³µí†µ í›„ì²˜ë¦¬: URL ì¤‘ë³µ ì œê±° + ë‚ ì§œ í•„í„° + ê¸°ë³¸ í•„í„°
            # -----------------------------
            for a in tier_articles:
                # ì–¸ì–´ë³„ ëª©í‘œ ê°œìˆ˜ ì²´í¬
                if lang == "ko" and len(collected_ko) >= ARTICLES_PER_LANG_KO:
                    break
                if lang == "en" and len(collected_en) >= ARTICLES_PER_LANG_EN:
                    break

                url = a.get("url")
                if not url or url in seen_urls:
                    continue

                published_at_raw = a.get("publishedAt")

                # 1) published_atì´ ì•„ì˜ˆ ì—†ëŠ” ê²½ìš°
                if not published_at_raw:
                    # âœ… ë‚ ì§œ ì •ë³´ê°€ ì—†ìœ¼ë©´ ì–¸ì–´ êµ¬ë¶„ ì—†ì´ ì˜¤ëŠ˜ ë‚ ì§œë¡œ ê°„ì£¼
                    published_dt = datetime.fromisoformat(DATE_TO).date()
                    # continue ì œê±° â†’ ëª¨ë“  ì–¸ì–´ ì‚´ë¦¼
                else:
                    # 2) ë‚ ì§œ ë¬¸ìì—´ì´ ì´ìƒí•œ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ try/except
                    try:
                        parsed = dateparser.parse(published_at_raw)
                        if parsed is None:
                            raise ValueError("parsed is None")
                        published_dt = parsed.date()
                    except Exception:
                        # âœ… íŒŒì‹± ì‹¤íŒ¨ ì‹œ ì–¸ì–´ êµ¬ë¶„ ì—†ì´ ì˜¤ëŠ˜ ë‚ ì§œ ì‚¬ìš©
                        published_dt = datetime.fromisoformat(DATE_TO).date()

                # 3) ë‚ ì§œ ë²”ìœ„ í•„í„° ì ìš© (7ì¼ ë²”ìœ„)
                from_dt = datetime.fromisoformat(DATE_FROM).date()
                to_dt = datetime.fromisoformat(DATE_TO).date()
                if not (from_dt <= published_dt <= to_dt):
                    continue

                # 4) ê´‘ê³ /íŠœí† ë¦¬ì–¼ ë“± 1ì°¨ í•„í„°
                if not is_basic_newsworthy(a):
                    continue

                # 5) ìµœì¢… ì±„íƒ (ì–¸ì–´ë³„ ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€)
                seen_urls.add(url)
                article_data = {
                    "topic_seed": topic_id,
                    "source_name": a.get("source", {}).get("name"),
                    "author": a.get("author"),
                    "original_title": a.get("title"),
                    "description": a.get("description"),
                    "content": a.get("content"),
                    "url": url,
                    "published_at": str(published_dt),
                }
                target_list.append(article_data)

    # ìµœì¢… ê²°ê³¼: í•œê¸€ + ì˜ì–´ í•©ì¹˜ê¸°
    collected = collected_ko + collected_en
    print(f"  â”” ì£¼ì œ {topic_id}: í•œê¸€ {len(collected_ko)}ê°œ, ì˜ì–´ {len(collected_en)}ê°œ ìˆ˜ì§‘ë¨")
    return collected




print("=== [1ë‹¨ê³„] NewsAPIì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ===")

raw_articles = []

for t_id, kws in TOPIC_KEYWORDS.items():
    arts = collect_articles_for_topic(t_id, kws)
    print(f"ì£¼ì œ {t_id} ({TOPIC_DESC[t_id]}) : {len(arts)}ê°œ ê¸°ì‚¬ í›„ë³´ ìˆ˜ì§‘")
    raw_articles.extend(arts)


df_raw = pd.DataFrame(raw_articles)
print("\n[1ì°¨ ìˆ˜ì§‘ ê²°ê³¼ ê°œìˆ˜] :", len(df_raw))
if IN_COLAB:
    display(df_raw.head())


# # **03-1 ì–¸ì–´ë³„ ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜**

# In[16]:


# ============================
# 3-1. ì–¸ì–´ë³„ ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# ============================

def calculate_language_ratio(articles):
    """ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•œê¸€/ì˜ë¬¸ ë¹„ìœ¨ ê³„ì‚°"""
    total = len(articles)
    if total == 0:
        return 0, 0, 0, 0

    korean_count = sum(1 for art in articles if is_korean_article(art))
    english_count = total - korean_count

    korean_pct = (korean_count / total) * 100
    english_pct = (english_count / total) * 100

    return korean_count, english_count, korean_pct, english_pct


def is_korean_article(article_dict):
    """ê¸°ì‚¬ê°€ í•œê¸€ì¸ì§€ íŒë‹¨"""
    text = " ".join([
        str(article_dict.get("original_title") or ""),
        str(article_dict.get("title_ko") or ""),
        str(article_dict.get("summary_ko") or ""),
    ])
    return any('\uac00' <= ch <= '\ud7a3' for ch in text)


# # **04 GPT (ì—„ê²© í•„í„°ë§/ë¶„ë¥˜/ìš”ì•½)**

# In[17]:


# ============================
# 4. GPT minië¡œ ìš”ì•½/ë¶„ë¥˜/í•„í„°ë§
# ============================

SYSTEM_PROMPT_STRICT = """
ë‹¹ì‹ ì€ B2B í…Œí¬ ë‰´ìŠ¤ë ˆí„° í¸ì§‘ìì…ë‹ˆë‹¤.
ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©° ê¸°ì‚¬ë¥¼ ì„ ë³„í•˜ê³  í•œêµ­ì–´ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

[ì£¼ì œ ë²”ìœ„]
1) GeoINT / ì§€ë¦¬ê³µê°„ ì •ë³´ / ìœ„ì„±ì •ë³´ / ì§€ë„ë°ì´í„° ë¶„ì„
2) ë“œë¡  / UAV / UAM ì‚°ì—… ë° ê¸°ìˆ 
3) AI ë°ì´í„°Â·ë¶„ì„ í”Œë«í¼ / MLOps / ìë™í™”Â·ì—”í„°í”„ë¼ì´ì¦ˆ AI ì¸í”„ë¼
   ë¿ë§Œ ì•„ë‹ˆë¼, ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ë„ ëª¨ë‘ 3ë²ˆìœ¼ë¡œ í¬í•¨í•©ë‹ˆë‹¤.
   - ì—”í„°í”„ë¼ì´ì¦ˆ/ì‚°ì—… ë¶„ì•¼ì˜ AI ë„ì…Â·ì „ëµÂ·ê±°ë²„ë„ŒìŠ¤
   - ìƒì„±í˜• AI(GenAI) ë° LLM í™œìš© ì‚¬ë¡€, AI ê¸°ë°˜ ì—…ë¬´ ìë™í™”
   - AI í´ë¼ìš°ë“œ ì¸í”„ë¼, GPU/ê°€ì†ê¸°, ëª¨ë¸ í—ˆë¸Œ/ë§ˆì¼“í”Œë ˆì´ìŠ¤
   - ë°ì´í„°Â·ë¶„ì„ ì œí’ˆì— AI ê¸°ëŠ¥ì´ í¬ê²Œ ì¶”ê°€ë˜ëŠ” ê²½ìš°
   - ê¸°ì—…ìš© í˜‘ì—…íˆ´Â·ì—…ë¬´íˆ´ì—ì„œ AI ì—ì´ì „íŠ¸/ì½”íŒŒì¼ëŸ¿ì´ í•µì‹¬ì¸ ì—…ë°ì´íŠ¸

   ë‹¨, ìˆœìˆ˜ ì†Œë¹„ììš© ì„œë¹„ìŠ¤(ê²Œì„ ì¶”ì²œ, SNS í•„í„° ë“±)ë§Œ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ëŠ” ë˜ë„ë¡ ì œì™¸í•©ë‹ˆë‹¤.

4) ìœ„ì„± ì˜ìƒ(SARÂ·ê´‘í•™Â·í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´) ë¶„ì„

[íŠ¹ë³„ ê·œì¹™: í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬]
- ê¸°ì‚¬ ì œëª©Â·ë³¸ë¬¸Â·ì„¤ëª…ì— 'í•œì»´', 'í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤', 'Hancom InSpace', 'InSpace' ë“±ì´ í¬í•¨ë˜ë©´
  -> ë°˜ë“œì‹œ keep = true ë¡œ ì„¤ì •í•˜ì„¸ìš”.
- ì£¼ì œ ë²”ìœ„ë¥¼ ì•½ê°„ ë²—ì–´ë‚˜ë„ ê´œì°®ìŠµë‹ˆë‹¤.
- topic_finalì€ 1~4 ì¤‘ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë²ˆí˜¸ë¡œ ì„ íƒí•˜ì„¸ìš”.
- reason í•„ë“œì— ë°˜ë“œì‹œ "[í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨]"ì„ í¬í•¨í•˜ì„¸ìš”.

[ë¶ˆí—ˆ ì»¨í…ì¸ ]
- ë‹¨ìˆœ ë¸”ë¡œê·¸ ê¸€, ì œí’ˆ ê´‘ê³ , ê¸°ì´ˆ ì„¤ëª…, íŠœí† ë¦¬ì–¼ì€ keep=false

[ì¶œë ¥ JSON í˜•ì‹]
{
  "keep": true ë˜ëŠ” false,
  "topic_final": 1~4,
  "title_ko": "...",
  "summary_ko": "...",
  "reason": "..."
}
"""

SYSTEM_PROMPT_LOOSE = SYSTEM_PROMPT_STRICT + """
[ì¶”ê°€ ì™„í™” ê·œì¹™]
- keep ì—¬ë¶€ë¥¼ íŒë‹¨í•  ë•Œ, ì£¼ì œì™€ì˜ ê´€ë ¨ì„±ì´ ì•½ê°„ ì• ë§¤í•˜ë”ë¼ë„ ê°€ëŠ¥í•œ í•œ keep=true ìª½ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- íŠ¹íˆ ì •ë¶€ ì •ì±…, ê³µê³µê¸°ê´€, ëŒ€ê¸°ì—…ì˜ AIÂ·ìœ„ì„±Â·ë“œë¡ Â·ë°ì´í„° ê´€ë ¨ ë°œí‘œÂ·MOUÂ·ì‚¬ì—…ê³„íš ë“±ì€ í­ë„“ê²Œ í¬í•¨í•©ë‹ˆë‹¤.
- 'íŠœí† ë¦¬ì–¼/ìˆœìˆ˜ ê´‘ê³ /ì™„ì „ ë‹¤ë¥¸ ë¶„ì•¼(ì—°ì˜ˆÂ·ìŠ¤í¬ì¸  ë“±)'ê°€ ì•„ë‹Œ ì´ìƒ, ì—”í„°í”„ë¼ì´ì¦ˆ/ì‚°ì—…Â·ê³µê³µê³¼ ì¡°ê¸ˆì´ë¼ë„ ì ‘ì ì´ ë³´ì´ë©´ keep=trueë¡œ ë‘ì„¸ìš”.
"""


def build_user_prompt(row):
    topic_hint = TOPIC_DESC.get(row["topic_seed"], "")
    text = f"""
[seed topic ì •ë³´]
- id: {row['topic_seed']}
- desc: {topic_hint}

[ê¸°ì‚¬ ë©”íƒ€]
- source: {row.get('source_name')}
- url: {row.get('url')}
- published_at: {row.get('published_at')}

[ì œëª©]
{row.get('original_title')}

[description]
{row.get('description')}

[ë³¸ë¬¸ content]
{row.get('content')}
"""
    return text


# ì „ì—­ ë³€ìˆ˜ë¡œ í† í° ëˆ„ì 
if 'total_tokens_used' not in globals():
    total_tokens_used = {"input": 0, "output": 0}


def gpt_process_article(row, system_prompt=SYSTEM_PROMPT_STRICT, model=MODEL_NAME):
    user_prompt = build_user_prompt(row)
    response = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3  # ğŸ”¥ 0ì—ì„œ 0.3ìœ¼ë¡œ ë³€ê²½
    )

        # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
    if hasattr(response, 'usage'):
        total_tokens_used["input"] += response.usage.input_tokens
        total_tokens_used["output"] += response.usage.output_tokens

    text_out = response.output[0].content[0].text
    return json.loads(text_out)



print("\n=== [4ë‹¨ê³„] GPT ì—„ê²© í•„í„°ë§ + í•œì»´ ê°•ì œ í¬í•¨ ë¡œì§ ===")


# ============================================================
# âœ… (NEW) GPT ê¸°ë°˜ "ê°™ì€ ì‚¬ê±´/ê°™ì€ ë‚´ìš©" ì¤‘ë³µ íŒë³„
# - ì œëª©ì´ ë‹¬ë¼ë„ ìš”ì•½/ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
# - ë¹„ìš©/ì†ë„ ë•Œë¬¸ì— 1ì°¨(ë¬¸ì ìœ ì‚¬ë„) â†’ 2ì°¨(GPT) 2ë‹¨ê³„ë¡œ ì‚¬ìš© ê¶Œì¥
# ============================================================

SEMANTIC_DEDUP_ENABLE = True

# ì œëª© ìœ ì‚¬ë„ê°€ ì´ ê°’ ì´ìƒì´ë©´ GPT ì—†ì´ë„ ì¤‘ë³µ ì²˜ë¦¬(ë¹ ë¥´ê³  ì €ë ´)
FAST_TITLE_DUP_THRESHOLD = 0.88

# ì œëª© ìœ ì‚¬ë„ê°€ ì´ êµ¬ê°„ì´ë©´(ì• ë§¤) GPTì—ê²Œ ì˜ë¯¸ì¤‘ë³µ ì—¬ë¶€ë¥¼ ë¬¼ì–´ë´„
SEMANTIC_CHECK_MIN = 0.45
SEMANTIC_CHECK_MAX = 0.88

def gpt_is_same_story(a_title: str, a_summary: str, b_title: str, b_summary: str, model=MODEL_NAME) -> bool:
    """
    GPTê°€ ë‘ ê¸°ì‚¬ê°€ ê°™ì€ ì‚¬ê±´/ë‚´ìš©ì¸ì§€(ì˜ë¯¸ ì¤‘ë³µ) íŒë‹¨.
    ë°˜í™˜: True(ì¤‘ë³µ) / False(ì¤‘ë³µ ì•„ë‹˜)
    """
    # ì…ë ¥ì´ ë„ˆë¬´ ë¹„ë©´ íŒë‹¨ ë¶ˆê°€ â†’ ì¤‘ë³µ ì•„ë‹˜ ì²˜ë¦¬
    if not (a_title or a_summary) or not (b_title or b_summary):
        return False

    system = (
        "You are a strict news deduplication engine.\n"
        "Decide whether two news items refer to the same underlying event/story.\n"
        "Return ONLY valid JSON."
    )

    user = f"""
[Article A]
title: {a_title}
summary: {a_summary}

[Article B]
title: {b_title}
summary: {b_summary}

Rules:
- Consider them duplicates if they describe the same event/announcement/deal/report, even if wording differs.
- Consider them NOT duplicates if they are different events, different companies, different timeframes, or one is analysis/opinion while the other is a different news event.
Output JSON schema:
{{"duplicate": true/false, "confidence": 0-100}}
"""

    try:
        resp = client.responses.create(
            model=model,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.0,
        )
        out = resp.output[0].content[0].text
        data = json.loads(out)
        dup = bool(data.get("duplicate", False))
        conf = int(data.get("confidence", 0) or 0)

        # ë³´ìˆ˜ì ìœ¼ë¡œ: confidenceê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì•ˆ ë´„
        if dup and conf >= 65:
            return True
        return False

    except Exception as e:
        print(f"[WARN] gpt_is_same_story ì‹¤íŒ¨: {e}")
        return False


def is_korean_article_row(row) -> bool:
    """
    df_rawì˜ í•œ í–‰ì´ í•œêµ­ì–´ ê¸°ì‚¬ì¸ì§€ ê°„ë‹¨íˆ íŒë³„:
    ì œëª©/description/content ì¤‘ í•˜ë‚˜ì— í•œê¸€ì´ ìˆìœ¼ë©´ True
    """
    text = " ".join([
        str(row.get("original_title") or ""),
        str(row.get("description") or ""),
        str(row.get("content") or ""),
    ])
    return any('\uac00' <= ch <= '\ud7a3' for ch in text)


strict_rows = []
hancom_keywords = [
    "í•œì»´", "í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤", "hancom", "hancom inspace", "inspace", "ì¸ìŠ¤í˜ì´ìŠ¤"
]

# ë³‘ë ¬ ì²˜ë¦¬ìš© í•¨ìˆ˜ ì •ì˜
def process_single_article(idx_row):
    """
    ë‹¨ì¼ ê¸°ì‚¬ë¥¼ GPTë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    idx, row = idx_row
    url = row.get("url")
    if not url:
        return None

    # 1) GPT ì²˜ë¦¬
    try:
        result = gpt_process_article(row, system_prompt=SYSTEM_PROMPT_STRICT)
    except Exception as e:
        print(f"[ERROR] GPT ì²˜ë¦¬ ì‹¤íŒ¨ (url={url}): {e}")
        return None

    # 2) í•œì»´ í‚¤ì›Œë“œ ì²´í¬
    title_lower = str(row.get("original_title") or "").lower()
    desc_lower = str(row.get("description") or "").lower()

    is_hancom = any(
        kw.lower() in title_lower or kw.lower() in desc_lower
        for kw in hancom_keywords
    )

    keep = result.get("keep", False)
    if is_hancom:
        keep = True
        result["keep"] = True
        result["reason"] = "[í•œì»´ ê´€ë ¨] " + result.get("reason", "")

    # 3) keep=Falseì´ë©´ None ë°˜í™˜
    if not keep:
        return None

    # 4) ê²°ê³¼ ë°˜í™˜
    return {
        "topic_seed": row.get("topic_seed"),
        "topic_final": row.get("topic_seed"),               # â† ì¶”ê°€!
        "korean_summary": result.get("summary_ko", ""),
        "english_summary": "",
        "title_ko": result.get("title_ko", ""),
        "summary_ko": result.get("summary_ko", ""),
        "title_en": "",                                      # â† ì¶”ê°€!
        "summary_en": "",                                    # â† ì¶”ê°€!
        "original_title": row.get("original_title"),
        "description": row.get("description"),
        "url": url,
        "published_at": row.get("published_at"),
        "source_name": row.get("source_name"),
        "reason": result.get("reason"),
    }

# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
print(f"\nì´ {len(df_raw)}ê°œ ê¸°ì‚¬ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë™ì‹œ ì²˜ë¦¬: 20ê°œ)...")
with ThreadPoolExecutor(max_workers=20) as executor:
    # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì œì¶œ
    futures = {executor.submit(process_single_article, (idx, row)): idx
               for idx, row in df_raw.iterrows()}

    # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
    completed = 0
    for future in as_completed(futures):
        result = future.result()
        if result:
            strict_rows.append(result)

        completed += 1
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (20ê°œë§ˆë‹¤)
        if completed % 20 == 0:  # â† ì—¬ê¸°! for ë£¨í”„ ì•ˆì— ìˆì–´ìš”
            print(f"  ì§„í–‰: {completed}/{len(df_raw)} ({completed*100//len(df_raw)}%) - ì„ ë³„ëœ ê¸°ì‚¬: {len(strict_rows)}ê°œ")

print(f"\në³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(strict_rows)}ê°œ ê¸°ì‚¬ ì„ ë³„ë¨")


df_strict = pd.DataFrame(strict_rows)
print("\n[GTP í•„í„°ë§ í›„ strict_rows ê°œìˆ˜]:", len(df_strict))
if IN_COLAB:
    display(df_strict.head())

# ---------- URL ê¸°ì¤€ ì¤‘ë³µ ì •ë¦¬ ----------
dedup_rows = []

if not df_strict.empty:
    for url, group in df_strict.groupby("url"):
        # topic_final ë¹ˆë„ìˆ˜ ê¸°ë°˜ ì„ íƒ
        topic_counts = group["topic_final"].value_counts()
        best_topic = int(topic_counts.index[0])

        # ìµœì‹  ê¸°ì‚¬ ì„ íƒ
        best_row = group[group["topic_final"] == best_topic] \
            .sort_values("published_at", ascending=False) \
            .iloc[0]

        dedup_rows.append(best_row.to_dict())

df_final = pd.DataFrame(dedup_rows).reset_index(drop=True)

print("\n[URL ì¤‘ë³µ ì œê±° í›„ df_final ê°œìˆ˜]:", len(df_final))
if IN_COLAB:
    display(df_final)


# # **05 ë¶€ì¡±í•œ í† í”½ì€ ë°±ì—… í”„ë¡¬í”„íŠ¸ë¡œ ì±„ìš°ê¸° + í† í”½ë‹¹ 3ê°œ ë§ì¶”ê¸°**

# In[18]:


# ============================
# 5. ì£¼ì œë³„ ìµœì†Œ 3ê°œ ê°•ì œ: ë¶€ì¡±ë¶„ì€ ë°±ì—… í”„ë¡¬í”„íŠ¸ë¡œ ì±„ìš°ê¸°
#    + í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
# ============================

def detect_hancom_priority(row):
    """
    í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬ì´ë©´ priority=1, ì•„ë‹ˆë©´ 0
    - ì œëª© / í•œê¸€ì œëª© / í•œê¸€ìš”ì•½ / ì†ŒìŠ¤ëª…ì— 'í•œì»´', 'InSpace' ë“±ì´ í¬í•¨ë˜ë©´ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
    """
    text = " ".join([
        str(row.get("original_title", "") or ""),
        str(row.get("title_ko", "") or ""),
        str(row.get("summary_ko", "") or ""),
        str(row.get("source_name", "") or "")
    ]).lower()

    keywords = [
        "í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤",
        "í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤",
        "hancominspace",
        "hancom inspace",
        "hancom",
        "inspace",
    ]

    return 1 if any(k in text for k in keywords) else 0


def fill_topic_with_backup(topic_id, needed, df_final, df_raw):
    """
    topic_idì— ëŒ€í•´ df_finalì— ë¶€ì¡±í•œ ê°œìˆ˜(needed)ë§Œí¼
    df_rawì—ì„œ ì•„ì§ ì•ˆ ì“´ URLì„ ê³¨ë¼
    'ëŠìŠ¨í•œ' í”„ë¡¬í”„íŠ¸ë¡œ ìš”ì•½í•´ì„œ df_finalì— ì¶”ê°€.
    """
    used_urls = set(df_final["url"].tolist())
    cand = df_raw[(df_raw["topic_seed"] == topic_id) & (~df_raw["url"].isin(used_urls))]
    cand = cand.sort_values("published_at", ascending=False)

    added_rows = []
    for _, row in cand.iterrows():
        if needed <= 0:
            break
        try:
            result = gpt_process_article(row, system_prompt=SYSTEM_PROMPT_LOOSE)
        except Exception as e:
            print(f"[ë°±ì—… ì—ëŸ¬] topic {topic_id}, url={row.get('url')}: {e}")
            continue

        # ì–´ì°¨í”¼ seed topicìš©ì´ë¯€ë¡œ ê°•ì œë¡œ topic_final/keep ë³´ì •
        result_topic = topic_id
        title_ko = result.get("title_ko")
        summary_ko = result.get("summary_ko")
        reason = result.get("reason", "")

        if not title_ko or not summary_ko:
            # ë„ˆë¬´ ë¶€ì‹¤í•˜ë©´ ìŠ¤í‚µ
            continue

        added_rows.append({
            "topic_final": result_topic,
            "keep": True,
            "title_ko": title_ko,
            "summary_ko": summary_ko,
            "original_title": row.get("original_title"),
            "url": row.get("url"),
            "published_at": row.get("published_at"),
            "source_name": row.get("source_name"),
            "reason": f"[ë°±ì—…ê¸°ì‚¬] {reason}",
        })
        needed -= 1
        time.sleep(0.05)

    if added_rows:
        df_add = pd.DataFrame(added_rows)
        df_final = pd.concat([df_final, df_add], ignore_index=True)

    return df_final


# í˜„ì¬ í† í”½ë³„ ê°œìˆ˜ í™•ì¸ í›„ ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°
for t in [1, 2, 3, 4]:
    cur = df_final[df_final["topic_final"] == t]
    cnt = len(cur)

    # [ìˆ˜ì •] ì˜ˆì „ì—ëŠ” "3ê°œ ì´ìƒì´ë©´ í†µê³¼"ì˜€ëŠ”ë°,
    # ì´ì œëŠ” "ìµœì†Œ 9ê°œ(MIN_TOTAL_PER_TOPIC) ì´ìƒ"ì´ì–´ì•¼ í†µê³¼
    if cnt >= MIN_TOTAL_PER_TOPIC:
        continue

    needed = MIN_TOTAL_PER_TOPIC - cnt
    print(
        f"í† í”½ {t} ë³´ê°• í•„ìš”: {cnt}ê°œ â†’ {MIN_TOTAL_PER_TOPIC}ê°œë¡œ, "
        f"{needed}ê°œ ë°±ì—… ìˆ˜ì§‘ ì‹œë„"
    )
    df_final = fill_topic_with_backup(t, needed, df_final, df_raw)

# ê·¸ë˜ë„ í˜¹ì‹œ ë¶€ì¡±í•˜ë©´(NewsAPIì— ê¸°ì‚¬ ìì²´ê°€ ì—†ëŠ” ê²½ìš°) ê²½ê³ ë§Œ ì°ê³  ë„˜ì–´ê°
for t in [1, 2, 3, 4]:
    cnt = len(df_final[df_final["topic_final"] == t])
    if cnt < ARTICLES_PER_TOPIC_FINAL:
        print(f"[ê²½ê³ ] í† í”½ {t}ëŠ” NewsAPIì— ê¸°ì‚¬ê°€ ë¶€ì¡±í•´ì„œ {cnt}ê°œë§Œ í‘œì‹œë©ë‹ˆë‹¤.")

# ---------- í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ì—¬ë¶€ í”Œë˜ê·¸ ----------
df_final["hancom_priority"] = df_final.apply(detect_hancom_priority, axis=1)
print("í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜:", int(df_final["hancom_priority"].sum()))

# ì—¬ê¸°ì„œëŠ” ì •ë ¬í•˜ì§€ ì•Šê³ , ë’¤ì—ì„œ 'ì‚¬íšŒì  ê´€ì‹¬ ê¸°ë°˜ priority'ë¥¼ ê³„ì‚°í•œ ë‹¤ìŒì— ì •ë ¬í•©ë‹ˆë‹¤.


print("\n[ë°±ì—… í¬í•¨ ìµœì¢… ê¸°ì‚¬ ê°œìˆ˜] :", len(df_final))
if IN_COLAB:
    display(df_final)

df_final.to_csv("newsletter_articles.csv", index=False)
print("CSV ì €ì¥ ì™„ë£Œ: newsletter_articles.csv")


# # **06 ë©”ì¸(3ê°œ) + ë”ë³´ê¸° ê¸°ì‚¬ ë¶„ë¦¬**

# In[19]:


# ============================
# 6. ë©”ì¸(3ê°œ) + ë”ë³´ê¸°(ìµœëŒ€ 10ê°œ) ê¸°ì‚¬ ë¶„ë¦¬
# ============================

from collections import defaultdict
import difflib
import re

# ---------- ì‚¬íšŒì  ê´€ì‹¬ ê¸°ë°˜ ì¤‘ìš”ë„(priority) ê³„ì‚° ----------

# 1) ë§¤ì²´ ê°€ì¤‘ì¹˜: ê¸°ë³¸ê°’ 1.0, ì£¼ìš” ë§¤ì²´ëŠ” ë” ë†’ê²Œ
SOURCE_WEIGHTS = {
    "ì—°í•©ë‰´ìŠ¤": 2.0,
    "ì¡°ì„ ë¹„ì¦ˆ": 2.0,
    "í•œêµ­ê²½ì œ": 2.0,
    "ë§¤ì¼ê²½ì œ": 2.0,
    "ì „ìì‹ ë¬¸": 1.5,
    "ì„œìš¸ê²½ì œ": 1.5,
    # í•„ìš”í•˜ë©´ ì—¬ê¸° ê³„ì† ì¶”ê°€
}

def _get_source_weight(source_name):
    name = str(source_name or "").strip()
    return SOURCE_WEIGHTS.get(name, 1.0)  # ê¸°ë³¸ê°’ 1.0

# 2) í›„ì†/ë¶„ì„ ê¸°ì‚¬ ì—¬ë¶€ (followup_score)
FOLLOWUP_KEYWORDS = [
    "ë¶„ì„", "ì „ë§", "ì˜í–¥", "ì˜ë¯¸",
    "íŒŒê¸‰", "í•´ì„¤", "ì™œ", "ì´ìœ "
]

def _get_followup_score(title_ko, original_title):
    text = f"{title_ko or ''} {original_title or ''}"
    return 1 if any(k in text for k in FOLLOWUP_KEYWORDS) else 0

# 3) í´ëŸ¬ìŠ¤í„°(ë¹„ìŠ·í•œ ê¸°ì‚¬ ìˆ˜) + í™•ì‚° ì†ë„(velocity) ê³„ì‚°
#    - ì „ì²´ df_final ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
import pandas as pd  # ì´ë¯¸ ìœ„ì— ìˆì„ ê°€ëŠ¥ì„±ì´ í¬ì§€ë§Œ, í•œ ë²ˆ ë” ì¨ë„ ë¬¸ì œ ì—†ìŒ

# published_atì„ datetimeìœ¼ë¡œ ë³€í™˜ (ì†ë„ ê³„ì‚°ìš©)
df_final["published_at_dt"] = pd.to_datetime(df_final["published_at"], errors="coerce")

# ë¹„êµì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ (ì œëª© + í•œê¸€ì œëª© + ìš”ì•½)
texts = (
    df_final["original_title"].fillna("") + " || " +
    df_final["title_ko"].fillna("") + " || " +
    df_final["summary_ko"].fillna("")
).tolist()

times = df_final["published_at_dt"].tolist()
n = len(df_final)

# ië²ˆì§¸ ê¸°ì‚¬ì™€ ë¹„ìŠ·í•œ ê¸°ì‚¬ ì¸ë±ìŠ¤ ëª¨ìŒ
similar_indices = [set() for _ in range(n)]

for i in range(n):
    for j in range(i + 1, n):
        a = texts[i].lower().strip()
        b = texts[j].lower().strip()
        if not a or not b:
            continue
        sim = difflib.SequenceMatcher(None, a, b).ratio()
        # ìœ ì‚¬ë„ thresholdëŠ” 0.75 ì •ë„ë¡œ ì„¤ì •
        if sim >= 0.75:
            similar_indices[i].add(j)
            similar_indices[j].add(i)

cluster_scores = []
velocity_scores = []

for i in range(n):
    cnt = len(similar_indices[i]) + 1

    # (1) cluster_score: ë¹„ìŠ·í•œ ê¸°ì‚¬ ê°œìˆ˜ì— ë”°ë¼ ì ìˆ˜ ë¶€ì—¬
    if cnt <= 1:
        cluster = 0
    elif cnt <= 3:
        cluster = 1
    elif cnt <= 6:
        cluster = 2
    else:
        cluster = 3
    cluster_scores.append(cluster)

    # (2) velocity_score: ì§§ì€ ì‹œê°„ ë™ì•ˆ ì–¼ë§ˆë‚˜ ë§ì´ ë‚˜ì™”ëŠ”ì§€
    if cnt <= 1:
        velocity = 0
    else:
        idxs = [i] + list(similar_indices[i])
        ts = [times[k] for k in idxs if pd.notnull(times[k])]
        if len(ts) >= 2:
            span_hours = (max(ts) - min(ts)).total_seconds() / 3600.0
            if span_hours <= 48:
                velocity = 2
            elif span_hours <= 168:
                velocity = 1
            else:
                velocity = 0
        else:
            velocity = 0
    velocity_scores.append(velocity)

# 4) df_finalì— ì»¬ëŸ¼ìœ¼ë¡œ ë¶™ì´ê¸°
df_final["cluster_score"]   = cluster_scores
df_final["velocity_score"]  = velocity_scores
df_final["source_weight"]   = df_final["source_name"].apply(_get_source_weight)
df_final["followup_score"]  = df_final.apply(
    lambda r: _get_followup_score(r.get("title_ko"), r.get("original_title")),
    axis=1
)

# 5) priority ê³„ì‚°
df_final["priority"] = (
    df_final["hancom_priority"] * 100 +
    df_final["cluster_score"] * 10 +
    df_final["velocity_score"] * 5 +
    df_final["source_weight"] * 3 +
    df_final["followup_score"] * 2
)

# âœ… ìƒˆë¡œ ì¶”ê°€: ì–¸ì–´ ê· í˜• ë³´ë„ˆìŠ¤
print("\n" + "="*60)
print("ğŸ¯ ì–¸ì–´ ê· í˜• ì¡°ì • ì¤‘...")
print("="*60)

TARGET_ENGLISH_RATIO = 70  # ëª©í‘œ ì˜ë¬¸ ë¹„ìœ¨
BALANCE_BONUS = 8          # ê· í˜• ë³´ë„ˆìŠ¤ ì ìˆ˜ (ë„ˆë¬´ í¬ì§€ ì•Šê²Œ)

for topic_num in [1, 2, 3, 4]:
    topic_mask = df_final["topic_final"] == topic_num
    topic_articles = df_final[topic_mask].to_dict('records')

    if len(topic_articles) == 0:
        continue

    ko_count, en_count, ko_pct, en_pct = calculate_language_ratio(topic_articles)

    # ì˜ë¬¸ì´ ë¶€ì¡±í•˜ë©´ ì˜ë¬¸ ê¸°ì‚¬ì— ë³´ë„ˆìŠ¤
    if en_pct < TARGET_ENGLISH_RATIO - 10:  # 60% ë¯¸ë§Œì´ë©´
        english_bonus = BALANCE_BONUS
        korean_bonus = 0
        print(f"[í† í”½ {topic_num}] ì˜ë¬¸ ë¶€ì¡±({en_pct:.1f}%) â†’ ì˜ë¬¸ ê¸°ì‚¬ ë³´ë„ˆìŠ¤ +{english_bonus}")

    # í•œê¸€ì´ ë¶€ì¡±í•˜ë©´ í•œê¸€ ê¸°ì‚¬ì— ë³´ë„ˆìŠ¤
    elif ko_pct < (100 - TARGET_ENGLISH_RATIO) - 10:  # 20% ë¯¸ë§Œì´ë©´
        english_bonus = 0
        korean_bonus = BALANCE_BONUS
        print(f"[í† í”½ {topic_num}] í•œê¸€ ë¶€ì¡±({ko_pct:.1f}%) â†’ í•œê¸€ ê¸°ì‚¬ ë³´ë„ˆìŠ¤ +{korean_bonus}")

    # ê· í˜•ì´ ì ë‹¹í•˜ë©´ ë³´ë„ˆìŠ¤ ì—†ìŒ
    else:
        english_bonus = 0
        korean_bonus = 0
        print(f"[í† í”½ {topic_num}] í˜„ì¬ ë¹„ìœ¨: ì˜ë¬¸ {en_pct:.1f}%, í•œê¸€ {ko_pct:.1f}% (ë³´ë„ˆìŠ¤ ì—†ìŒ)")

    # ë³´ë„ˆìŠ¤ ì ìš©
    if english_bonus > 0 or korean_bonus > 0:
        topic_indices = df_final[topic_mask].index

        for idx in topic_indices:
            row = df_final.loc[idx]
            text = " ".join([
                str(row.get("original_title") or ""),
                str(row.get("title_ko") or ""),
                str(row.get("summary_ko") or ""),
            ])
            is_korean = any('\uac00' <= ch <= '\ud7a3' for ch in text)

            bonus = korean_bonus if is_korean else english_bonus
            df_final.at[idx, "priority"] += bonus

# ë³´ë„ˆìŠ¤ ì ìš© í›„ ì¬ì •ë ¬
df_final = df_final.sort_values(
    ["priority", "published_at_dt"],
    ascending=[False, False]
).reset_index(drop=True)

print("\nâœ… ì–¸ì–´ ê· í˜• ì¡°ì • ì™„ë£Œ\n")

print("\n[ì¤‘ìš”ë„(priority) ê³„ì‚° í›„ ìƒìœ„ 5ê°œ]:")
if IN_COLAB:
    display(df_final.head())


topic_main_articles = {}
topic_extra_articles = {}

def _normalize_text(s):
    if s is None:
        return ""
    s = s.lower()
    # í•œê¸€(ê°€-í£), ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ê³µë°± ì²˜ë¦¬
    s = re.sub(r"[^0-9a-zA-Z\uAC00-\uD7A3]+", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def is_near_duplicate(text_a, text_b, threshold=0.8):
    """
    ë‘ í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ê°™ì€ì§€ íŒë‹¨.
    thresholdëŠ” 0.8~0.9 ì‚¬ì´ì—ì„œ ì¡°ì ˆí•˜ë©´ ë¨.
    """
    a = _normalize_text(text_a)
    b = _normalize_text(text_b)
    if not a or not b:
        return False
    return difflib.SequenceMatcher(None, a, b).ratio() >= threshold

def df_row_to_article(row, topic_num):
    topic_title = f"{topic_num}) {TOPIC_DESC[topic_num]}"
    return {
        "topic_title": topic_title,
        "ko_title": row["title_ko"],
        "orig_title": row["original_title"],
        "topic_num": str(topic_num),
        "url": row["url"],
        "date": row["published_at"],
        "summary": row["summary_ko"],
        "priority": row.get("priority", 0),
    }

for topic_num in [1, 2, 3, 4]:
    topic_df = df_final[df_final["topic_final"] == topic_num].copy()

    # 1) priority ìˆœìœ¼ë¡œë§Œ ì •ë ¬ (ë‚ ì§œ ì •ë ¬ ì œê±°)
    topic_df = topic_df.sort_values(
        "priority",
        ascending=False
    )

    # 2) ê±°ì˜ ë™ì¼í•œ ë‚´ìš© ê¸°ì‚¬ ì œê±° (âœ… GPT ì˜ë¯¸ ê¸°ë°˜ dedup)
    dedup_rows = []
    for _, row in topic_df.iterrows():
        cur_title_raw = row.get("title_ko") or row.get("original_title") or ""
        cur_title = _normalize_text(cur_title_raw)
        cur_summary = (row.get("summary_ko") or "").strip()

        is_dup = False

        for kept in dedup_rows:
            kept_title_raw = kept.get("title_ko") or kept.get("original_title") or ""
            kept_title = _normalize_text(kept_title_raw)
            kept_summary = (kept.get("summary_ko") or "").strip()

            # â‘  ì™„ì „ ë™ì¼ ì œëª©ì´ë©´ ì¦‰ì‹œ ì¤‘ë³µ
            if cur_title and kept_title and cur_title == kept_title:
                is_dup = True
                break

            # â‘¡ ì œëª© ìœ ì‚¬ë„(ë¹ ë¥¸ 1ì°¨)
            title_sim_dup = is_near_duplicate(cur_title, kept_title, threshold=FAST_TITLE_DUP_THRESHOLD)
            if title_sim_dup:
                is_dup = True
                break

            # â‘¢ ì• ë§¤í•œ êµ¬ê°„ì´ë©´ GPTë¡œ "ì˜ë¯¸ ì¤‘ë³µ" íŒë³„
            if SEMANTIC_DEDUP_ENABLE:
                # ì œëª© ìœ ì‚¬ë„ ìˆ˜ì¹˜ê°€ í•„ìš”í•˜ë¯€ë¡œ SequenceMatcher ì§ì ‘ ê³„ì‚°
                a = _normalize_text(cur_title)
                b = _normalize_text(kept_title)
                sim = difflib.SequenceMatcher(None, a, b).ratio() if a and b else 0.0

                if SEMANTIC_CHECK_MIN <= sim < SEMANTIC_CHECK_MAX:
                    if gpt_is_same_story(
                        a_title=cur_title_raw, a_summary=cur_summary,
                        b_title=kept_title_raw, b_summary=kept_summary,
                    ):
                        is_dup = True
                        break

        if not is_dup:
            dedup_rows.append(row)

    topic_df = pd.DataFrame(dedup_rows)


    if len(topic_df) == 0:
        print(f"[ê²½ê³ ] í† í”½ {topic_num}ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        topic_main_articles[topic_num] = []
        topic_extra_articles[topic_num] = []
        continue

    # 3) ë©”ì¸ 3ê°œ
    main_df = topic_df.head(ARTICLES_PER_TOPIC_FINAL)

    # 4) ì¶”ê°€ ê¸°ì‚¬
    extra_df = topic_df.iloc[ARTICLES_PER_TOPIC_FINAL :]

    main_list = [df_row_to_article(row, topic_num) for _, row in main_df.iterrows()]
    extra_list = [df_row_to_article(row, topic_num) for _, row in extra_df.iterrows()]

    topic_main_articles[topic_num] = main_list
    topic_extra_articles[topic_num] = extra_list

# ë””ë²„ê·¸ ì¶œë ¥
total_main = sum(len(v) for v in topic_main_articles.values())
total_extra = sum(len(v) for v in topic_extra_articles.values())
print("ë©”ì¸ ê¸°ì‚¬ ì´ ê°œìˆ˜:", total_main)
print("ì¶”ê°€ ê¸°ì‚¬ ì´ ê°œìˆ˜:", total_extra)
for t in [1, 2, 3, 4]:
    print(f"í† í”½ {t}: ë©”ì¸ {len(topic_main_articles[t])}ê°œ, ì¶”ê°€ {len(topic_extra_articles[t])}ê°œ")

# âœ… ìµœì¢… ì–¸ì–´ ë¹„ìœ¨ ëª¨ë‹ˆí„°ë§
print("\n" + "="*60)
print("ğŸ“Š í† í”½ë³„ ìµœì¢… ê¸°ì‚¬ ì–¸ì–´ ë¹„ìœ¨")
print("="*60)

for topic_num in [1, 2, 3, 4]:
    main_list = topic_main_articles.get(topic_num, [])
    extra_list = topic_extra_articles.get(topic_num, [])

    all_articles = main_list + extra_list
    ko_count, en_count, ko_pct, en_pct = calculate_language_ratio(all_articles)

    print(f"\ní† í”½ {topic_num} ({TOPIC_DESC[topic_num]}):")
    print(f"  ì˜ë¬¸: {en_count}ê°œ ({en_pct:.1f}%)")
    print(f"  í•œê¸€: {ko_count}ê°œ ({ko_pct:.1f}%)")
    print(f"  ì´: {len(all_articles)}ê°œ")

    # ê²½ê³  í‘œì‹œ
    if en_pct < 50:
        print(f"  âš ï¸ ì˜ë¬¸ ê¸°ì‚¬ ë¶€ì¡± ({en_pct:.1f}%)")
    elif ko_pct < 20:
        print(f"  âš ï¸ í•œê¸€ ê¸°ì‚¬ ë¶€ì¡± ({ko_pct:.1f}%)")
    else:
        print(f"  âœ… ê· í˜• ì–‘í˜¸")

print("\n" + "="*60 + "\n")


# # **07 ìµœì‹  ì—°êµ¬ë™í–¥ (í•™ìˆ ì§€ ì„¹ì…˜) ì„¤ì •**

# In[20]:


# ============================================
# 7. ìµœì‹  ì—°êµ¬ë™í–¥ (í•™ìˆ ì§€ ì„¹ì…˜) ì„¤ì •
# ============================================

RESEARCH_SECTION_TITLE = "ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥"

# ì €ë„ë³„ ìµœëŒ€ ê°€ì ¸ì˜¬ ë…¼ë¬¸ ìˆ˜ (ì €ë„ 2ê°œ Ã— 250í¸ = ìµœëŒ€ 500í¸)
RESEARCH_MAX_PER_JOURNAL = 50

# ì—°êµ¬ë™í–¥ ì „ìš© ê³ ì • ì¸ë„¤ì¼
RESEARCH_THUMB = "https://img.freepik.com/premium-vector/laboratory-research-flat-style-design-vector-illustration-stock-illustration_357500-507.jpg"

# CrossRefì—ì„œ ì‚¬ìš©í•  ì €ë„ ëª©ë¡ (ISSN ê¸°ë°˜)
CROSSREF_JOURNALS = [
    {
        "journal_name": "Remote Sensing (MDPI)",
        "issn": "2072-4292",
        "logo_url": "https://www.mdpi.com/img/journals/rs-logo.png",
        "rss_url": "https://www.mdpi.com/rss/journal/remotesensing",
    },
    {
        "journal_name": "Journal of Applied Remote Sensing (SPIE)",
        "issn": "1931-3195",
        "logo_url": "https://www.spiedigitallibrary.org/by-type/journals/journal-of-applied-remote-sensing/~/media/Images/Journals/JARS/JARS_Cover.ashx",
        # SPIE RSSëŠ” í™•ì‹¤í•œ ê³µì‹ ë§í¬ê°€ ì—†ì–´ì„œ ì¼ë‹¨ ë¹„ì›Œë‘ê³  CrossRefì— ë§¡ê¹€
        "rss_url": None,
    },
]

# ============================================
# OpenAlex ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ìˆ˜ì§‘ (1ìˆœìœ„)
# ============================================

def _openalex_rebuild_abstract(inv_idx: dict) -> str:
    """
    OpenAlexì˜ abstract_inverted_indexë¥¼ í‰ë¬¸ ë¬¸ìì—´ë¡œ ë³µì›í•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜.
    inv_idx ì˜ˆì‹œ:
      {
        "remote": [0, 10],
        "sensing": [1],
        ...
      }
    â†’ ìœ„ì¹˜ ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë°°ì—´ì„ ë§Œë“¤ì–´ í•©ì¹œë‹¤.
    """
    if not inv_idx:
        return ""
    # pos â†’ token ë§¤í•‘
    pos_map = {}
    for token, positions in inv_idx.items():
        for p in positions:
            pos_map[p] = token
    if not pos_map:
        return ""
    max_pos = max(pos_map.keys())
    tokens = []
    for i in range(max_pos + 1):
        tokens.append(pos_map.get(i, ""))
    return " ".join(t for t in tokens if t).strip()


def collect_research_articles_from_openalex(
    max_per_journal: int = RESEARCH_MAX_PER_JOURNAL,
):
    """
    OpenAlex APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ë„ë³„ ìµœì‹  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    - MDPI, SPIE ë‘˜ ë‹¤ OpenAlexë¥¼ 1ìˆœìœ„ë¡œ ì‚¬ìš©
    - ë°˜í™˜ í˜•ì‹ì€ collect_research_articles_from_crossrefì™€ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤.
    """
    base_url = "https://api.openalex.org/works"
    headers = {
        "User-Agent": "InspaceNewsletterBot/1.0 (mailto:newsletter@example.com)"
    }

    collected = []

    for j in CROSSREF_JOURNALS:
        journal_name = j["journal_name"]
        issn = j["issn"]
        logo_url = j.get("logo_url", RESEARCH_THUMB)

        # OpenAlex filter:
        #  - primary_location.source.issn : ì €ë„ ISSN
        #  - from_publication_date / to_publication_date : ì´ë²ˆ ì£¼ ë‚ ì§œ ë²”ìœ„
        oa_filter = ",".join([
            f"primary_location.source.issn:{issn}",
            f"from_publication_date:{DATE_FROM}",
            f"to_publication_date:{DATE_TO}",
        ])

        params = {
            "filter": oa_filter,
            "sort": "publication_date:desc",
            "per_page": max_per_journal,
        }

        print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name} ë…¼ë¬¸ ëª©ë¡ ìˆ˜ì§‘ (ISSN={issn})")

        try:
            resp = requests.get(base_url, params=params, headers=headers, timeout=10)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name} ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        items = data.get("results") or []
        print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name} ì‘ë‹µ ë…¼ë¬¸ ìˆ˜:", len(items))

        used = 0
        for it in items[:max_per_journal]:
            # 1) ì œëª©
            original_title = (it.get("title") or it.get("display_name") or "").strip()
            if not original_title:
                continue

            # 2) ì´ˆë¡ (abstract_inverted_index ìš°ì„  ì‚¬ìš©)
            inv = it.get("abstract_inverted_index") or {}
            if inv:
                abstract_clean = _openalex_rebuild_abstract(inv)
            else:
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ OpenAlexê°€ ì œê³µí•˜ëŠ” ê°„ë‹¨ description ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
                abstract_clean = (it.get("abstract") or "").strip()

            # 3) ë§í¬ (landing_page_url â†’ pdf_url â†’ doi_url ìˆœì„œ)
            primary_loc = it.get("primary_location") or {}
            landing_url = primary_loc.get("landing_page_url")
            pdf_url = primary_loc.get("pdf_url")

            doi = it.get("doi")  # ë³´í†µ 'https://doi.org/...' í˜•íƒœì¼ ìˆ˜ë„ ìˆìŒ
            if doi and not doi.startswith("http"):
                doi_url = f"https://doi.org/{doi}"
            else:
                doi_url = doi

            click_url = landing_url or pdf_url or doi_url
            if not click_url:
                # ë§ˆì§€ë§‰ìœ¼ë¡œ OpenAlex ìì²´ ë§í¬ë¼ë„ ì‚¬ìš©
                click_url = it.get("id")

            # 4) ë°œí–‰ì¼
            published_at = (it.get("publication_date") or "")[:10]
            if not published_at:
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì´ë²ˆ ì£¼ ë ë‚ ì§œë¡œ ëŒ€ì²´
                published_at = DATE_TO

            collected.append({
                "journal_name": journal_name,
                "logo_url": logo_url,
                "original_title": original_title,
                "summary_en": abstract_clean,
                "url": click_url,          # âœ… ë‰´ìŠ¤ë ˆí„°ì—ì„œ í´ë¦­ë˜ëŠ” ë§í¬
                "published_at": published_at,
                # CrossRefì™€ í¬ë§· ë§ì¶”ê¸° ìœ„í•´ ë³´ë„ˆìŠ¤ í•„ë“œ
                "doi_url": doi_url or click_url,
                "publisher_url": landing_url or pdf_url or doi_url,
            })
            used += 1

        print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name}: ì‹¤ì œ ìˆ˜ì§‘ {used}í¸")

    # ë‚ ì§œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (CrossRef í•¨ìˆ˜ì™€ ë™ì¼í•œ ì •ë ¬ ë°©ì‹ ìœ ì§€)
    def _parse_date_safe(s):
        try:
            return datetime.strptime(s, "%Y-%m-%d")
        except Exception:
            return datetime.now()

    collected = sorted(
        collected,
        key=lambda x: _parse_date_safe(x["published_at"]),
        reverse=True,
    )

    return collected


def collect_research_articles_from_rss(max_per_journal=RESEARCH_MAX_PER_JOURNAL):
    """
    CrossRefê°€ í„°ì¡Œì„ ë•Œë¥¼ ìœ„í•œ ë°±ì—… ì†ŒìŠ¤:
    MDPI / SPIE RSSì—ì„œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì„œ
    7-2 ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    all_articles = []

    for j in CROSSREF_JOURNALS:
        rss_url = j.get("rss_url")
        if not rss_url:
            # ì´ ì €ë„ì€ RSS ì£¼ì†Œë¥¼ ì•„ì§ ì•ˆ ë„£ì—ˆìœ¼ë©´ ìŠ¤í‚µ
            continue

        print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSS ìˆ˜ì§‘ ì‹œë„: {rss_url}")
        try:
            feed = feedparser.parse(rss_url)
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

        if not feed.entries:
            print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSS ì—”íŠ¸ë¦¬ ì—†ìŒ")
            continue

        # ìµœì‹  ìˆœìœ¼ë¡œ ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì•ì—ì„œ max_per_journalê°œë§Œ ì‚¬ìš©
        for entry in feed.entries[:max_per_journal]:
            title = getattr(entry, "title", "").strip()
            link = getattr(entry, "link", "").strip()

            # pubDate â†’ YYYY-MM-DD ë¡œ ë³€í™˜ ì‹œë„
            pub_date_str = ""
            if hasattr(entry, "published"):
                try:
                    dt = dateparser.parse(entry.published)
                    if dt:
                        pub_date_str = dt.date().isoformat()
                except Exception:
                    pass

            # ìš”ì•½ (summary / description)
            summary = getattr(entry, "summary", "") or getattr(entry, "description", "") or ""
            summary = " ".join(str(summary).split())

            # 7-2 ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•„ë“œ ì´ë¦„ì— ë§ì¶°ì„œ ë¦¬í„´
            all_articles.append({
                "journal_name": j["journal_name"],
                "logo_url": j["logo_url"],
                "original_title": title,
                "summary_en": summary,
                "url": link,                # âœ… ì—¬ê¸°ì„œ ë°”ë¡œ ì €ë„ ì›ë¬¸ ë§í¬
                "published_at": pub_date_str or DATE_TO,  # ë‚ ì§œ ëª» íŒŒì‹±í•˜ë©´ ì¼ë‹¨ ì´ë²ˆ ì£¼ ë ë‚ ì§œë¡œ
            })

        print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSSì—ì„œ {min(len(feed.entries), max_per_journal)}í¸ ìˆ˜ì§‘")

    return all_articles


SYSTEM_PROMPT_RESEARCH = """
ë‹¹ì‹ ì€ ì›ê²©íƒì‚¬(Remote Sensing) / ìœ„ì„± ì˜ìƒ ë¶„ì„ ë¶„ì•¼ì˜ ì—°êµ¬ë™í–¥ì„ ì •ë¦¬í•˜ëŠ” ì—ë””í„°ì…ë‹ˆë‹¤.

ì…ë ¥ìœ¼ë¡œ í•™ìˆ  ë…¼ë¬¸ì˜ ë©”íƒ€ë°ì´í„°(ì €ë„ëª…, ì˜ë¬¸ ì œëª©, ì˜ë¬¸ ì´ˆë¡/ìš”ì•½, ë°œí–‰ì¼, ë§í¬)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ë…¼ë¬¸ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 3~4ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•˜ì„¸ìš”.
- í•œêµ­ì–´ ì œëª©ì€ ì‚¬ë‚´ ë‰´ìŠ¤ë ˆí„°ìš©ìœ¼ë¡œ, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ 30ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ê°€ëŠ¥í•œ í•œ, ë¹„ì „ë¬¸ê°€ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¶€ë“œëŸ¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

ì¶œë ¥ JSON í˜•ì‹:
{
  "title_ko": "í•œêµ­ì–´ ì œëª©",
  "summary_ko": "í•œêµ­ì–´ ìš”ì•½ (3~4ë¬¸ì¥)"
}
"""

def summarize_research_article_to_ko(meta: dict, model: str = MODEL_NAME):
    global total_tokens_used

    """
    CrossRefì—ì„œ ê°€ì ¸ì˜¨ ë…¼ë¬¸ ë©”íƒ€ ì •ë³´ë¥¼ í•œêµ­ì–´ ì œëª©/ìš”ì•½ìœ¼ë¡œ ë³€í™˜
    meta ì˜ˆì‹œ:
      {
        "journal_name": "...",
        "original_title": "...",
        "summary_en": "...",
        "published_at": "YYYY-MM-DD",
        "url": "...",
      }
    """
    user_prompt = f"""
[ì €ë„ëª…]
{meta.get('journal_name')}

[ë…¼ë¬¸ ì œëª©]
{meta.get('original_title')}

[ë…¼ë¬¸ ìš”ì•½(ì˜ë¬¸)]
{meta.get('summary_en')}

[ë°œí–‰ì¼]
{meta.get('published_at')}

[ë§í¬]
{meta.get('url')}
"""

    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": SYSTEM_PROMPT_RESEARCH},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3,
    )

    # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
    if hasattr(resp, 'usage'):
        total_tokens_used["input"] += resp.usage.input_tokens
        total_tokens_used["output"] += resp.usage.output_tokens

    text_out = resp.output[0].content[0].text
    try:
        data = json.loads(text_out)
    except Exception:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°„ë‹¨íˆ fallback
        data = {
            "title_ko": (meta.get("original_title") or "")[:30],
            "summary_ko": (meta.get("summary_en") or "")[:400],
        }
    return data

def collect_research_articles_from_crossref(
    max_per_journal: int = RESEARCH_MAX_PER_JOURNAL
):
    """
    CrossRef APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ë„ë³„ ìµœì‹  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    ë°˜í™˜: ë¦¬ìŠ¤íŠ¸[dict]
      dict = {
        "journal_name": str,
        "logo_url": str,
        "original_title": str,
        "summary_en": str,
        "url": str,
        "published_at": "YYYY-MM-DD",
      }
    """
    collected = []

    headers = {
        "User-Agent": "InspaceNewsletterBot/1.0 (mailto:newsletter@example.com)"
    }
    base_url = "https://api.crossref.org/journals"

    for j in CROSSREF_JOURNALS:
        journal_name = j["journal_name"]
        issn = j["issn"]
        logo_url = j.get("logo_url", RESEARCH_THUMB)

        api_url = f"{base_url}/{issn}/works?sort=published&order=desc&rows={max_per_journal}"
        print(f"[ì—°êµ¬ë™í–¥] {journal_name} ë…¼ë¬¸ ëª©ë¡ ìˆ˜ì§‘ (CrossRef, ISSN={issn})")

        try:
            resp = requests.get(api_url, headers=headers, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥] {journal_name} CrossRef ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        try:
            data = resp.json()
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥] {journal_name} CrossRef JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

        items = (data.get("message") or {}).get("items") or []
        print(f"[ì—°êµ¬ë™í–¥] {journal_name} CrossRef ì‘ë‹µ ë…¼ë¬¸ ìˆ˜:", len(items))

        used = 0
        for it in items[:max_per_journal]:
            title_list = it.get("title") or []
            original_title = title_list[0] if title_list else ""
            if not original_title:
                continue

            abstract_raw = it.get("abstract") or ""
            if abstract_raw:
                try:
                    abstract_clean = BeautifulSoup(abstract_raw, "html.parser").get_text(" ", strip=True)
                except Exception:
                    abstract_clean = abstract_raw
            else:
                abstract_clean = ""

            # --- (1) DOI URL (í´ë¦­ìš©) ---
            doi = it.get("DOI")
            doi_url = f"https://doi.org/{doi}" if doi else (it.get("URL") or "")

            # --- (2) publisher URL (í¬ë¡¤ë§ìš©, ìˆìœ¼ë©´ ì‚¬ìš©) ---
            publisher_url = None
            for link in it.get("link") or []:
                u = link.get("URL")
                if not u:
                    continue
                # ìš°ì„ ìˆœìœ„: ì‹¤ì œ ì €ë„ ë„ë©”ì¸
                if "spiedigitallibrary.org" in u or "mdpi.com" in u:
                    publisher_url = u
                    break
                if not publisher_url:
                    publisher_url = u

            # --- (3) ë°œí–‰ì¼ ---
            pub = it.get("published-print") or it.get("published-online") or {}
            date_parts = (pub.get("date-parts") or [[]])[0]
            if date_parts:
                year = date_parts[0]
                month = date_parts[1] if len(date_parts) > 1 else 1
                day = date_parts[2] if len(date_parts) > 2 else 1
                published_at = f"{year:04d}-{month:02d}-{day:02d}"
            else:
                published_at = datetime.now(timezone.utc).date().isoformat()

            collected.append({
                "journal_name": journal_name,
                "logo_url": logo_url,
                "original_title": original_title,
                "summary_en": abstract_clean,

                # âœ… ë‰´ìŠ¤ë ˆí„°ì—ì„œ ì‚¬ìš©ìê°€ í´ë¦­í•  ë§í¬ëŠ” DOI ì£¼ì†Œ(ë˜ëŠ” CrossRef URL)ë¡œ
                "url": doi_url,

                "published_at": published_at,

                # âœ… ë‚˜ì¤‘ì— fallback í¬ë¡¤ë§í•  ë•Œ ì“°ë ¤ê³  ë”°ë¡œ ë³´ê´€
                "doi_url": doi_url,
                "publisher_url": publisher_url,
            })
            used += 1

        print(f"[ì—°êµ¬ë™í–¥] {journal_name}: ì‹¤ì œ ìˆ˜ì§‘ {used}í¸")

    def _parse_date_safe(s):
        try:
            return datetime.strptime(s, "%Y-%m-%d")
        except Exception:
            return datetime.now()

    collected = sorted(
        collected,
        key=lambda x: _parse_date_safe(x["published_at"]),
        reverse=True,
    )
    return collected


# # **07-1 ì¸ë„¤ì¼ ì¶”ì¶œ (ê¸°ë³¸ ì¸ë„¤ì¼ í¬í•¨)**

# In[21]:


# ============================
# 7-1. ì¸ë„¤ì¼ ì¶”ì¶œ (í›„ë³´ ì˜ì—­ í•œì • + ìŠ¤ë§ˆíŠ¸ í•„í„° + canonical ì¶”ì )
# ============================
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlsplit  # urlsplit ì¶”ê°€

# ê´‘ê³ /íŠ¸ë˜í‚¹ ì „ìš© "ì˜êµ¬ ë¸”ë™ë¦¬ìŠ¤íŠ¸" (ì´ê±´ ì¸ë„¤ì¼ì´ ë  ì¼ì´ ì—†ìŒ)
ALWAYS_BAD_HOST_SNIPPETS = [
    "doubleclick.net",
    "googlesyndication.com",
    "scorecardresearch.com",
    "pixel.wp.com",
    "adservice",
    "adsystem",
    "biztoc.com",  # âœ¨ ì• ê·¸ë¦¬ê²Œì´í„° ì¸ë„¤ì¼ì€ í•­ìƒ ê¸°ë³¸ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´
]

# ì‹¤í–‰ ì¤‘ì— ìë™ìœ¼ë¡œ ìŒ“ì´ëŠ” "ë¬¸ì œ í˜¸ìŠ¤íŠ¸" ì¹´ìš´íŠ¸
HOST_FAIL_COUNTS = {}        # host -> fail count
HOST_FAIL_THRESHOLD = 3      # ì´ íšŸìˆ˜ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ê·¸ í˜¸ìŠ¤íŠ¸ëŠ” ë¬¸ì œ í˜¸ìŠ¤íŠ¸ë¡œ ê°„ì£¼

# "ì‚¬ì´ë“œë°”/ì¶”ì²œ ì˜ì—­"ìœ¼ë¡œ ê°„ì£¼í•  ë§Œí•œ í‚¤ì›Œë“œ
SIDEBAR_KEYWORDS = [
    "sidebar", "aside", "related", "recommend", "recom", "widget",
    "trending", "popular", "most-read", "mostread", "sponsor",
    "sponsored", "ads", "advert", "outbrain", "taboola",
]

# "ë³¸ë¬¸ ì˜ì—­"ìœ¼ë¡œ ì¸ì‹í•  ë§Œí•œ í‚¤ì›Œë“œ
MAIN_CONTENT_KEYWORDS = [
    "article", "post", "content", "entry", "story", "main", "body",
]


def _get_host(url: str) -> str:
    try:
        host = urlsplit(url).hostname or ""
        return host.lower()
    except Exception:
        return ""


def _mark_host_fail(host: str):
    if not host:
        return
    HOST_FAIL_COUNTS[host] = HOST_FAIL_COUNTS.get(host, 0) + 1


def is_bad_image_url(url, timeout=3):
    """
    'ì´ê±´ ì¸ë„¤ì¼ë¡œ ì“°ê¸°ì—” ìˆ˜ìƒí•˜ë‹¤' ì‹¶ì€ ì´ë¯¸ì§€ë§Œ ê³¨ë¼ì„œ ê±°ë¥¸ë‹¤.
    - ê´‘ê³ /íŠ¸ë˜í‚¹ ë„ë©”ì¸: ë¬´ì¡°ê±´ ì œì™¸
    - ê°™ì€ í˜¸ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ ë²ˆ ì‹¤íŒ¨í•˜ë©´: ìë™ìœ¼ë¡œ "ë¬¸ì œ í˜¸ìŠ¤íŠ¸"ë¡œ ê°„ì£¼
    - ê·¸ ì™¸ì—ëŠ” HEAD ê¸°ì¤€ìœ¼ë¡œ ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ / ì´ë¯¸ì§€ ì•„ë‹˜ / ì—ëŸ¬ ë“±ë§Œ ì œì™¸
    """
    if not url:
        return True

    url = url.strip()
    lower = url.lower()
    host = _get_host(url)

    # data: URI, 1x1 pixel ë“±ì€ ì œì™¸
    if lower.startswith("data:"):
        return True

    if "1x1" in lower and (".gif" in lower or ".png" in lower or ".jpg" in lower):
        return True

    # 1) ê´‘ê³ /íŠ¸ë˜í‚¹ ë„ë©”ì¸ì€ ì˜êµ¬ì ìœ¼ë¡œ ì œì™¸
    if any(sn in host for sn in ALWAYS_BAD_HOST_SNIPPETS):
        return True

    # 2) ì´ë¯¸ ì—¬ëŸ¬ ë²ˆ ì‹¤íŒ¨í•œ "ë¬¸ì œ í˜¸ìŠ¤íŠ¸"ëŠ” ë¹ ë¥´ê²Œ ì œì™¸
    if HOST_FAIL_COUNTS.get(host, 0) >= HOST_FAIL_THRESHOLD:
        return True

    # 3) ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì¼ì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ íŒë‹¨
    path = urlsplit(url).path.lower()
    looks_like_image = path.endswith((".jpg", ".jpeg", ".png", ".webp", ".gif"))

    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True)
        status = resp.status_code

        # HEADê°€ ì•„ì˜ˆ ì‹¤íŒ¨ê±°ë‚˜ 4xx/5xxë©´ ì¼ë‹¨ ì´ URLì€ ë‚˜ì¨
        # (ì´ ê²½ìš°ëŠ” í˜¸ìŠ¤íŠ¸ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ì— ë°˜ì˜)
        if status < 200 or status >= 400:
            _mark_host_fail(host)
            # í˜¹ì‹œ 403/405/406ì¸ë° ì´ë¯¸ì§€ì²˜ëŸ¼ ë³´ì´ëŠ” ê²½ìš°ëŠ”
            # "ì´ í™˜ê²½ì—ì„œë§Œ ë§‰íŒ ê²ƒ"ì¼ ìˆ˜ ìˆì–´ì„œ, í•œ ë²ˆì€ ë´ì¤€ë‹¤.
            if looks_like_image and status in (403, 405, 406):
                return False
            return True

        ctype = resp.headers.get("content-type", "").lower()

        # content-typeì— image ê°€ ì—†ê³ , í™•ì‹¤íˆ ì´ë¯¸ì§€ í™•ì¥ìë„ ì•„ë‹ˆë©´ ì œì™¸
        if "image" not in ctype and not looks_like_image:
            _mark_host_fail(host)
            return True

        # content-typeì´ ë¹„ì–´ ìˆëŠ”ë° í™•ì‹¤íˆ ì´ë¯¸ì§€ í™•ì¥ìì¸ ê²½ìš°ëŠ” í—ˆìš©
        if not ctype and looks_like_image:
            return False

        # ë„ˆë¬´ ì‘ì€ content-length (í”½ì…€/ì•„ì´ì½˜ ë“±)ëŠ” ì œì™¸
        clen_header = resp.headers.get("content-length")
        try:
            clen = int(clen_header) if clen_header else 0
        except ValueError:
            clen = 0

        if 0 < clen < 4000:  # 4KB ë¯¸ë§Œ
            return True

        if lower.endswith(".gif") and 0 < clen < 8000:
            return True

    except Exception:
        # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë“±ì€ host failë¡œ ê°„ì£¼
        _mark_host_fail(host)
        # ì´ë¯¸ì§€ì²˜ëŸ¼ ìƒê¸´ URLì´ë©´ í•œ ë²ˆì€ ë´ì£¼ê³ , ì´í›„ ì¹´ìš´íŠ¸ ëˆ„ì ë˜ë©´ ìë™ ì°¨ë‹¨
        return not looks_like_image

    return False


def is_sidebar_like(tag):
    """
    ì£¼ì–´ì§„ íƒœê·¸(ë˜ëŠ” ìƒìœ„ íŠ¸ë¦¬)ê°€ sidebar/related/recommend ì˜ì—­ì¸ì§€ íŒë‹¨
    """
    cur = tag
    depth = 0
    while cur is not None and depth < 6:  # ë„ˆë¬´ ê¹Šì´ ì•ˆ ì˜¬ë¼ê°€ë„ ë¨
        cid = (cur.get("id") or "").lower()
        cls = " ".join(cur.get("class", [])).lower()

        text = cid + " " + cls
        if any(key in text for key in SIDEBAR_KEYWORDS):
            return True

        cur = cur.parent
        depth += 1
    return False


def is_main_content_like(tag):
    """
    ì£¼ì–´ì§„ íƒœê·¸(ë˜ëŠ” ìƒìœ„ íŠ¸ë¦¬)ê°€ ë³¸ë¬¸(article/main/content) ì˜ì—­ì¸ì§€ ëŒ€ëµ íŒë‹¨
    """
    cur = tag
    depth = 0
    while cur is not None and depth < 6:
        cid = (cur.get("id") or "").lower()
        cls = " ".join(cur.get("class", [])).lower()
        text = cid + " " + cls

        if any(key in text for key in MAIN_CONTENT_KEYWORDS):
            return True

        cur = cur.parent
        depth += 1
    return False


def extract_image_candidates(page_url, html):
    """
    í˜ì´ì§€ì—ì„œ ì¸ë„¤ì¼ í›„ë³´ë¥¼ ëª¨ìœ¼ë˜,
    - 1ìˆœìœ„: og:image / twitter:image / ê¸°íƒ€ meta
    - 2ìˆœìœ„: "ë³¸ë¬¸ ì˜ì—­(article/main/content ë“±)" ë‚´ë¶€ì˜ <img> (sidebar/related ì œì™¸)
    - 3ìˆœìœ„: (ë³¸ë¬¸ ì˜ì—­ì„ ëª» ì°¾ì€ ê²½ìš°ì—ë§Œ) í˜ì´ì§€ ì „ì²´ <img>ì—ì„œ sidebar/related ì œì™¸ í›„ ì‚¬ìš©
    """
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    def add_candidate(src, score):
        if not src:
            return
        src = src.strip()
        if not src or src.startswith("data:"):
            return
        full = urljoin(page_url, src)
        candidates.append((score, full))

    # 1) og:image, twitter:image ë“± ë©”íƒ€ íƒœê·¸ ìš°ì„ 
    for meta in soup.find_all("meta"):
        prop = (meta.get("property") or "").lower()
        name = (meta.get("name") or "").lower()
        content = meta.get("content") or ""

        if prop in ("og:image", "og:image:url", "og:image:secure_url"):
            add_candidate(content, 100)
        elif name in ("twitter:image", "twitter:image:src"):
            add_candidate(content, 95)
        elif name == "image" or prop == "image":
            add_candidate(content, 90)

    # 2) "ë³¸ë¬¸ ì˜ì—­" í›„ë³´ ì°¾ê¸° (article/main/ë‚´ìš© div ë“±)
    content_areas = []

    # ëŒ€í‘œì ì¸ ë³¸ë¬¸ íƒœê·¸ë“¤
    for tag in soup.find_all(["article", "main", "section", "div"]):
        cid = (tag.get("id") or "").lower()
        cls = " ".join(tag.get("class", [])).lower()
        text = cid + " " + cls

        # MAIN_CONTENT_KEYWORDS ê¸°ì¤€ìœ¼ë¡œ ë³¸ë¬¸ì²˜ëŸ¼ ë³´ì´ëŠ” ì˜ì—­ë§Œ ì„ íƒ
        if any(k in text for k in MAIN_CONTENT_KEYWORDS):
            content_areas.append(tag)

    # ë³¸ë¬¸ ì˜ì—­ì„ í•˜ë‚˜ë„ ëª» ì°¾ìœ¼ë©´, fallbackìœ¼ë¡œ body(or ì „ì²´)ë¥¼ ì‚¬ìš©
    if not content_areas:
        if soup.body:
            content_areas = [soup.body]
        else:
            content_areas = [soup]

    # 3) ë³¸ë¬¸ ì˜ì—­ ë‚´ë¶€ì˜ <img>ë§Œ í›„ë³´ë¡œ ì‚¬ìš© (sidebar-like ì œì™¸)
    for area in content_areas:
        for img in area.find_all("img"):
            src = img.get("src") or ""
            if not src or src.startswith("data:"):
                continue

            # sidebar/related/recommend ì˜ì—­ì´ë©´ ì œì™¸
            if is_sidebar_like(img):
                continue

            score = 40  # ê¸°ë³¸ ì ìˆ˜

            # ë³¸ë¬¸(article/main/content) ìª½ì´ë©´ ì ìˆ˜ ê°€ì‚°
            if is_main_content_like(img):
                score += 20

            # width/heightë¡œ ëŒ€ì¶© í° ì´ë¯¸ì§€ì— ë³´ë„ˆìŠ¤
            w = img.get("width")
            h = img.get("height")
            try:
                w = int(w)
                h = int(h)
                if w >= 200 and h >= 150:
                    score += 10
            except Exception:
                pass

            # classì— hero/featured/main ë“±ì´ ìˆìœ¼ë©´ ì¶”ê°€ ê°€ì‚°
            cls = " ".join(img.get("class", [])).lower()
            if any(x in cls for x in ["hero", "featured", "main", "headline"]):
                score += 10

            add_candidate(src, score)

    # (ì˜µì…˜) ê·¸ë˜ë„ í›„ë³´ê°€ ì „í˜€ ì—†ìœ¼ë©´, ë§ˆì§€ë§‰ ìˆ˜ë‹¨ìœ¼ë¡œ í˜ì´ì§€ ì „ì²´ <img>ì—ì„œ sidebar ì œì™¸ í›„ ì‚¬ìš©
    if not candidates:
        for img in soup.find_all("img"):
            src = img.get("src") or ""
            if not src or src.startswith("data:"):
                continue
            if is_sidebar_like(img):
                continue
            add_candidate(src, 20)

    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ + ì¤‘ë³µ ì œê±°
    seen = set()
    unique = []
    for score, url in sorted(candidates, key=lambda x: x[0], reverse=True):
        if url in seen:
            continue
        seen.add(url)
        unique.append((score, url))

    return unique


def _maybe_follow_canonical(page_url, html, timeout=6, headers=None):
    """
    ì• ê·¸ë¦¬ê²Œì´í„°(biztoc ë“±) íŠ¹ì´ ì¼€ì´ìŠ¤ ì¡ê¸°:
    - í˜ì´ì§€ ì•ˆì— <link rel="canonical" href="..."> ê°€ ìˆê³ 
    - ê·¸ hostê°€ ê¸°ì¡´ page_url ê³¼ ë‹¤ë¥´ë©´
      â†’ ì›ë³¸ ê¸°ì‚¬ë¡œ ê°„ì£¼í•˜ê³  ê·¸ ìª½ HTMLì„ ë‹¤ì‹œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
    """
    try:
        soup = BeautifulSoup(html, "html.parser")
        link = soup.find("link", rel=lambda v: v and "canonical" in v.lower())
        if not link:
            return page_url, html

        href = (link.get("href") or "").strip()
        if not href:
            return page_url, html

        canonical_url = urljoin(page_url, href)
        orig_host = _get_host(page_url)
        canon_host = _get_host(canonical_url)

        # hostê°€ ë‹¤ë¥´ê³ , canonical ì´ ì¡´ì¬í•˜ë©´ ì›ë³¸ ê¸°ì‚¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        if canon_host and orig_host and canon_host != orig_host:
            try:
                resp2 = requests.get(
                    canonical_url,
                    timeout=timeout,
                    headers=headers or {}
                )
                if resp2.status_code == 200 and resp2.text:
                    # ì›ë³¸ ê¸°ì‚¬ HTMLë¡œ êµì²´
                    return canonical_url, resp2.text
            except Exception:
                # canonical ë”°ë¼ê°€ë‹¤ ì‹¤íŒ¨í•˜ë©´ ì›ë˜ ê²ƒ ìœ ì§€
                return page_url, html
    except Exception:
        return page_url, html

    return page_url, html


def fetch_thumbnail(url, timeout=6):
    """
    ë‰´ìŠ¤ URLì—ì„œ ì¸ë„¤ì¼ ì¶”ì¶œ:
    - ë©”íƒ€ íƒœê·¸ ê¸°ë°˜ í›„ë³´ + ë³¸ë¬¸ ì˜ì—­ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©
    - sidebar/related ì˜ì—­ì€ ì•„ì˜ˆ í›„ë³´ì—ì„œ ì œì™¸
    - is_bad_image_url ë¡œ 1ì°¨ í•„í„°
    - ì• ê·¸ë¦¬ê²Œì´í„°ì¸ ê²½ìš° canonical ì„ ë”°ë¼ê°€ì„œ ì›ë³¸ ê¸°ì‚¬ì—ì„œ ë‹¤ì‹œ ì¶”ì¶œ
    """
    if not url:
        return DEFAULT_THUMB

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0 Safari/537.36"
        )
    }

    try:
        resp = requests.get(url, timeout=timeout, headers=headers)
        if resp.status_code != 200:
            return DEFAULT_THUMB
    except Exception:
        return DEFAULT_THUMB

    html = resp.text

    # ğŸ”¹ ì—¬ê¸°ì„œ canonical ì„ í•œ ë²ˆ ë”°ë¼ê°€ ë³¸ë‹¤ (ì• ê·¸ë¦¬ê²Œì´í„° íŠ¹ì´ ì¼€ì´ìŠ¤ìš©)
    page_for_image, html_for_image = _maybe_follow_canonical(
        url, html, timeout=timeout, headers=headers
    )

    candidates = extract_image_candidates(page_for_image, html_for_image)

    for score, img_url in candidates:
        try:
            if not is_bad_image_url(img_url):
                return img_url
        except Exception:
            continue

    # ì ì ˆí•œ í›„ë³´ê°€ í•˜ë‚˜ë„ ì—†ìœ¼ë©´ ê¸°ë³¸ ì¸ë„¤ì¼
    return DEFAULT_THUMB


# ë©”ì¸ + ì¶”ê°€ ê¸°ì‚¬ ëª¨ë‘ì— ëŒ€í•´ ì¸ë„¤ì¼ ì±„ìš°ê¸° (ë³‘ë ¬ ì²˜ë¦¬)
print("\nì¸ë„¤ì¼ ì¶”ì¶œ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")

def fetch_thumbnail_for_article(article_data):
    """
    ë‹¨ì¼ ê¸°ì‚¬ì˜ ì¸ë„¤ì¼ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    topic_num, art = article_data
    news_url = art.get("url", "")
    try:
        thumb = fetch_thumbnail(news_url)
        art["thumbnail_url"] = thumb or DEFAULT_THUMB
        return True
    except Exception as e:
        art["thumbnail_url"] = DEFAULT_THUMB
        return False

# ëª¨ë“  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
all_articles = []
for topic_num in topic_main_articles:
    for art in topic_main_articles[topic_num]:
        all_articles.append((topic_num, art))

for topic_num in topic_extra_articles:
    for art in topic_extra_articles[topic_num]:
        all_articles.append((topic_num, art))

# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
total_articles = len(all_articles)
print(f"ì´ {total_articles}ê°œ ê¸°ì‚¬ì˜ ì¸ë„¤ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (ë™ì‹œ ì²˜ë¦¬: 15ê°œ)...")

with ThreadPoolExecutor(max_workers=20) as executor:
    futures = {executor.submit(fetch_thumbnail_for_article, art_data): idx
               for idx, art_data in enumerate(all_articles)}

    completed = 0
    success = 0
    for future in as_completed(futures):
        if future.result():
            success += 1
        completed += 1

        # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ê°œë§ˆë‹¤)
        if completed % 10 == 0:
            print(f"  ì§„í–‰: {completed}/{total_articles} ({completed*100//total_articles}%) - ì„±ê³µ: {success}ê°œ")

print(f"\nì¸ë„¤ì¼ ì¶”ì¶œ ì™„ë£Œ! ì„±ê³µ: {success}/{total_articles}ê°œ")
print("(ë³¸ë¬¸ ì˜ì—­ ìœ„ì£¼ + sidebar/related ì œì™¸ + ìŠ¤ë§ˆíŠ¸ í•„í„° + canonical ì¶”ì )")


# # **07-2 ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€**

# In[22]:


# ============================================
# 7-2. ìµœì‹  ì—°êµ¬ë™í–¥(í•™ìˆ ì§€) ìˆ˜ì§‘ & ìš”ì•½
# ============================================

def clean_and_shorten_summary(text, max_chars=220):
    """
    CrossRefì—ì„œ ê°€ì ¸ì˜¨ ì˜ì–´ ì´ˆë¡ì„ ì •ë¦¬í•˜ê³ ,
    ë„ˆë¬´ ê¸¸ë©´ max_chars ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì„œ '... 'ì„ ë¶™ì—¬ì¤€ë‹¤.
    """
    if not text:
        return ""

    # ì¤„ë°”ê¿ˆ/ê³µë°± ì •ë¦¬
    text = " ".join(str(text).split())

    # ì´ë¯¸ ì¶©ë¶„íˆ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if len(text) <= max_chars:
        return text

    # ê¸€ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ë˜, ë‹¨ì–´ ì¤‘ê°„ì—ì„œ ëŠê¸°ì§€ ì•Šê²Œ ë§ˆì§€ë§‰ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
    cut = text[:max_chars]
    last_space = cut.rfind(" ")
    if last_space > 0:
        cut = cut[:last_space]

    return cut + "..."

def fetch_publisher_abstract(url, timeout=8):
    """
    CrossRef ìš”ì•½ì´ ë„ˆë¬´ ì§§ì„ ë•Œ, ì›ë¬¸ í˜ì´ì§€ì—ì„œ abstractë¥¼ í•œ ë²ˆ ë” ë½‘ì•„ë³´ëŠ” í•¨ìˆ˜.
    - ì‹¤íŒ¨í•˜ë©´ ""ì„ ë°˜í™˜ (ì ˆëŒ€ ì˜ˆì™¸ë¥¼ ìœ„ë¡œ ì˜¬ë¦¬ì§€ ì•ŠìŒ)
    """
    if not url:
        return ""

    # SPIE ê°™ì€ ë°ì„œ 403 ë§‰ëŠ” ê±¸ í”¼í•˜ë ¤ê³  ë¸Œë¼ìš°ì € User-Agent ì„¸íŒ…
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9",
    }

    try:
        resp = requests.get(url, timeout=timeout, headers=headers)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ì—°êµ¬ë™í–¥] fallback ìš”ì•½ ìˆ˜ì§‘ ì‹¤íŒ¨({url}): {e}")
        return ""

    html = resp.text
    soup = BeautifulSoup(html, "html.parser")

    # ì‚¬ì´íŠ¸ë§ˆë‹¤ êµ¬ì¡°ê°€ ë‹¤ë¥´ì§€ë§Œ, ê°€ì¥ í”í•œ abstract ì˜ì—­ ëª‡ êµ°ë°ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„
    selectors = [
        "section.abstract",
        "section#abstract",
        "div.abstract",
        "div#abstract",
        "div.article__body",
    ]

    abstract_el = None
    for sel in selectors:
        abstract_el = soup.select_one(sel)
        if abstract_el:
            break

    if not abstract_el:
        return ""

    text = " ".join(abstract_el.get_text(" ", strip=True).split())
    return text


def fetch_article_abstract_fallback(url, timeout=15):
    """
    CrossRef ìš”ì•½ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ì„ ë•Œ,
    ì‹¤ì œ ì €ë„ í˜ì´ì§€(SPIE, MDPI ë“±)ì— ë“¤ì–´ê°€ì„œ abstract/descriptionì„ ìµœëŒ€í•œ ë½‘ì•„ì˜¤ëŠ” í•¨ìˆ˜.
    """
    if not url:
        return ""

    try:
        headers = {
            "User-Agent": "InspaceNewsletterBot/1.0 (mailto:newsletter@example.com)"
        }
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ì—°êµ¬ë™í–¥] fallback ìš”ì•½ ìˆ˜ì§‘ ì‹¤íŒ¨({url}): {e}")
        return ""

    soup = BeautifulSoup(resp.text, "html.parser")

    # 1) meta íƒœê·¸ ìª½ì—ì„œ abstract/description ìš°ì„  ì‹œë„
    for meta in soup.find_all("meta"):
        name = (meta.get("name") or "").lower()
        prop = (meta.get("property") or "").lower()
        content = (meta.get("content") or "").strip()

        if not content:
            continue

        # SPIE/ì €ë„ ì‚¬ì´íŠ¸ë“¤ì´ ìì£¼ ì“°ëŠ” íŒ¨í„´ë“¤
        if name in ["description", "dc.description", "citation_abstract"] or prop in ["og:description"]:
            content = " ".join(content.split())
            if len(content) > 40:
                return content

    # 2) 'abstract' ê´€ë ¨ ë¸”ë¡ì„ ì§ì ‘ ì°¾ì•„ë³´ê¸°
    candidates = []
    selectors = [
        "div.abstract",
        "section.abstract",
        "div#abstract",
        "section#abstract",
        "div.article__abstract",
    ]
    for sel in selectors:
        for node in soup.select(sel):
            text = " ".join(node.get_text(" ", strip=True).split())
            if len(text) > 40:
                candidates.append(text)

    if candidates:
        # ê¸¸ì´ê°€ ì œì¼ ê¸´ ê±¸ í•˜ë‚˜ ê³ ë¦„
        return max(candidates, key=len)

    # 3) ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´, ë³¸ë¬¸ <p> ì¤‘ ì•ì˜ ëª‡ ê°œë¥¼ ì´ì–´ë¶™ì—¬ì„œ pseudo-abstractë¡œ ì‚¬ìš©
    paras = []
    for p in soup.find_all("p"):
        t = " ".join(p.get_text(" ", strip=True).split())
        if len(t) < 40:
            continue
        paras.append(t)
        if len(paras) >= 3:
            break

    return " ".join(paras)

print("\n=== [7-2 ë‹¨ê³„] ìµœì‹  ì—°êµ¬ë™í–¥(í•™ìˆ ì§€) ìˆ˜ì§‘ & ìš”ì•½ ===")

# 1ìˆœìœ„: OpenAlex ê¸°ë°˜ ë…¼ë¬¸ ë©”íƒ€ ìˆ˜ì§‘
research_raw_articles = collect_research_articles_from_openalex(
    max_per_journal=RESEARCH_MAX_PER_JOURNAL
)
print("OpenAlex ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))

# OpenAlexì—ì„œ ì•„ë¬´ê²ƒë„ ëª» ê°€ì ¸ì™”ì„ ë•Œë§Œ CrossRefë¡œ í´ë°±
if not research_raw_articles:
    print("[ì—°êµ¬ë™í–¥] OpenAlex ê²°ê³¼ ì—†ìŒ â†’ CrossRefë¡œ í´ë°± ì‹œë„")
    research_raw_articles = collect_research_articles_from_crossref(
        max_per_journal=RESEARCH_MAX_PER_JOURNAL
    )
    print("CrossRef ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))

# (ì„ íƒ) ê·¸ë˜ë„ 0ê°œë©´, ë§ˆì§€ë§‰ìœ¼ë¡œ RSSê¹Œì§€ ì¨ë³´ê³  ì‹¶ë‹¤ë©´:
if not research_raw_articles:
    print("[ì—°êµ¬ë™í–¥] OpenAlex + CrossRef ëª¨ë‘ ì‹¤íŒ¨ â†’ RSSë¡œ í´ë°± ì‹œë„")
    research_raw_articles = collect_research_articles_from_rss(
        max_per_journal=RESEARCH_MAX_PER_JOURNAL
    )
    print("RSS ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))


# 2) CrossRef ê²°ê³¼ê°€ 0í¸ì´ë©´ â†’ RSS ë°±ì—… ì†ŒìŠ¤ ì‚¬ìš©
if not research_raw_articles:
    print("[ì—°êµ¬ë™í–¥] CrossRef ê²°ê³¼ 0í¸ â†’ RSS ë°±ì—… ì†ŒìŠ¤ ì‹œë„")
    research_raw_articles = collect_research_articles_from_rss(
        max_per_journal=RESEARCH_MAX_PER_JOURNAL
    )
    print("RSS ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))


research_processed_articles = []

# for meta in research_raw_articles:
#     # 1) CrossRefì—ì„œ ê°€ì ¸ì˜¨ ì˜ì–´ ìš”ì•½ ë¨¼ì € êº¼ë‚´ê¸°
#     raw_summary_en = (meta.get("summary_en") or "").strip()

#     # ìš”ì•½ì´ ì™„ì „íˆ ë¹„ì—ˆëŠ”ì§€ ì—¬ë¶€
#     has_any_crossref_text = bool(raw_summary_en)

#     # ë„ˆë¬´ ì§§ê±°ë‚˜(= ê±°ì˜ ì—†ìŒ) ì•„ì˜ˆ ì—†ì–´ë„ í•œ ë²ˆì€ ì›ë¬¸ í˜ì´ì§€ì—ì„œ ë³´ì¶© ì‹œë„
#     MIN_LEN_FOR_FALLBACK = 80  # í•„ìš”í•˜ë©´ 60~100 ì‚¬ì´ì—ì„œ ì¡°ì ˆ ê°€ëŠ¥

#     fallback_summary = ""
#     if len(raw_summary_en) < MIN_LEN_FOR_FALLBACK:
#         target_url = meta.get("publisher_url") or meta.get("url")
#         print(
#             f"  - [fallback ì‹œë„] CrossRef ìš”ì•½ ë¶€ì¡± â†’ ì›ë¬¸ í˜ì´ì§€ íŒŒì‹± ì‹œë„: "
#             f"{meta.get('original_title')} | {target_url}"
#         )
#         fallback_summary = fetch_publisher_abstract(target_url)



#     # 2ë‹¨ê³„: fallback ìš”ì•½ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ CrossRef ìš”ì•½ ì‚¬ìš©
#     final_summary_en = (fallback_summary or raw_summary_en).strip()

#     # 3ë‹¨ê³„: ê·¸ë˜ë„ ë‘˜ ë‹¤ ë¹„ì–´ ìˆìœ¼ë©´, ìŠ¤í‚µí•˜ì§€ ë§ê³  ìµœì†Œ ì„¤ëª…ìœ¼ë¡œ ì‚´ë ¤ë‘ê¸°
#     if not final_summary_en:
#         title = (meta.get("original_title") or "").strip()
#         journal = (meta.get("journal_name") or "").strip()
#         final_summary_en = (
#             f"No abstract available. This paper ({title}) was published in {journal}."
#         )



#     # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ final_summary_enì—ëŠ” ë­”ê°€ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ
#     raw_summary_en = final_summary_en

#     # 3) ìš”ì•½ ê¸¸ì´ ì •ë¦¬ (ì¤„ë°”ê¿ˆ ì œê±° + 1~2ì¤„ ì •ë„ë¡œ ìë¥´ê¸°)
#     short_summary_en = clean_and_shorten_summary(raw_summary_en, max_chars=200)

#     # 4) í•œê¸€ ìš”ì•½ (GPT)
#     try:
#         ko_data = summarize_research_article_to_ko({
#             **meta,
#             "summary_en": short_summary_en,  # ì •ë¦¬ëœ ì˜ì–´ ìš”ì•½ ê¸°ì¤€ìœ¼ë¡œ ì „ë‹¬
#         })
#     except Exception as e:
#         print(f"[ì—°êµ¬ë™í–¥ ìš”ì•½ ì˜¤ë¥˜] {meta.get('url')}: {e}")
#         ko_data = {}

#     thumb_url = RESEARCH_THUMB

#     research_processed_articles.append({
#         "journal_name": meta["journal_name"],
#         "logo_url": meta["logo_url"],
#         "original_title": meta["original_title"],
#         "summary_en": short_summary_en,
#         "url": meta["url"],
#         "published_at": meta["published_at"],
#         "title_ko": ko_data.get("title_ko", meta["original_title"][:30]),
#         "summary_ko": ko_data.get("summary_ko", short_summary_en),
#         "thumbnail_url": thumb_url,
#     })
# ì—°êµ¬ ë…¼ë¬¸ ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
def process_single_research_article(meta):
    """
    ë‹¨ì¼ ì—°êµ¬ ë…¼ë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # 1) CrossRefì—ì„œ ê°€ì ¸ì˜¨ ì˜ì–´ ìš”ì•½ ë¨¼ì € êº¼ë‚´ê¸°
        raw_summary_en = (meta.get("summary_en") or "").strip()

        # ìš”ì•½ì´ ì™„ì „íˆ ë¹„ì—ˆëŠ”ì§€ ì—¬ë¶€
        has_any_crossref_text = bool(raw_summary_en)

        # ë„ˆë¬´ ì§§ê±°ë‚˜(= ê±°ì˜ ì—†ìŒ) ì•„ì˜ˆ ì—†ì–´ë„ í•œ ë²ˆì€ ì›ë¬¸ í˜ì´ì§€ì—ì„œ ë³´ì¶© ì‹œë„
        MIN_LEN_FOR_FALLBACK = 80

        fallback_summary = ""
        if len(raw_summary_en) < MIN_LEN_FOR_FALLBACK:
            target_url = meta.get("publisher_url") or meta.get("url")
            print(
                f"  - [fallback ì‹œë„] CrossRef ìš”ì•½ ë¶€ì¡± â†’ ì›ë¬¸ í˜ì´ì§€ íŒŒì‹± ì‹œë„: "
                f"{meta.get('original_title')} | {target_url}"
            )
            fallback_summary = fetch_publisher_abstract(target_url)

        # 2ë‹¨ê³„: fallback ìš”ì•½ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ CrossRef ìš”ì•½ ì‚¬ìš©
        final_summary_en = (fallback_summary or raw_summary_en).strip()

        # 3ë‹¨ê³„: ê·¸ë˜ë„ ë‘˜ ë‹¤ ë¹„ì–´ ìˆìœ¼ë©´, ìŠ¤í‚µí•˜ì§€ ë§ê³  ìµœì†Œ ì„¤ëª…ìœ¼ë¡œ ì‚´ë ¤ë‘ê¸°
        if not final_summary_en:
            title = (meta.get("original_title") or "").strip()
            journal = (meta.get("journal_name") or "").strip()
            final_summary_en = (
                f"No abstract available. This paper ({title}) was published in {journal}."
            )

        # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ final_summary_enì—ëŠ” ë­”ê°€ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ
        raw_summary_en = final_summary_en

        # 3) ìš”ì•½ ê¸¸ì´ ì •ë¦¬
        short_summary_en = clean_and_shorten_summary(raw_summary_en, max_chars=200)

        # 4) í•œê¸€ ìš”ì•½ (GPT)
        try:
            ko_data = summarize_research_article_to_ko({
                **meta,
                "summary_en": short_summary_en,
            })
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥ ìš”ì•½ ì˜¤ë¥˜] {meta.get('url')}: {e}")
            ko_data = {}

        thumb_url = RESEARCH_THUMB

        return {
            "journal_name": meta["journal_name"],
            "logo_url": meta["logo_url"],
            "original_title": meta["original_title"],
            "summary_en": short_summary_en,
            "url": meta["url"],
            "published_at": meta["published_at"],
            "title_ko": ko_data.get("title_ko", meta["original_title"][:30]),
            "summary_ko": ko_data.get("summary_ko", short_summary_en),
            "thumbnail_url": thumb_url,
        }
    except Exception as e:
        print(f"[ì—°êµ¬ë™í–¥ ì²˜ë¦¬ ì˜¤ë¥˜] {meta.get('url')}: {e}")
        return None


# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
print(f"\nì´ {len(research_raw_articles)}ê°œ ì—°êµ¬ ë…¼ë¬¸ì„ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë™ì‹œ ì²˜ë¦¬: 20ê°œ)...")
research_processed_articles = []

with ThreadPoolExecutor(max_workers=20) as executor:
    futures = {executor.submit(process_single_research_article, meta): idx
               for idx, meta in enumerate(research_raw_articles)}

    completed = 0
    for future in as_completed(futures):
        result = future.result()
        if result:
            research_processed_articles.append(result)

        completed += 1
        if completed % 5 == 0:
            print(f"  ì§„í–‰: {completed}/{len(research_raw_articles)} ({completed*100//len(research_raw_articles)}%) - ì„±ê³µ: {len(research_processed_articles)}ê°œ")

print(f"\nì—°êµ¬ ë…¼ë¬¸ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(research_processed_articles)}ê°œ ë…¼ë¬¸ ì²˜ë¦¬ë¨")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ì—¬ê¸°ë¶€í„° ìƒˆë¡œ ì¶”ê°€: ì—°êµ¬ë™í–¥ ë‚ ì§œ í•„í„°
#    (ë‰´ìŠ¤ ì„¹ì…˜ì— ì“°ëŠ” DATE_FROM ~ DATE_TOì™€ ë§ì¶”ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from_dt = datetime.fromisoformat(DATE_FROM).date()
    to_dt = datetime.fromisoformat(DATE_TO).date()
except Exception as e:
    print("[ì—°êµ¬ë™í–¥] DATE_FROM/DATE_TO íŒŒì‹± ì˜¤ë¥˜, í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤:", e)
else:
    def _parse_pubdate_safe(s: str):
        try:
            # CrossRefì—ì„œ ë§Œë“  published_atì€ 'YYYY-MM-DD' í˜•ì‹
            return datetime.strptime(s, "%Y-%m-%d").date()
        except Exception:
            return None

    filtered = []
    for art in research_processed_articles:
        d = _parse_pubdate_safe(art.get("published_at", ""))
        if d is None:
            continue  # ë‚ ì§œ ì´ìƒí•œ ê±´ ë²„ë¦¼
        if from_dt <= d <= to_dt:
            filtered.append(art)

    print(f"[ì—°êµ¬ë™í–¥] ë‚ ì§œ í•„í„° ì ìš© ì „: {len(research_processed_articles)}í¸")
    print(f"[ì—°êµ¬ë™í–¥] {DATE_FROM} ~ {DATE_TO} ë²”ìœ„ ë‚´: {len(filtered)}í¸")

    research_processed_articles = filtered

    # (ì˜µì…˜) ë‹¤ì‹œ ë‚ ì§œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    research_processed_articles = sorted(
        research_processed_articles,
        key=lambda x: datetime.strptime(x["published_at"], "%Y-%m-%d"),
        reverse=True,
    )

# ë©”ì¸ 3ê°œ + ì¶”ê°€ í•™ìˆ ì§€ + (ì¶”ê°€ í˜ì´ì§€ìš©) ë¦¬ìŠ¤íŠ¸ ë¶„ë¦¬
research_main_articles = []
research_extra_articles = []
research_more_articles = []

if research_processed_articles:
    # 1) ë©”ì¸ ì„¹ì…˜ì— ë³´ì—¬ì¤„ 3ê°œ
    research_main_articles = research_processed_articles[:3]

    # 2) ë©”ì¸ 3ê°œ ì´í›„ë¥¼ 'ì¶”ê°€ í•™ìˆ ì§€'ë¡œ ì‚¬ìš©
    if len(research_processed_articles) > 3:
        research_extra_articles = research_processed_articles[3:]
        # 3) ì¶”ê°€ ì—°êµ¬ë™í–¥ í˜ì´ì§€ì—ëŠ”
        #    "ë©”ì¸ 3ê°œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì „ë¶€"ë§Œ ë„£ê¸°
        research_more_articles = research_extra_articles[:]   # âœ… ì—¬ê¸°ë§Œ ë³€ê²½
    else:
        research_extra_articles = []
        research_more_articles = []

    print("ì—°êµ¬ë™í–¥ ë©”ì¸ í•™ìˆ ì§€ ê°œìˆ˜:", len(research_main_articles))
    print("ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ ê°œìˆ˜:", len(research_extra_articles))
else:
    print("[ì•Œë¦¼] ì—°êµ¬ë™í–¥ (CrossRef)ì—ì„œ ê°€ì ¸ì˜¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")


# 
# # **08 ì¹´ë“œ/ì„¹ì…˜ HTML + ìµœì¢… ë‰´ìŠ¤ë ˆí„° HTML ìƒì„±**

# In[59]:


# ============================
# 8. ì¹´ë“œ/ì„¹ì…˜ HTML + ë”ë³´ê¸° í˜ì´ì§€ + ìµœì¢… ë‰´ìŠ¤ë ˆí„° HTML
# ============================

# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ ì „ìš© í˜ì´ì§€ íŒŒì¼ëª…
# (ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë°”ê¿”ë„ ë˜ëŠ”ë°, ë©”ì¸ ë²„íŠ¼ ë§í¬ì™€ ê°™ì´ ì¨ì•¼ í•¨)
TOPIC_MORE_FILENAMES = {
    1: "more_geoint.html",        # GeoINT ì „ìš© ì¶”ê°€ ê¸°ì‚¬
    2: "more_aviation.html",      # í•­ê³µ/ë“œë¡  ì „ìš© ì¶”ê°€ ê¸°ì‚¬
    3: "more_ai_platform.html",   # AI ë°ì´í„° í”Œë«í¼ ì „ìš© ì¶”ê°€ ê¸°ì‚¬
    4: "more_satellite.html",     # ìœ„ì„± ì˜ìƒ ì „ìš© ì¶”ê°€ ê¸°ì‚¬
}

# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ í—¤ë” íƒ€ì´í‹€
TOPIC_MORE_TITLES = {
    1: "GeoINT - ì¶”ê°€ ê¸°ì‚¬",
    2: "í•­ê³µ / ë“œë¡ Â·ë¬´ì¸ê¸°(UAV/UAS) - ì¶”ê°€ ê¸°ì‚¬",
    3: "AI ë°ì´í„° í”Œë«í¼ - ì¶”ê°€ ê¸°ì‚¬",
    4: "ìœ„ì„± ì˜ìƒ - ì¶”ê°€ ê¸°ì‚¬",
}

# ì—°êµ¬ë™í–¥ more í˜ì´ì§€
RESEARCH_MORE_FILENAME = "more_research.html"


USE_KOREAN_FIRST = True  # Trueë©´ í•œê¸€ ì œëª© ìš°ì„ , Falseë©´ ì˜ë¬¸ ìš°ì„ 

# ============================
# ê³µí†µ ìœ í‹¸: ì €ë„/ì†ŒìŠ¤ ë¼ë²¨ ì •ë¦¬
# ============================
def format_journal_source_label(journal_name: str) -> str:
    """
    'Remote Sensing (MDPI)' -> 'MDPI - Remote Sensing'
    'Journal of Applied Remote Sensing (SPIE)' -> 'SPIE - Journal of Applied Remote Sensing'
    """
    if not journal_name:
        return ""

    journal_name = journal_name.strip()
    m = re.match(r"(.+?)\s*\((.+)\)", journal_name)
    if not m:
        return journal_name

    main = m.group(1).strip()
    site = m.group(2).strip()
    return f"{site} - {main}"

def make_card(article, use_korean_first=USE_KOREAN_FIRST):
    """
    ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œ (ì¸ë„¤ì¼ + ì œëª©/ë¶€ì œëª© + ë‚ ì§œ + ìš”ì•½)
    """
    thumb = article.get("thumbnail_url") or DEFAULT_THUMB

    ko = article.get("ko_title", "") or ""
    en = article.get("orig_title", "") or ""
    date = article.get("date", "") or ""
    summary = article.get("summary", "") or ""
    url = article.get("url", "") or ""

    # í•œ/ì˜ ì œëª© ì •ë¦¬
    if ko and en:
        main_title, sub_title = (ko, en) if use_korean_first else (en, ko)
    else:
        main_title = ko or en
        sub_title = ""

    sub_block = ""
    if sub_title:
        sub_block = f"""
            <div style="font-size:14px; color:#555; margin-bottom:4px; line-height:1.35;">
              {h(sub_title)}
            </div>"""

    return f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td style="padding:0; margin:0;">

      <table width="100%" cellpadding="0" cellspacing="0" border="0">

        <tr>

          <!-- ì¸ë„¤ì¼ -->
          <td valign="top" width="135" style="padding:0;">
            <a href="{h(url)}" target="_blank" style="text-decoration:none;">
              <img src="{h(thumb)}" width="120" height="120"
                   style="display:block; border-radius:14px; object-fit:cover;"
                   onerror="this.onerror=null; this.src='{DEFAULT_THUMB}';">
            </a>
          </td>

          <!-- ì œëª©/ë¶€ì œëª© -->
          <td valign="top" style="padding-left:6px;">

            <div style="font-size:18px; font-weight:700; color:#111;
                        margin-bottom:4px; line-height:1.35;">
              {h(main_title)}
            </div>

            {sub_block}

            <div style="font-size:12px; color:#888; margin-bottom:2px;">
              {h(date)}
            </div>

          </td>

        </tr>

        <!-- ìš”ì•½ -->
        <tr>
          <td colspan="2" style="padding-top:12px;">
            <div style="font-size:14px; color:#333; line-height:1.8;">
              {h(summary)}
            </div>
          </td>
        </tr>

        <!-- ì›ë¬¸ ë§í¬ -->
        <tr>
          <td colspan="2" style="padding-top:6px;">
            <a href="{h(url)}" target="_blank"
               style="font-size:13px; color:#0066cc; text-decoration:none;">
              ìì„¸íˆ ë³´ê¸° â†’
            </a>
          </td>
        </tr>

      </table>

    </td>
  </tr>
</table>
"""


def make_research_card_text_only(article):
    """
    ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ ì „ìš© í…ìŠ¤íŠ¸ ì¹´ë“œ
    - ì¸ë„¤ì¼ ì—†ìŒ
    - ì¹´ë“œ ì „ì²´ê°€ ì›ë¬¸ ë§í¬ë¡œ í´ë¦­ë˜ë„ë¡ êµ¬ì„±
    """
    title_en = article.get("title_en", "") or ""
    date = article.get("date", "") or ""
    summary = article.get("summary", "") or ""
    url = article.get("url", "") or ""

    journal_name = article.get("journal_name", "") or ""
    source_label = format_journal_source_label(journal_name)

    return f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:18px;">
  <tr>
    <!-- ì—¬ê¸° tdì—ëŠ” íŒ¨ë”©/ë§ˆì§„ë§Œ ë‘ê³ , ì‹¤ì œ ì¹´ë“œ ìŠ¤íƒ€ì¼ì€ ì•ˆìª½ divì— ì¤Œ -->
    <td style="padding:0; margin:0;">

      <!-- âœ… ì¹´ë“œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë§í¬ -->
      <a href="{h(url)}" target="_blank"
         style="display:block; text-decoration:none; color:inherit;">

        <!-- ì‹¤ì œ ì¹´ë“œ ëª¨ì–‘(ë°°ê²½, í…Œë‘ë¦¬, íŒ¨ë”©)ì€ ì´ divì— ì ìš© -->
        <div style="background:#ffffff; border-radius:12px;
                    border:1px solid #e5e7eb;
                    padding:16px 18px; box-sizing:border-box;">

          <!-- ì˜ë¬¸ ì›ì œëª© (êµµê²Œ) -->
          <div style="font-size:17px; font-weight:700; color:#111827;
                      line-height:1.4; margin-bottom:4px;">
            {h(title_en)}
          </div>

          <!-- ì†Œ -->
          <div style="font-size:12px; font-weight:500; color:#374151; margin-bottom:2px;">
            {h(source_label)}
          </div>

          <!-- ë°œí–‰ì¼ -->
          <div style="font-size:12px; color:#6b7280; margin-bottom:8px;">
            {h(date)}
          </div>

          <!-- ìš”ì•½ (ë³¸ë¬¸/ìš”ì•½ ë‚´ìš©) -->
          <div style="font-size:14px; color:#374151; line-height:1.7;">
            {h(summary)}
          </div>

        </div>
      </a>
      <!-- ë§í¬ ë -->

    </td>
  </tr>
</table>
"""




def build_sections_html(topic_main_articles, topic_extra_articles):
    """
    ë©”ì¼ ë³¸ë¬¸ìš© ì„¹ì…˜:
    - ê° í† í”½ì˜ ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œë“¤ë§Œ ë³´ì—¬ì¤Œ
    - ê° í† í”½ ë§ˆì§€ë§‰ì— 'ì¶”ê°€ ê¸°ì‚¬' ê´€ë ¨ UI í‘œì‹œ
        â†’ extra ìˆìœ¼ë©´ : 'ì´ ì£¼ì œ ì¶”ê°€ ê¸°ì‚¬ ë³´ê¸° â†’'
        â†’ extra ì—†ìœ¼ë©´ : 'ì´ë²ˆ ì£¼ëŠ” ì¶”ê°€ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤'
    """
    html_parts = []

    for topic_num in [4, 3, 2, 1]:
        main_list = topic_main_articles.get(topic_num, [])
        extras = topic_extra_articles.get(topic_num, [])
        topic_title = f"{TOPIC_ICON[topic_num]} {TOPIC_DESC[topic_num]}"

        if not main_list:
            continue

        topic_inner = []

        # í† í”½ ì œëª©
        topic_inner.append(f"""
<div style="font-size:26px; font-weight:800; color:#111827;
            margin-bottom:24px; line-height:1.3;">
  {h(topic_title)}
</div>
""")

        # ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œ
        for art in main_list:
            topic_inner.append(make_card(art))

        # â­ ì¶”ê°€ ê¸°ì‚¬ ë²„íŠ¼ / ë˜ëŠ” ì¶”ê°€ ê¸°ì‚¬ ì—†ìŒ ë©”ì‹œì§€
        if extras:
            # ì •ìƒ ë²„íŠ¼ ë²„ì „
            topic_inner.append(f"""
<div style="margin-top:12px; text-align:right;">
  <a href="{TOPIC_MORE_URLS[topic_num]}"
     style="display:inline-block; font-size:13px; font-weight:600;
            color:#111827; text-decoration:none;
            padding:6px 12px; border-radius:999px;
            border:1px solid #d1d5db; background:#ffffff;">
    ì´ ì£¼ì œ ì¶”ê°€ ê¸°ì‚¬ ë³´ê¸° â†’
  </a>
</div>
""")
        else:
            # â—ì¶”ê°€ ê¸°ì‚¬ ì—†ìŒ ë©”ì‹œì§€ ë²„íŠ¼
            topic_inner.append(f"""
<div style="margin-top:12px; text-align:right;">
  <div style="display:inline-block; font-size:13px; font-weight:600;
              color:#9ca3af;
              padding:6px 12px; border-radius:999px;
              border:1px solid #e5e7eb; background:#f9fafb;">
    ì´ë²ˆ ì£¼ëŠ” ì¶”ê°€ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤
  </div>
</div>
""")

        topic_html = "".join(topic_inner)

        html_parts.append(f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:36px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#f9fafb;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            {topic_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
""")

    return "".join(html_parts)

def build_research_section_html(main_articles, extra_articles, more_url):
    """
    ë©”ì¸ ë‰´ìŠ¤ë ˆí„° ë³¸ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” 'ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥' ì„¹ì…˜ HTML
    - ì¹´ë“œ UIëŠ” ê¸°ì¡´ ë‰´ìŠ¤ ì¹´ë“œì™€ ë™ì¼í•˜ê²Œ make_card() ì¬ì‚¬ìš©
    - 'ìµœì‹  ì—°êµ¬ ë” ë³´ê¸° â†’' ë²„íŠ¼ë§Œ í‘œì‹œ
    """
    if not main_articles:
        return ""

    inner = []

    # ì„¹ì…˜ ì œëª©
    inner.append(f"""
<div style="font-size:26px; font-weight:800; color:#111827;
            margin-bottom:24px; line-height:1.3;">
  {h(RESEARCH_SECTION_TITLE)}
</div>
""")

    # 1) ë©”ì¸ ì—°êµ¬ë™í–¥ ì¹´ë“œë“¤ â†’ 'í…ìŠ¤íŠ¸-only' ì—°êµ¬ ì¹´ë“œ ì‚¬ìš©
    for art in main_articles:
        converted = {
            "title_en": art.get("original_title", ""),
            "date": art.get("published_at", ""),
            "summary": art.get("summary_en", ""),
            "url": art.get("url", ""),
            "journal_name": art.get("journal_name", ""),  # âœ… ì¶”ê°€
        }


        inner.append(make_research_card_text_only(converted))



    # 2) 'ìµœì‹  ì—°êµ¬ ë” ë³´ê¸° â†’' ë²„íŠ¼ë§Œ í‘œì‹œ
    if extra_articles:
        inner.append(f"""
<div style="margin-top:12px; text-align:right;">
  <a href="{h(more_url)}"
     style="display:inline-block; font-size:13px; font-weight:600;
            color:#111827; text-decoration:none;
            padding:6px 12px; border-radius:999px;
            border:1px solid #d1d5db; background:#ffffff;">
    ìµœì‹  ì—°êµ¬ ë” ë³´ê¸° â†’
  </a>
</div>
""")

    # 3) ì „ì²´ ë°•ìŠ¤ ë ˆì´ì•„ì›ƒ ê°ì‹¸ê¸°
    body_html = "".join(inner)

    return f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:36px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#f9fafb;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            {body_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
"""


def build_more_page_html(topic_extra_articles, date_range, newsletter_date, header_title):
    """
    í† í”½ë³„ 'ì¶”ê°€ ê¸°ì‚¬' í˜ì´ì§€ HTML ìƒì„±
    - topic_extra_articles: {í† í”½ë²ˆí˜¸: [ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸]} í˜•íƒœ (ë³´í†µ 1ê°œ í† í”½ë§Œ ë“¤ì–´ì˜¤ê²Œ ì‚¬ìš©)
    - header_title: ìƒë‹¨ í—¤ë”ì— ë“¤ì–´ê°ˆ íƒ€ì´í‹€ (ì˜ˆ: "GeoINT - ì¶”ê°€ ê¸°ì‚¬")
    """
    sections = []

    for topic_num in [1, 2, 3, 4]:
        extras = topic_extra_articles.get(topic_num, [])
        if not extras:
            continue

        topic_title = f"{TOPIC_ICON[topic_num]} {TOPIC_DESC[topic_num]}"

        rows = []
        for idx, art in enumerate(extras):
            ko = art.get("ko_title", "") or ""
            en = art.get("orig_title", "") or ""
            url = art.get("url", "") or ""
            date = art.get("date", "") or ""
            thumb = art.get("thumbnail_url") or DEFAULT_THUMB

            main_title = ko or en
            sub_title = en if ko else ""

            sub_block = ""
            if sub_title:
                sub_block = f"""
        <div style="font-size:12px; color:#6b7280; margin-top:2px; line-height:1.4;">
          {h(sub_title)}
        </div>"""

            # âœ… í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ: <tr> ì•ˆì— ë‹¤ì‹œ <tr> ë„£ì§€ ì•Šê¸°
            rows.append(f"""
              <tr class="topic-article-row" data-index="{idx}">
                <td style="padding:10px 0; border-bottom:1px solid #e5e7eb;">
                  <table width="100%" cellpadding="0" cellspacing="0" border="0">
                    <tr>
                      <!-- ì¸ë„¤ì¼ -->
                      <td valign="top" width="96" style="padding-right:8px;">
                        <a href="{h(url)}" target="_blank" style="text-decoration:none;">
                          <img src="{h(thumb)}" width="80" height="80"
                               style="display:block; border-radius:10px; object-fit:cover;"
                               onerror="this.onerror=null; this.src='{DEFAULT_THUMB}';">
                        </a>
                      </td>

                      <!-- ì œëª©/ë¶€ì œëª©/ë‚ ì§œ -->
                      <td valign="top">
                        <div style="font-size:14px; line-height:1.5; margin-bottom:2px;">
                          <a href="{h(url)}" target="_blank"
                             style="color:#111827; text-decoration:none;">
                            {h(main_title)}
                          </a>
                        </div>
                        {sub_block}
                        <div style="font-size:11px; color:#9ca3af; margin-top:2px;">
                          {h(date)}
                        </div>
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            """)


        table_html = f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0">
  {''.join(rows)}
</table>
"""

        # Gmail ìŠ¤íƒ€ì¼ í˜ì´ì§€ ì •ë³´ + ì¢Œìš° ë²„íŠ¼ ì˜ì—­
        pager_html = f"""
<div id="topic-pagination"
     style="display:flex;
            justify-content:flex-end;
            align-items:center;
            font-size:14px;
            font-weight:500;
            color:#374151;
            margin-top:-10px;
            margin-bottom:10px;">

  <span id="topic-page-info"
        style="margin-right:14px;"></span>

  <!-- ì´ì „ ë²„íŠ¼ -->
  <a href="#" id="topic-prev"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       margin-right:6px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x2039;
    </span>
  </a>

  <!-- ë‹¤ìŒ ë²„íŠ¼ -->
  <a href="#" id="topic-next"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x203A;
    </span>
  </a>
</div>
"""

        sections.append(f"""
<!-- í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ ë°•ìŠ¤ -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#ffffff;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            <div style="font-size:24px; font-weight:700; color:#111827;
                        margin-bottom:28px;">
              {h(topic_title)}
            </div>
            {pager_html}
            {table_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
""")

    if not sections:
        body_inner = """
<p style="font-size:14px; color:#4b5563;">
  ì´ë²ˆ ì£¼ëŠ” ë³„ë„ì˜ ì¶”ê°€ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.
</p>
"""
    else:
        body_inner = "".join(sections)

    more_html = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>{header_title}</title>

<style>
  /* ëª¨ë°”ì¼ ìµœì í™” */
  @media (max-width: 768px) {{
    .hero-bg {{
      background-position:center 0 !important;
    }}
    .hero-header-cell {{
      padding:32px 0 28px 0 !important;
    }}
  }}

  /* í‘¸í„°ë¥¼ í•­ìƒ ì•„ë˜ë¡œ ë°€ì–´ì£¼ëŠ” ë ˆì´ì•„ì›ƒ */
  html, body {{
    height: 100%;
    margin: 0;
  }}

  body {{
    display: flex;
    flex-direction: column;
    min-height: 100vh;
    background:#f3f4f6;
    font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;
  }}


  .content-wrapper {{
    flex: 1;
  }}
</style>

</head>

<body>

  <!-- í—¤ë” (ë©”ì¼ê³¼ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼) -->
  <table class="hero-bg" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background-image:url('{HEADER_BACKGROUND}');
              background-size:cover;
              background-position:center -60px;
              background-repeat:no-repeat;">
    <tr>
      <td align="center" class="hero-header-cell"
          bgcolor="#000000"
          style="padding:0 24px 14px 24px;
                 background: linear-gradient(to bottom right,
                             rgba(0,0,40,0.55),
                             rgba(0,0,0,0.55));
                 color:#ffffff; ;">

        <table cellpadding="0" cellspacing="0" border="0"
               style="max-width:{CONTENT_WIDTH}px; width:100%;
                      color:#ffffff; margin:0 auto;">

          <tr>
            <td style="padding:16px 24px 8px 24px;">
              <table width="100%">
                <tr>
                  <td align="left">
                    <img src="{LOGO_URL}" style="max-width:110px; display:block;">
                  </td>
                  <td align="right"
                      style="text-transform:uppercase; font-size:13px;
                             color:#ffffff; ;">
                    WWW.INSPACE.CO.KR
                  </td>
                </tr>
              </table>
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0px 24px 12px 24px;
                       font-size:28px; font-weight:600;
                       color:#ffffff; ;">
              {h(header_title)}
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0 24px 48px 24px;
                       font-size:14px; font-weight:300; opacity:0.9; line-height:1.5;
                       color:#ffffff; ;">
              {date_range}<br>
              {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°
            </td>
          </tr>

        </table>

      </td>
    </tr>
  </table>


  <div class="content-wrapper">
  <!-- ë³¸ë¬¸ -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0" style="padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td style="padding:0 16px 0 16px;">

              <!-- ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼ (ì¹´ë“œ ìœ„ ì™¼ìª½ ì •ë ¬) -->
              <div style="padding-bottom:20px; text-align:left;">
                <a href="{MAIN_PAGE_URL}"
                  style="display:inline-block;
                          font-size:13px;
                          padding:6px 12px;
                          border-radius:999px;
                          background:#111827;
                          color:#ffffff;
                          text-decoration:none;">
                  â† ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸°
                </a>
              </div>

              {body_inner}

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
  </div> <!-- content-wrapper ë -->

  <!-- í‘¸í„° -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0"
        style="background:#1e293b; padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td class="inner-padding"
                style="padding:12px 16px;
                      font-size:12px;
                      color:#e2e8f0;
                      text-align:center;
                      line-height:1.6;">

              ë³¸ ë©”ì¼ì€ í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ì‚¬ë‚´ êµ¬ì„±ì›ì„ ìœ„í•œ ì£¼ê°„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì…ë‹ˆë‹¤.<br>
              ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ì „ì¬ ë° ê³µìœ ëŠ” ì§€ì–‘í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br><br>
              &copy; {now_kst.year} Hancom InSpace. All Rights Reserved.

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>

  <script>
  (function() {{
    // âœ… í† í”½ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ë„¤ì´ì…˜ (GeoINT/ë“œë¡ /AI/ìœ„ì„± ê³µí†µ)
    var PAGE_SIZE = 5;  // í•œ í˜ì´ì§€ì— 5ê°œì”© ë³´ì—¬ì¤Œ (10ê°œë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° ìˆ«ìë§Œ 10ìœ¼ë¡œ ë³€ê²½)

    var rows = Array.prototype.slice.call(
      document.querySelectorAll('.topic-article-row')
    );
    var total = rows.length;
    var totalPages = Math.max(1, Math.ceil(total / PAGE_SIZE));
    var currentPage = 1;

    var infoEl = document.getElementById('topic-page-info');
    var prevEl = document.getElementById('topic-prev');
    var nextEl = document.getElementById('topic-next');

    function render(page) {{
      if (page < 1 || page > totalPages) return;
      currentPage = page;

      var start = (currentPage - 1) * PAGE_SIZE;
      var end = start + PAGE_SIZE;

      rows.forEach(function(row, idx) {{
        row.style.display = (idx >= start && idx < end) ? '' : 'none';
      }});

      if (infoEl) {{
        var from = total === 0 ? 0 : start + 1;
        var to = Math.min(end, total);
        infoEl.textContent = from + 'â€“' + to + ' of ' + total;
      }}

      if (prevEl) {{
        if (currentPage === 1) {{
          prevEl.style.opacity = '0.3';
          prevEl.style.pointerEvents = 'none';
        }} else {{
          prevEl.style.opacity = '1';
          prevEl.style.pointerEvents = 'auto';
        }}
      }}

      if (nextEl) {{
        if (currentPage === totalPages) {{
          nextEl.style.opacity = '0.3';
          nextEl.style.pointerEvents = 'none';
        }} else {{
          nextEl.style.opacity = '1';
          nextEl.style.pointerEvents = 'auto';
        }}
      }}
    }}

    if (prevEl) prevEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage - 1);
    }});

    if (nextEl) nextEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage + 1);
    }});

    render(1);
  }})();
  </script>


</body>
</html>
"""
    return more_html


def build_research_more_page_html(extra_articles, date_range, newsletter_date):
    """
    ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€ í˜ì´ì§€ HTML
    - ë ˆì´ì•„ì›ƒ: build_more_page_html(GeoINT ë“±)ê³¼ ë™ì¼
      * ê°™ì€ í—¤ë”(íˆì–´ë¡œ ì˜ì—­)
      * ê°™ì€ 'ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸°' ë²„íŠ¼
      * ê°™ì€ í‘¸í„°
      * ë³¸ë¬¸ì€ í°ìƒ‰ ì¹´ë“œ ë°•ìŠ¤ ì•ˆì— 'ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥' ì œëª© + ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    # 1) ë³¸ë¬¸(ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸) HTML ë§Œë“¤ê¸°
    if not extra_articles:
        # ì¶”ê°€ í•™ìˆ ì§€ê°€ ì—†ì„ ë•Œ: ì¹´ë“œ ì•ˆì— ì•ˆë‚´ ë¬¸êµ¬ë§Œ
        topic_title = "ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€"
        body_inner = f"""
<!-- ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ ë°•ìŠ¤ (ë°ì´í„° ì—†ìŒ) -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#ffffff;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            <div style="font-size:24px; font-weight:700; color:#111827;
                        margin-bottom:16px;">
              {h(topic_title)}
            </div>
            <p style="font-size:14px; color:#4b5563; margin:0;">
              ì´ë²ˆ ì£¼ì—ëŠ” ì¶”ê°€ í•™ìˆ ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.
            </p>
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
"""


    else:
        # ë…¼ë¬¸ë“¤ ë¦¬ìŠ¤íŠ¸ë¥¼ rowsë¡œ ìŒ“ê¸°
        rows = []
        for idx, art in enumerate(extra_articles):
            title_en = art.get("original_title", "") or ""
            date = art.get("published_at", "") or ""
            url = art.get("url", "") or ""
            journal_name = art.get("journal_name", "") or ""

            # 'Remote Sensing (MDPI)' -> 'MDPI - Remote Sensing' ë³€í™˜
            source_label = format_journal_source_label(journal_name)

            rows.append(f"""
              <tr class="research-article-row" data-index="{idx}">
                <td style="padding:8px 0; border-bottom:1px solid #e5e7eb;">
                  <!-- ì˜ë¬¸ ì›ì œëª© -->
                  <div style="font-size:14px; font-weight:600;
                              line-height:1.5; margin-bottom:4px;">
                    <a href="{h(url)}" target="_blank"
                      style="color:#111827; text-decoration:none;">
                      {h(title_en)}
                    </a>
                  </div>

                  <!-- ë‚ ì§œ + ì†ŒìŠ¤ ì›¹ì‚¬ì´íŠ¸/ì €ë„ í‘œì‹œ -->
                  <div style="font-size:12px; font-weight:500; color:#374151; margin-bottom:2px;">
                    {h(source_label)}
                  </div>

                  <!-- ë‚ ì§œ -->
                  <div style="font-size:12px; color:#9ca3af; margin-bottom:4px;">
                    {h(date)}
                  </div>
                </td>
              </tr>
            """)



        table_html = f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0">
  {''.join(rows)}
</table>
"""

        # Gmail ìŠ¤íƒ€ì¼ í˜ì´ì§€ ì •ë³´ + ì¢Œìš° ë²„íŠ¼ ì˜ì—­
        pager_html = f"""
<div id="research-pagination"
     style="display:flex;
            justify-content:flex-end;
            align-items:center;
            font-size:14px;
            font-weight:500;
            color:#374151;
            margin-top:-10px;
            margin-bottom:10px;">

  <span id="research-page-info"
        style="margin-right:14px;"></span>

  <!-- ì´ì „ ë²„íŠ¼ -->
  <a href="#" id="research-prev"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       margin-right:6px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x2039;
    </span>
  </a>

  <!-- ë‹¤ìŒ ë²„íŠ¼ -->
  <a href="#" id="research-next"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x203A;
    </span>
  </a>
</div>
"""



        topic_title = "ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥"

        # GeoINTìš© more í˜ì´ì§€ì™€ ë™ì¼í•œ ì¹´ë“œ ë°•ìŠ¤ êµ¬ì¡°ë¡œ ê°ì‹¸ê¸°
        body_inner = f"""
<!-- ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ ë°•ìŠ¤ -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#ffffff;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            <div style="font-size:24px; font-weight:700; color:#111827;
                        margin-bottom:28px;">
              {h(topic_title)}
            </div>
              {pager_html}
              {table_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
"""

    # ìƒë‹¨ íˆì–´ë¡œ í—¤ë”ì— ë“¤ì–´ê°ˆ íƒ€ì´í‹€ (íƒ­ ì œëª©ë„ ê°™ì´ ì‚¬ìš©)
    header_title = "ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€"

    more_html = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">



<title>{header_title}</title>

<style>
  /* ëª¨ë°”ì¼ ìµœì í™” */
  @media (max-width: 768px) {{
    .hero-bg {{
      background-position:center 0 !important;
    }}
    .hero-header-cell {{
      padding:32px 0 28px 0 !important;
    }}
  }}

  /* í‘¸í„°ë¥¼ í•­ìƒ ì•„ë˜ë¡œ ë°€ì–´ì£¼ëŠ” ë ˆì´ì•„ì›ƒ */
  html, body {{
    height: 100%;
    margin: 0;
  }}

  body {{
    display: flex;
    flex-direction: column;
    min-height: 100vh;
    background:#f3f4f6;
    font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;
  }}

  .content-wrapper {{
    flex: 1;
  }}
</style>

</head>

<body>

  <!-- í—¤ë” (more_geointì™€ ë™ì¼ êµ¬ì¡°) -->
  <table class="hero-bg" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background-image:url('{HEADER_BACKGROUND}');
              background-size:cover;
              background-position:center -60px;
              background-repeat:no-repeat;">
    <tr>
      <td align="center" class="hero-header-cell"
          bgcolor="#000000"
          style="padding:0 24px 14px 24px;
                 background: linear-gradient(to bottom right,
                             rgba(0,0,40,0.55),
                             rgba(0,0,0,0.55));
                 color:#ffffff; ;">

        <table cellpadding="0" cellspacing="0" border="0"
               style="max-width:{CONTENT_WIDTH}px; width:100%;
                      color:#ffffff; margin:0 auto;">

          <tr>
            <td style="padding:16px 24px 8px 24px;">
              <table width="100%">
                <tr>
                  <td align="left">
                    <img src="{LOGO_URL}" style="max-width:110px; display:block;">
                  </td>
                  <td align="right"
                      style="text-transform:uppercase; font-size:13px;
                             color:#ffffff; ;">
                    WWW.INSPACE.CO.KR
                  </td>
                </tr>
              </table>
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0px 24px 12px 24px;
                       font-size:28px; font-weight:600;
                       color:#ffffff; ;">
              {h(header_title)}
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0 24px 48px 24px;
                       font-size:14px; font-weight:300; opacity:0.9; line-height:1.5;
                       color:#ffffff; ;">
              {date_range}<br>
              {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°
            </td>
          </tr>

        </table>

      </td>
    </tr>
  </table>


  <div class="content-wrapper">
  <!-- ë³¸ë¬¸ -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0" style="padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td style="padding:0 16px 0 16px;">

              <!-- ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼ -->
              <div style="padding-bottom:20px; text-align:left;">
                <a href="{MAIN_PAGE_URL}"
                  style="display:inline-block;
                          font-size:13px;
                          padding:6px 12px;
                          border-radius:999px;
                          background:#111827;
                          color:#ffffff;
                          text-decoration:none;">
                  â† ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸°
                </a>
              </div>

              {body_inner}

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
  </div> <!-- content-wrapper ë -->


  <!-- í‘¸í„° (more_geointì™€ ë™ì¼) -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0"
        style="background:#1e293b; padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td class="inner-padding"
                style="padding:12px 16px;
                      font-size:12px;
                      color:#e2e8f0;
                      text-align:center;
                      line-height:1.6;">

              ë³¸ ë©”ì¼ì€ í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ì‚¬ë‚´ êµ¬ì„±ì›ì„ ìœ„í•œ ì£¼ê°„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì…ë‹ˆë‹¤.<br>
              ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ì „ì¬ ë° ê³µìœ ëŠ” ì§€ì–‘í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br><br>
              &copy; {now_kst.year} Hancom InSpace. All Rights Reserved.

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>


  <script>
  (function() {{
    var PAGE_SIZE = 5; // âœ… 5ê°œì”© ë³´ì—¬ì£¼ê¸°

    var rows = Array.prototype.slice.call(
      document.querySelectorAll('.research-article-row')
    );
    var total = rows.length;
    var totalPages = Math.max(1, Math.ceil(total / PAGE_SIZE));
    var currentPage = 1;

    var infoEl = document.getElementById('research-page-info');
    var prevEl = document.getElementById('research-prev');
    var nextEl = document.getElementById('research-next');

    function render(page) {{
      if (page < 1 || page > totalPages) return;
      currentPage = page;

      var start = (currentPage - 1) * PAGE_SIZE;
      var end = start + PAGE_SIZE;

      rows.forEach(function(row, idx) {{
        row.style.display = (idx >= start && idx < end) ? '' : 'none';
      }});

      if (infoEl) {{
        var from = total === 0 ? 0 : start + 1;
        var to = Math.min(end, total);
        infoEl.textContent = from + 'â€“' + to + ' of ' + total;
      }}

      if (prevEl) {{
        if (currentPage === 1) {{
          prevEl.style.opacity = '0.3';
          prevEl.style.pointerEvents = 'none';
        }} else {{
          prevEl.style.opacity = '1';
          prevEl.style.pointerEvents = 'auto';
        }}
      }}

      if (nextEl) {{
        if (currentPage === totalPages) {{
          nextEl.style.opacity = '0.3';
          nextEl.style.pointerEvents = 'none';
        }} else {{
          nextEl.style.opacity = '1';
          nextEl.style.pointerEvents = 'auto';
        }}
      }}

    }}

    if (prevEl) prevEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage - 1);
    }});

    if (nextEl) nextEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage + 1);
    }});

    render(1);
  }})();
  </script>


</body>
</html>
"""
    return more_html


# ============================================================
# Weekly Focus Insight (ì£¼ê°„ í¬ì»¤ìŠ¤ ì¸ì‚¬ì´íŠ¸)
# - ì…ë ¥: ì£¼ì œë³„ ë‰´ìŠ¤(ìš°ì„ ìˆœìœ„ ìƒìœ„ 10ê°œ) + ì—°êµ¬ë™í–¥(ìƒìœ„ 10ê°œ)
# - ì¶œë ¥: 1~3ì¤„ í•œêµ­ì–´ ì¡°ì–¸(ë¬¸ì¥í˜•)
# ============================================================

WEEKLY_FOCUS_TITLE = "ğŸ” Weekly Focus Insight"
MAX_INSIGHT_ITEMS_PER_TOPIC = 10
MAX_INSIGHT_ITEMS_RESEARCH = 10

def _take_top_n(items, n):
    return (items or [])[:n]

def build_weekly_focus_context(topic_main_articles, topic_extra_articles,
                               research_main_articles, research_extra_articles):
    """
    ì£¼ì œë‹¹ 10ê°œê¹Œì§€ë§Œ GPTì— ì œê³µ.
    ì£¼ì˜: topic_main_articlesëŠ” ì´ë¯¸ ìƒìœ„ 3ê°œì´ê³ ,
          topic_extra_articlesëŠ” ê·¸ ë‹¤ìŒ ìš°ì„ ìˆœìœ„ë“¤ì´ë¼ê³  ê°€ì •(í˜„ì¬ íŒŒì´í”„ë¼ì¸ êµ¬ì¡°ìƒ).
    """
    ctx = {"topics": {}, "research": []}

    # í† í”½(1~4)
    for t in [1, 2, 3, 4]:
        combined = (topic_main_articles.get(t, []) or []) + (topic_extra_articles.get(t, []) or [])
        top10 = _take_top_n(combined, MAX_INSIGHT_ITEMS_PER_TOPIC)
        ctx["topics"][f"topic_{t}"] = [
            {
                "title_ko": a.get("title_ko") or "",
                "original_title": a.get("original_title") or "",
                "summary_ko": a.get("summary_ko") or "",
                "date": a.get("date") or "",
                "url": a.get("url") or "",
            }
            for a in top10
        ]

    # ì—°êµ¬ë™í–¥(ìƒìœ„ 10ê°œ)
    research_combined = (research_main_articles or []) + (research_extra_articles or [])
    research_top10 = _take_top_n(research_combined, MAX_INSIGHT_ITEMS_RESEARCH)
    ctx["research"] = [
        {
            "original_title": a.get("original_title") or "",
            "summary_en": a.get("summary_en") or "",
            "journal_name": a.get("journal_name") or "",
            "date": a.get("date") or "",
            "url": a.get("url") or "",
        }
        for a in research_top10
    ]

    return ctx

def generate_weekly_focus_insight(
    topic_main_articles,
    topic_extra_articles,
    research_main_articles,
    research_extra_articles,
    top_k_per_topic=10
):
    """
    Weekly Focus Insight:
    - í† í”½(1~4) ë‰´ìŠ¤ + ìµœì‹  ì—°êµ¬ë™í–¥ ìš”ì•½(ìƒìœ„ í•­ëª©ë“¤)ì„ ì½ê³ 
      í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ì—ê²Œ 1~3ì¤„ í•œêµ­ì–´ ì¡°ì–¸ì„ ìƒì„±.
    - í† í° ì ˆì•½: í† í”½ë³„ ìƒìœ„ top_k_per_topicê°œê¹Œì§€ë§Œ ì‚¬ìš©.
    """

    def _pick_top_k(article_list, k):
        return (article_list or [])[:k]

    def _news_payload(a: dict):
        # âœ… (ì¤‘ìš”) ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í‚¤(ko_title/summary) + ê³¼ê±° í‚¤(title_ko/summary_ko) ëª¨ë‘ í˜¸í™˜
        return {
            "title_ko": (a.get("ko_title") or a.get("title_ko") or "").strip(),
            "original_title": (a.get("orig_title") or a.get("original_title") or "").strip(),
            "summary_ko": (a.get("summary") or a.get("summary_ko") or "").strip(),
            "source": (a.get("source") or "").strip(),
            "date": (a.get("date") or "").strip(),
            "url": (a.get("url") or "").strip(),
            "priority": a.get("priority", None),
        }

    def _research_payload(a: dict):
        return {
            "title_ko": (a.get("title_ko") or a.get("ko_title") or "").strip(),
            "original_title": (a.get("original_title") or a.get("orig_title") or "").strip(),
            "summary_ko": (a.get("summary_ko") or a.get("summary") or "").strip(),
            "journal": (a.get("journal_name") or a.get("journal") or "").strip(),
            "published_at": (a.get("published_at") or a.get("date") or "").strip(),
            "url": (a.get("url") or "").strip(),
            "priority": a.get("priority", None),
        }

    ctx = {
        "topics": {},
        "research": {
            "main": [],
            "extra": []
        }
    }

    for topic_num in [1, 2, 3, 4]:
        main_top = _pick_top_k(topic_main_articles.get(topic_num, []), top_k_per_topic)
        extra_top = _pick_top_k(topic_extra_articles.get(topic_num, []), top_k_per_topic)

        ctx["topics"][str(topic_num)] = {
            "main": [_news_payload(a) for a in main_top],
            "extra": [_news_payload(a) for a in extra_top],
        }

    research_main_top = _pick_top_k(research_main_articles or [], top_k_per_topic)
    research_extra_top = _pick_top_k(research_extra_articles or [], top_k_per_topic)

    ctx["research"]["main"] = [_research_payload(a) for a in research_main_top]
    ctx["research"]["extra"] = [_research_payload(a) for a in research_extra_top]

    system = (
        "ë„ˆëŠ” ê¸€ë¡œë²Œ ì‚°ì—…Â·ê¸°ìˆ  ë³€í™”ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ì‹œë‹ˆì–´ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. "
        "ì…ë ¥ìœ¼ë¡œ í•´ë‹¹ ì£¼ì˜ ë‰´ìŠ¤ì™€ ì—°êµ¬ë™í–¥ ìš”ì•½ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤. "
        "ì¶œë ¥ì€ ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ì— ë“¤ì–´ê°ˆ 'Weekly Focus Insight'ì…ë‹ˆë‹¤. "

        "ì´ ì¸ì‚¬ì´íŠ¸ëŠ” ë‹¨ìˆœ ìš”ì•½ì´ë‚˜ í˜„ìƒ ë‚˜ì—´ì´ ì•„ë‹ˆë¼, "
        "ì—¬ëŸ¬ ì‚¬ê±´ê³¼ ê¸°ìˆ  ë³€í™”ë¥¼ í•˜ë‚˜ì˜ ê´€ì ìœ¼ë¡œ ë¬¶ì–´ í•´ì„í•˜ëŠ” ë¶„ì„ ë¬¸ë‹¨ì´ì–´ì•¼ í•©ë‹ˆë‹¤. "

        "ë‹¤ìŒ ê¸°ì¤€ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”: "
        "1) ê¸€ ì „ì²´ëŠ” í•˜ë‚˜ì˜ ì¤‘ì‹¬ ë…¼ì§€(throughline)ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. "
        "   (ì˜ˆ: 'ì§€ë¦¬ê³µê°„ ì •ë³´ê°€ ê¸°ìˆ ì„ ë„˜ì–´ ì „ëµ ìì‚°ìœ¼ë¡œ ì „í™˜ë˜ê³  ìˆë‹¤') "
        "2) ê° ë¬¸ì¥ì€ ì• ë¬¸ì¥ì˜ ì›ì¸Â·ê²°ê³¼Â·ì˜ë¯¸ í™•ì¥ ê´€ê³„ë¡œ ì—°ê²°ë˜ì–´ì•¼ í•˜ë©°, "
        "   ë…ë¦½ì ì¸ ìš”ì•½ ë¬¸ì¥ì„ ë‚˜ì—´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "3) 'Aë„ ì¼ì–´ë‚˜ê³  Bë„ ì¼ì–´ë‚˜ê³  Cë„ ì¼ì–´ë‚œë‹¤' ì‹ì˜ ë³‘ë ¬ ë‚˜ì—´ì€ ê¸ˆì§€í•©ë‹ˆë‹¤. "
        "4) ìµœì†Œ í•œ ë²ˆ ì´ìƒ 'ì´ë¡œ ì¸í•´ / ê·¸ ê²°ê³¼ / ì´ì— ë”°ë¼ / ì´ëŸ¬í•œ íë¦„ì€'ê³¼ ê°™ì€ "
        "   ì¸ê³¼ ë˜ëŠ” êµ¬ì¡°ì  ì—°ê²° í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. "
        "5) ë³€í™”ì˜ ì˜ë¯¸ê°€ ë“œëŸ¬ë‚˜ë„ë¡ "
        "   'ê¸°ìˆ  â†’ ì „ëµ', 'ë„êµ¬ â†’ ì¸í”„ë¼', 'ìš´ì˜ â†’ ê±°ë²„ë„ŒìŠ¤'ì™€ ê°™ì€ ì „í™˜ ê´€ì ì„ í¬í•¨í•©ë‹ˆë‹¤. "
        "6) íŠ¹ì • ê¸°ì—…ì´ë‚˜ ì¡°ì§(í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ë“±)ì„ ì§ì ‘ ì§€ì¹­í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "7) ì¡´ëŒ“ë§ ì„œìˆ í˜•ìœ¼ë¡œ 2~3ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. "

        "ë¶ˆë¦¿, ë²ˆí˜¸, ë‹¨ìˆœ ìš”ì•½ì²´, ê³¼ì¥ë˜ê±°ë‚˜ ë‹¨ì •ì ì¸ ì˜ˆì¸¡ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )




    user = json.dumps(ctx, ensure_ascii=False)

    try:
        resp = client.responses.create(
            model=MODEL_NAME,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.4,
        )
        text = (resp.output[0].content[0].text or "").strip()
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return "\n".join(lines[:3]) if lines else ""
    except Exception as e:
        print(f"[WARN] Weekly Focus Insight ìƒì„± ì‹¤íŒ¨: {e}")
        return ""


# =========================
# âœ… Archive ì¹´ë“œìš© 1ì¤„ Insight ìƒì„±
# =========================
def summarize_insight_for_archive(one_to_three_lines: str) -> str:
    """
    ë©”ì¸ Weekly Focus Insight(1~3ì¤„)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,
    ì•„ì¹´ì´ë¸Œ ì¹´ë“œì— ë„£ì„ 'í•œ ì¤„' ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ë„ˆë¬´ ê¸¸ë©´ ìë™ìœ¼ë¡œ ì§§ê²Œ
    - ì‹¤íŒ¨ ì‹œ: ì²« ë¬¸ì¥/ì²« ì¤„ì„ ì˜ë¼ì„œë¼ë„ ë°˜í™˜
    """
    src = (one_to_three_lines or "").strip()
    if not src:
        return ""

    # 1) ê°€ë²¼ìš´ fallback(ëª¨ë¸ ì‹¤íŒ¨ ëŒ€ë¹„)
    fallback = src.splitlines()[0].strip()
    if len(fallback) > 120:
        fallback = fallback[:117].rstrip() + "â€¦"

    # 2) GPTë¡œ "í•œ ë¬¸ì¥" ì••ì¶•
    try:
        system = (
            "ë„ˆëŠ” ì£¼ê°„ ì‚°ì—…/ê¸°ìˆ  íë¦„ì„ ì•„ì¹´ì´ë¸Œ ì¹´ë“œìš© 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ë§Œë“œëŠ” ì—ë””í„°ì…ë‹ˆë‹¤. "
            "ì…ë ¥ì€ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸(1~3ì¤„)ì´ë©°, ì¶œë ¥ì€ í•œêµ­ì–´ ì¡´ëŒ“ë§ 1ë¬¸ì¥(ë§ˆì¹¨í‘œ 1ê°œ)ì…ë‹ˆë‹¤. "
            "\n\n"
            "[í•„ìˆ˜ ê·œì¹™]\n"
            "1) ë°˜ë“œì‹œ ë™í–¥ ë™ì‚¬(ê°•í™”/ê°€ì†/í™•ì‚°/ë¶€ìƒ/ì „í™˜/ì¬í¸ ì¤‘ 1ê°œ ì´ìƒ)ë¥¼ í¬í•¨í•´, "
            "â€˜ë¬´ì—‡ì´ ì–´ë–»ê²Œ ë³€í•˜ê³  ìˆëŠ”ì§€â€™ë¥¼ íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤.\n"
            "2) ì£¼ì œ ë‚˜ì—´(ì˜ˆ: â€˜Aì™€ Bì™€ Câ€™)ì´ë‚˜ ë©”íƒ€ ì„¤ëª…(ì˜ˆ: â€˜~ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤/ì†Œê°œí•©ë‹ˆë‹¤/ìš”ì•½í•©ë‹ˆë‹¤/ì „ë°˜ì ìœ¼ë¡œâ€™)ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "3) íŠ¹ì • ê¸°ì—…/ì¡°ì§/ë¸Œëœë“œ(í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ë“±) ì§ì ‘ ì§€ì¹­ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "4) ê³¼ì¥, ë‹¨ì •ì  ì˜ˆì¸¡, ê³¼ë„í•œ ê²°ë¡ , ë¶ˆë¦¿/ë²ˆí˜¸ëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "5) 80ì ì´ë‚´ì˜ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\n"
            "\n"
            "[ê¶Œì¥ êµ¬ì„±]\n"
            "â€˜ì´ë²ˆ ì£¼ì—ëŠ” [í•µì‹¬ íë¦„]ì´ [ê°•í™”/ê°€ì†/í™•ì‚°/ë¶€ìƒ]ë˜ê³  ìˆìœ¼ë©°, [ì´ìœ /ë°©í–¥]ì´ í•¨ê»˜ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.â€™\n"
            "\n"
            "[ì¢‹ì€ ì˜ˆ]\n"
            "â€˜ì´ë²ˆ ì£¼ì—ëŠ” ì‹¤ì‹œê°„ ê³µê°„ì •ë³´ì™€ ë¬´ì¸ì²´ê³„ ê²°í•© íë¦„ì´ ê°•í™”ë˜ë©°, êµ°Â·ìƒì—… í™œìš© í™•ì‚°ì´ ë‘ë“œëŸ¬ì§€ê³  ìˆìŠµë‹ˆë‹¤.â€™\n"
            "\n"
            "[ë‚˜ìœ ì˜ˆ]\n"
            "â€˜ì´ë²ˆ ì£¼ì—ëŠ” ìœ„ì„±, ë“œë¡ , AIë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.â€™\n"
            "â€˜Aì™€ Bê°€ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.â€™"
        )
        user = src

        resp = client.responses.create(
            model=MODEL_NAME,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = (resp.output[0].content[0].text or "").strip()

        MAX_ARCHIVE_INSIGHT_CHARS = 150

        # í•œ ë¬¸ì¥ë§Œ ì‚¬ìš©
        text = text.replace("\n", " ").strip()
        if not text:
            return fallback
        if len(text) > MAX_ARCHIVE_INSIGHT_CHARS:
            text = text[:MAX_ARCHIVE_INSIGHT_CHARS-1].rstrip() + "â€¦"
        return text
    except Exception as e:
        print(f"[WARN] Archive 1ì¤„ Insight ìƒì„± ì‹¤íŒ¨: {e}")
        return fallback



def build_archive_page_html(archive_items):
    rows = []

    for item in archive_items:
        date_str = item.get("date_str", "")
        insight = (item.get("insight") or item.get("insight_full") or "").strip()

        year_attr = ""
        month_attr = ""

        # "YYYY.MM.DD" í˜•ì‹ ê°€ì •
        if len(date_str) >= 7:
            year = date_str[:4]
            month_part = date_str[5:7]
            if year.isdigit() and month_part.isdigit():
                year_attr = year
                month_attr = str(int(month_part))  # "03" â†’ "3"

        insight_html = f"""
          <div class="archive-card-insight">
            {h(insight)}
          </div>
        """ if insight else ""

        rows.append(f"""
        <tr class="archive-row" data-year="{year_attr}" data-month="{month_attr}">
          <td style="padding-bottom:16px;">
            <a href="{item['url']}" class="archive-card">
              <div class="archive-card-label">
                {item['label']}
              </div>

              {insight_html}

              <div class="archive-card-date">
                ì—…ë¡œë“œ: {item.get('date_str','')}
              </div>
            </a>
          </td>
        </tr>
        """)

    card_list_html = "\n".join(rows)

    return f"""
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<title>ì•„ì¹´ì´ë¸Œ</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
  html, body {{
    margin: 0;
    padding: 0;
  }}

  body {{
    font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;
    background:#0b1120;
    color:#e5e7eb;
  }}

  .bg-video {{
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    object-fit: cover;
    z-index: -1;
    opacity: 0.5;
  }}

  .archive-wrap {{
    min-height: 100vh;
    padding: 40px 16px 64px 16px;
    background:
      radial-gradient(circle at top,
        rgba(148,163,184,0.25) 0,
        transparent 55%);
  }}

  .filter-wrap {{
    margin: 0 16px 12px 16px;
  }}

  .filter-group-label {{
    font-size:13px;
    color:#cbd5f5;
    margin-bottom:6px;
    opacity:0.8;
  }}

  .chips-row {{
    display:flex;
    flex-wrap:wrap;
    gap:8px;
    margin-bottom:6px;
  }}

  .chip-base {{
    border-radius:9999px;
    border:1px solid rgba(148,163,184,0.6);
    background:rgba(15,23,42,0.6);
    color:#e5e7eb;
    font-size:13px;
    padding:6px 12px;
    cursor:pointer;
    outline:none;
    white-space:nowrap;
    transition:background 0.18s ease, border-color 0.18s ease, transform 0.12s ease, box-shadow 0.12s ease;
  }}


  /* âœ… Weekly Focus Insight(ì•„ì¹´ì´ë¸Œ ì¹´ë“œìš©) */
  .archive-card-insight{{
    font-size:13px;
    line-height:1.45;
    color:#cbd5e1;
    margin:6px 0 8px 0;
    opacity:0.92;

    display:-webkit-box;
    -webkit-line-clamp:2;
    -webkit-box-orient:vertical;
    overflow:hidden;

    /* âœ… ë‹¨ì–´ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ë°©ì§€ */
    word-break: keep-all;
    overflow-wrap: break-word;
  }}


  .chip-base:hover {{
    background:rgba(30,64,175,0.7);
    border-color:#60a5fa;
    box-shadow:0 0 0 1px rgba(15,23,42,0.4);
    transform:translateY(-1px);
  }}

  .chip-base.active {{
    background:#2563eb;
    border-color:#2563eb;
    color:#ffffff;
    box-shadow:0 0 0 1px rgba(15,23,42,0.4);
  }}

  .archive-card {{
      display:block;
      text-decoration:none;
      border-radius:14px;
      border:1px solid rgba(255,255,255,0.55);
      padding:18px 20px;
      background:rgba(255,255,255,0.18);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      box-shadow:0 10px 28px rgba(15,23,42,0.6);
      transition:transform 0.22s ease, box-shadow 0.22s ease, border-color 0.22s ease, background 0.22s ease;
  }}

  .archive-card:hover {{
      transform:translateY(-4px);
      background:rgba(255,255,255,0.32);
      box-shadow:0 18px 40px rgba(15,23,42,0.85);
      border-color:rgba(255,255,255,0.95);
  }}

  .archive-card-label {{
    font-size:17px;
    font-weight:600;
    color:#ffffff;
    margin-bottom:4px;
  }}

  .archive-card-date {{
    font-size:13px;
    color:#e5e7eb;
  }}

  @media (max-width: 768px) {{
    .archive-card {{
      padding:14px;
    }}
  }}

  @media (max-width: 768px) {{
    .bg-video {{
      display: none;
    }}

    body {{
      background:
        url('https://hancom-inspace.github.io/Weekly-Newsletter/assets/archive_bg_fallback.png')
        center top / cover no-repeat;
      color: #e5e7eb;
    }}

    .archive-wrap {{
      background: radial-gradient(
        circle at top,
        rgba(148,163,184,0.18) 0,
        transparent 55%
      );
    }}
  }}
</style>
</head>

<body>
  <video id="bgVideo" class="bg-video" preload="auto" muted playsinline>
    <source src="{ARCHIVE_VIDEO_URL}" type="video/mp4" />
  </video>

  <div class="archive-wrap">
    <center>
      <table width="100%" cellpadding="0" cellspacing="0" border="0" style="max-width:700px; margin:0 auto;">
        <tr>
          <td style="padding:60px 16px 30px 16px; font-size:28px; font-weight:700; color:#f9fafb;">
            ğŸ“š ì´ì „ ë‰´ìŠ¤ë ˆí„° ë‹¤ì‹œë³´ê¸°
          </td>
        </tr>

        <tr>
          <td>
            <div class="filter-wrap">
              <div id="year-chips" class="chips-row"></div>
              <div id="month-chips" class="chips-row"></div>
            </div>
          </td>
        </tr>

        <tr>
          <td style="padding:4px 16px 8px 16px;">
            <table width="100%" cellpadding="0" cellspacing="0" border="0">
              <tbody>
                {card_list_html}
              </tbody>
            </table>
          </td>
        </tr>
      </table>
    </center>
  </div>

<script>
  document.addEventListener('DOMContentLoaded', function() {{
    var rows = Array.prototype.slice.call(document.querySelectorAll('.archive-row'));
    var yearContainer = document.getElementById('year-chips');
    var monthContainer = document.getElementById('month-chips');

    if (!rows.length || !yearContainer || !monthContainer) return;

    var yearSet = new Set();
    var yearMonthMap = {{}};

    rows.forEach(function(row) {{
      var y = row.getAttribute('data-year');
      var m = parseInt(row.getAttribute('data-month') || '0', 10);
      if (!y) return;

      yearSet.add(y);

      if (!yearMonthMap[y]) {{
        yearMonthMap[y] = new Set();
      }}
      if (m > 0) {{
        yearMonthMap[y].add(m);
      }}
    }});

    var years = Array.from(yearSet).sort();

    var today = new Date();
    var currentYear = String(today.getFullYear());

    var state = {{
      year: yearSet.has(currentYear) ? currentYear : 'all',
      month: null
    }};

    monthContainer.style.display = 'none';

    function createYearChip(value, label) {{
      var btn = document.createElement('button');
      btn.type = 'button';
      btn.className = 'chip-base year-chip';
      btn.setAttribute('data-year', value);
      btn.textContent = label;

      btn.addEventListener('click', function() {{
        state.year = value;
        state.month = null;

        if (value === 'all') {{
          clearMonthChips();
        }} else {{
          renderMonthChipsForYear(value);
        }}

        updateView();
      }});

      return btn;
    }}

    function createMonthChip(monthNumber) {{
      var btn = document.createElement('button');
      btn.type = 'button';
      btn.className = 'chip-base month-chip';
      btn.setAttribute('data-month', String(monthNumber));
      btn.textContent = monthNumber + 'ì›”';

      btn.addEventListener('click', function() {{
        state.month = monthNumber;
        updateView();
      }});

      return btn;
    }}

    function clearMonthChips() {{
      monthContainer.innerHTML = '';
      monthContainer.style.display = 'none';
    }}

    function renderMonthChipsForYear(year) {{
      monthContainer.innerHTML = '';

      var monthSet = yearMonthMap[year];
      if (!monthSet || monthSet.size === 0) {{
        monthContainer.style.display = 'none';
        return;
      }}

      var months = Array.from(monthSet);
      months.sort(function(a, b) {{ return a - b; }});

      months.forEach(function(m) {{
        monthContainer.appendChild(createMonthChip(m));
      }});

      monthContainer.style.display = 'flex';
    }}

    function updateView() {{
      rows.forEach(function(row) {{
        var rowYear = row.getAttribute('data-year');
        var rowMonth = parseInt(row.getAttribute('data-month') || '0', 10);

        var yearMatch = (state.year === 'all') || (rowYear === state.year);
        var monthMatch;

        if (state.year === 'all') {{
          monthMatch = true;
        }} else if (!state.month) {{
          monthMatch = true;
        }} else {{
          monthMatch = (rowMonth === state.month);
        }}

        row.style.display = (yearMatch && monthMatch) ? '' : 'none';
      }});

      var yearChips = document.querySelectorAll('.year-chip');
      yearChips.forEach(function(chip) {{
        var y = chip.getAttribute('data-year');
        chip.classList.toggle('active', y === state.year);
      }});

      var monthChips = document.querySelectorAll('.month-chip');
      monthChips.forEach(function(chip) {{
        var m = parseInt(chip.getAttribute('data-month') || '0', 10);
        var isActive = (state.year !== 'all') && (state.month === m);
        chip.classList.toggle('active', isActive);
      }});
    }}

    var allYearChip = createYearChip('all', 'ì „ì²´');
    yearContainer.appendChild(allYearChip);

    years.forEach(function(y) {{
      yearContainer.appendChild(createYearChip(y, y + 'ë…„'));
    }});

    if (state.year !== 'all') {{
      renderMonthChipsForYear(state.year);
    }} else {{
      clearMonthChips();
    }}

    updateView();
  }});
</script>

</body>
</html>
"""





# ============================================================
# â–¼ GitHub Pagesìš© ë‚ ì§œ í´ë” ë° URL ì„¤ì •
# ============================================================

# í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œë¡œ í´ë” ìƒì„± (ì˜ˆ: 2025/11/26)
KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST)

FOLDER_PATH = f"{now_kst.year}/{now_kst.month:02d}/{now_kst.day:02d}"
# ì˜ˆ: "2025/11/26"

# ë©”ì¸ í˜ì´ì§€ ë° í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ URL
MAIN_PAGE_URL = f"{BASE_URL}/{FOLDER_PATH}/index.html"

TOPIC_MORE_URLS = {
    t: f"{BASE_URL}/{FOLDER_PATH}/{TOPIC_MORE_FILENAMES[t]}"
    for t in TOPIC_MORE_FILENAMES
}

RESEARCH_MORE_URL = f"{BASE_URL}/{FOLDER_PATH}/{RESEARCH_MORE_FILENAME}"

# â–¼ ì´ì „ ë‰´ìŠ¤ë ˆí„° ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ ì„¤ì •
ARCHIVE_PAGE_PATH = "docs/archive.html"
ARCHIVE_PAGE_URL = f"{BASE_URL}/archive.html"

# â–¼ ì•„ì¹´ì´ë¸Œ ìƒë‹¨ ìŠ¤í¬ë¡¤ ë¹„ë””ì˜¤(mp4) ê²½ë¡œ (ì—¬ê¸°ì— ë„¤ ì˜ìƒ URL ë„£ê¸°)
ARCHIVE_VIDEO_URL = "https://hancom-inspace.github.io/Weekly-Newsletter/assets/archive_bg.mp4"

# â–¼ ì•„ì¹´ì´ë¸Œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œë„ ê´€ë¦¬ (ë§¤ë²ˆ ê¹ƒí—ˆë¸Œ í´ë”ë¥¼ ì½ì–´ì„œ ê°±ì‹ )
ARCHIVE_JSON_PATH = "docs/archive.json"


def list_github_directory(path_in_repo: str):
    """
    GitHub REST APIë¥¼ ì‚¬ìš©í•´ì„œ ì£¼ì–´ì§„ ê²½ë¡œì˜ ë””ë ‰í„°ë¦¬/íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¨ë‹¤.
    path_in_repo ì˜ˆ: 'docs', 'docs/2025', 'docs/2025/11' ë“±
    """
    if not GITHUB_TOKEN:
        print("[ê²½ê³ ] GITHUB_TOKEN ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë””ë ‰í„°ë¦¬ ëª©ë¡ ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{path_in_repo}"

    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    resp = requests.get(api_url, headers=headers)
    if resp.status_code == 200:
        return resp.json()
    else:
        print(f"[GitHub ë””ë ‰í„°ë¦¬ ì¡°íšŒ ì‹¤íŒ¨] {path_in_repo} status={resp.status_code}")
        try:
            print(resp.text)
        except Exception:
            pass
        return []


import base64

def get_github_file_text(path_in_repo: str):
    """
    GitHub repo ì•ˆì˜ íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    (ì˜ˆ: docs/archive.json)
    """
    if not GITHUB_TOKEN:
        return None

    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{path_in_repo}"
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    resp = requests.get(api_url, headers=headers)
    if resp.status_code != 200:
        return None

    data = resp.json()
    if data.get("encoding") != "base64":
        return None

    try:
        return base64.b64decode(data["content"]).decode("utf-8")
    except Exception:
        return None


def load_existing_archive():
    """
    âœ… 1ìˆœìœ„: docs/archive.json ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (insight ìœ ì§€)
    âœ… 2ìˆœìœ„: ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ í´ë” ìŠ¤ìº”
    """

    # --------------------------------------------------
    # 1) archive.json ë¨¼ì € ì‹œë„
    # --------------------------------------------------
    json_text = get_github_file_text("docs/archive.json")
    if json_text:
        try:
            items = json.loads(json_text)
            if isinstance(items, list):
                return items
        except Exception:
            pass  # ì‹¤íŒ¨í•˜ë©´ fallback

    # --------------------------------------------------
    # 2) fallback: ê¸°ì¡´ í´ë” ìŠ¤ìº” ë°©ì‹
    # --------------------------------------------------
    archive_items = []

    docs_children = list_github_directory("docs")
    year_dirs = [
        item["name"]
        for item in docs_children
        if item.get("type") == "dir" and item.get("name", "").isdigit()
    ]

    for year_name in year_dirs:
        year_path = f"docs/{year_name}"
        month_children = list_github_directory(year_path)
        month_dirs = [
            m["name"]
            for m in month_children
            if m.get("type") == "dir" and m.get("name", "").isdigit()
        ]

        for month_name in month_dirs:
            month_path = f"docs/{year_name}/{month_name}"
            day_children = list_github_directory(month_path)
            day_dirs = [
                d["name"]
                for d in day_children
                if d.get("type") == "dir" and d.get("name", "").isdigit()
            ]

            for day_name in day_dirs:
                day_path = f"docs/{year_name}/{month_name}/{day_name}"
                files = list_github_directory(day_path)

                has_index = any(
                    f.get("type") == "file" and f.get("name") == "index.html"
                    for f in files
                )
                if not has_index:
                    continue

                try:
                    date_obj = datetime(int(year_name), int(month_name), int(day_name)).date()
                except ValueError:
                    continue

                date_str = date_obj.strftime("%Y.%m.%d")
                week_no = ((date_obj.day - 1) // 7) + 1
                label = f"{date_obj.month}ì›” {week_no}ì£¼ì°¨ ë‰´ìŠ¤ë ˆí„°"
                url = f"{BASE_URL}/{year_name}/{month_name}/{day_name}/index.html"

                archive_items.append({
                    "label": label,
                    "date_str": date_str,
                    "url": url,
                    "insight": "",   # fallbackì—ëŠ” insight ì—†ìŒ
                })

    return archive_items



# ì‹¤í–‰ ì‹œì ì— ì´ì „ ì•„ì¹´ì´ë¸Œ ëª©ë¡ ë¡œë”©
NEWSLETTER_ARCHIVE_BASE = load_existing_archive()


# --- ì›” ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚° (í•´ë‹¹ ë‹¬ì—ì„œ ëª‡ ë²ˆì§¸ ì£¼ì¸ì§€) ---
today_date = now_kst.date()
day_in_month = today_date.day

# 1~7ì¼: 1ì£¼ì°¨, 8~14ì¼: 2ì£¼ì°¨, 15~21ì¼: 3ì£¼ì°¨, 22~28ì¼: 4ì£¼ì°¨, 29~31ì¼: 5ì£¼ì°¨
week_no = ((day_in_month - 1) // 7) + 1

WEEK_LABEL = f"{today_date.month}ì›” {week_no}ì£¼ì°¨"


# ë‰´ìŠ¤ë ˆí„° ì—…ë°ì´íŠ¸ ë‚ ì§œë„ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ
NEWSLETTER_DATE = now_kst.strftime("%Y.%m.%d")


# ============================================================
# â–¼ Section HTML ìƒì„±
# ============================================================

sections_html = build_sections_html(topic_main_articles, topic_extra_articles)

# ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ ì„¹ì…˜ ì¶”ê°€ (ìœ„ì„±ì˜ìƒ ì„¹ì…˜ ì•„ë˜ì— ë¶™ëŠ” íš¨ê³¼)
research_section_html = build_research_section_html(
    research_main_articles,
    research_extra_articles,
    RESEARCH_MORE_URL,
)

sections_html = sections_html + research_section_html


weekly_focus_insight = generate_weekly_focus_insight(
    topic_main_articles, topic_extra_articles,
    research_main_articles, research_extra_articles
)

weekly_focus_html = f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0" style="margin-bottom:24px;">
  <tr><td align="center">
    <table cellpadding="0" cellspacing="0" border="0"
           style="width:100%; max-width:{CONTENT_WIDTH}px;
                  background:#ffffff; border:1px solid #e5e7eb;
                  border-radius:12px; padding:18px; box-sizing:border-box;">
      <tr><td style="font-size:14px; font-weight:800; color:#111827;">
        {WEEKLY_FOCUS_TITLE}
      </td></tr>
      <tr><td style="font-size:14px; color:#374151; line-height:1.7; padding-top:2px; white-space:pre-line; word-break: keep-all; overflow-wrap: break-word;">
        {h(weekly_focus_insight) if weekly_focus_insight else "ì´ë²ˆ ì£¼ í¬ì»¤ìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
      </td></tr>
    </table>
  </td></tr>
</table>
"""


# ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
dates = []
for topic_dict in (topic_main_articles, topic_extra_articles):
    for t in [1, 2, 3, 4]:
        for art in topic_dict.get(t, []):
            raw = (art.get("date") or "").strip()
            try:
                d = datetime.strptime(raw, "%Y-%m-%d")
                dates.append(d)
            except Exception:
                pass

if dates:
    start_date = min(dates)
    end_date = max(dates)
    date_range = f"{start_date.year}ë…„ {start_date.month}ì›” {start_date.day}ì¼ ~ {end_date.year}ë…„ {end_date.month}ì›” {end_date.day}ì¼"
else:
    date_range = ""

# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ HTML ìƒì„±
more_pages_html = {}
for topic_num in [1, 2, 3, 4]:
    extras = topic_extra_articles.get(topic_num, [])
    if not extras:
        continue

    header_title = TOPIC_MORE_TITLES.get(topic_num, f"Topic {topic_num} - ì¶”ê°€ ê¸°ì‚¬")
    more_pages_html[topic_num] = build_more_page_html(
        {topic_num: extras},
        date_range,
        NEWSLETTER_DATE,
        header_title,
    )

# ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€ í˜ì´ì§€ HTML ìƒì„±
# - ì´ í˜ì´ì§€ì—ëŠ” 'ì •í•œ ê¸°ê°„ ë‚´ ëª¨ë“  í•™ìˆ ì§€'ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ research_more_articles ì‚¬ìš©
research_more_html = build_research_more_page_html(
    research_more_articles,
    date_range,
    NEWSLETTER_DATE,
)



# ============================================================
# â–¼ ìµœì¢… ë‰´ìŠ¤ë ˆí„° HTML
# ============================================================

import base64  # íŒŒì¼ ìƒë‹¨ì— ì´ë¯¸ ì—†ìœ¼ë©´, ë§¨ ìœ„ import ë¶€ë¶„ì— ì¶”ê°€ë˜ì–´ ìˆì–´ì•¼ í•¨.

def upload_file_to_github(path_in_repo: str, content_str: str, commit_message: str):
    """
    GitHub REST APIë¥¼ ì‚¬ìš©í•´ì„œ íŒŒì¼ 1ê°œë¥¼ docs/ ì´í•˜ì— ì—…ë¡œë“œ/ì—…ë°ì´íŠ¸.
    path_in_repo ì˜ˆ: 'docs/2025/11/26/index.html'
    """
    if not GITHUB_TOKEN:
        print("[ê²½ê³ ] GITHUB_TOKEN ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{path_in_repo}"

    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    # 1) ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ìˆìœ¼ë©´ sha í•„ìš”)
    sha = None
    get_resp = requests.get(api_url, headers=headers)
    if get_resp.status_code == 200:
        sha = get_resp.json().get("sha")

    # 2) ë‚´ìš© base64 ì¸ì½”ë”©
    encoded = base64.b64encode(content_str.encode("utf-8")).decode("utf-8")

    payload = {
        "message": commit_message,
        "content": encoded,
        "branch": "main",
    }
    if sha:
        payload["sha"] = sha  # ì—…ë°ì´íŠ¸

    put_resp = requests.put(api_url, headers=headers, json=payload)

    if put_resp.status_code in (200, 201):
        print(f"[GitHub ì—…ë¡œë“œ ì„±ê³µ] {path_in_repo}")
        return put_resp.json()
    else:
        print(f"[GitHub ì—…ë¡œë“œ ì‹¤íŒ¨] {path_in_repo} status={put_resp.status_code}")
        print(put_resp.text)
        return None


newsletter_html = f"""

<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°</title>

<style>
  /* í™”ë©´ í­ì´ 768px ì´í•˜(ëª¨ë°”ì¼)ì¼ ë•Œ ì ìš© */
  @media (max-width: 768px) {{
    .hero-bg {{
      background-position: center 0 !important;
    }}
    .hero-header-cell {{
      padding: 32px 0 28px 0 !important;
    }}
    .inner-padding {{
      padding-left: 16px !important;
      padding-right: 16px !important;
    }}
    .main-title {{
      font-size: 32px !important;
    }}
    .sub-title {{
      font-size: 14px !important;
    }}
  }}
</style>

</head>

<body style="margin:0; padding:0; background:#f3f4f6;
             font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;">


<!-- í—¤ë” -->

<table class="hero-bg" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background-image:url('{HEADER_BACKGROUND}');
              background-size:cover;
              background-position:center -60px;
              background-repeat:no-repeat;">
  <tr>
    <td align="center" class="hero-header-cell"
        bgcolor="#000000"
        style="padding:12px 24px 10px 24px;
               background: linear-gradient(to bottom right,
                           rgba(0,0,40,0.55),
                           rgba(0,0,0,0.55));
               color:#ffffff; ;">

      <table cellpadding="0" cellspacing="0" border="0"
             style="max-width:{CONTENT_WIDTH}px; width:100%;
                    color:#ffffff; margin:0 auto;">

        <tr>
          <td class="inner-padding" style="padding:16px 24px 8px 24px;">
            <table width="100%">
              <tr>
                <td align="left">
                  <img src="{LOGO_URL}" style="max-width:110px; display:block;">
                </td>
                <td align="right"
                    style="text-transform:uppercase; font-size:13px;
                           color:#ffffff; ;">
                  WWW.INSPACE.CO.KR
                </td>
              </tr>
            </table>
          </td>
        </tr>

        <tr>
          <td align="center" class="inner-padding"
              style="padding:12px 24px 6px 24px;
                     font-size:40px; font-weight:600;
                     color:#ffffff; ;">
            <div class="main-title"
                 style="color:#ffffff; ;">
              InSpace Weekly
            </div>
          </td>
        </tr>

        <tr>
          <td align="center" class="inner-padding sub-title"
              style="padding:0 24px 8px 24px;
                     font-size:15px; font-weight:300; opacity:0.85;
                     color:#ffffff; ;">
            {date_range}
          </td>
        </tr>

        <tr>
          <td class="inner-padding" style="padding:8px 24px 12px 24px;">
            <table width="100%">
              <tr>
                <td></td>
                <td align="right"
                    style="font-size:13px; line-height:1.6;
                           color:#ffffff; ;">
                  í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤<br>{WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°
                  <div style="margin-top:4px; letter-spacing:0.14em;
                              color:#ffffff; ;">
                    ì—…ë°ì´íŠ¸: {NEWSLETTER_DATE}
                  </div>
                </td>
              </tr>
            </table>
          </td>
        </tr>

      </table>

    </td>
  </tr>
</table>


<!-- ë³¸ë¬¸ -->

<table width="100%" cellpadding="0" cellspacing="0" border="0" style="padding:24px 0 16px 0;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
        <tr>
          <td class="inner-padding" style="padding:0 16px 0 16px;">

            <!-- ì´ì „ ë‰´ìŠ¤ë ˆí„° ë³´ê¸° ë²„íŠ¼ -->
            <table width="100%" cellpadding="0" cellspacing="0" border="0"
                  style="margin-bottom:24px;">
              <tr><td>
                <a href="{ARCHIVE_PAGE_URL}"
                  style="display:block; text-decoration:none; border-radius:12px;
                          border:1px solid #e5e7eb; padding:16px 18px;
                          background:#f9fafb;">
                  <div style="font-size:14px; color:#4b5563; margin-bottom:4px;">
                    ì•„ì¹´ì´ë¸Œ
                  </div>
                  <div style="font-size:16px; font-weight:600; color:#111827;">
                    ì´ì „ ë‰´ìŠ¤ë ˆí„° ë‹¤ì‹œ ë³´ê¸° â†’
                  </div>
                </a>
              </td></tr>
            </table>

            {weekly_focus_html}
            {sections_html}

          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>

<!-- í‘¸í„° -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background:#1e293b; padding:24px 0 32px 0;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
        <tr>
          <td class="inner-padding"
              style="padding:12px 16px;
                     font-size:12px;
                     color:#e2e8f0;
                     text-align:center;
                     line-height:1.6;">

            ë³¸ ë©”ì¼ì€ í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ì‚¬ë‚´ êµ¬ì„±ì›ì„ ìœ„í•œ ì£¼ê°„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì…ë‹ˆë‹¤.<br>
            ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ì „ì¬ ë° ê³µìœ ëŠ” ì§€ì–‘í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br><br>
            &copy; {now_kst.year} Hancom InSpace. All Rights Reserved.

          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>


</body>
</html>
"""


# íŒŒì¼ ì €ì¥
with open("newsletter.html", "w", encoding="utf-8") as f:
    f.write(newsletter_html)

for topic_num, filename in TOPIC_MORE_FILENAMES.items():
    html = more_pages_html.get(topic_num)
    if not html:
        continue
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html)

# ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ í˜ì´ì§€ ì €ì¥
with open("more_research.html", "w", encoding="utf-8") as f:
    f.write(research_more_html)

print("more_research.html ì €ì¥ ì™„ë£Œ")

print("newsletter.html ì €ì¥ ì™„ë£Œ")
for topic_num, filename in TOPIC_MORE_FILENAMES.items():
    if topic_num in more_pages_html:
        print(f"{filename} ì €ì¥ ì™„ë£Œ")

if IN_COLAB:
    display(HTML(newsletter_html))

# ============================================================
# â–¼ GitHub Pages ì—…ë¡œë“œ
# ============================================================

main_repo_path = f"docs/{FOLDER_PATH}/index.html"

commit_msg_main = f"Add newsletter main: {FOLDER_PATH}"
upload_file_to_github(main_repo_path, newsletter_html, commit_msg_main)

# â–¼ ì´ì „ ë‰´ìŠ¤ë ˆí„° ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ ìƒì„±
archive_items = list(NEWSLETTER_ARCHIVE_BASE)

# âœ… ë©”ì¸ í˜ì´ì§€ìš©(1~3ì¤„) insightëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
weekly_focus_insight_full = weekly_focus_insight

# âœ… ì•„ì¹´ì´ë¸Œ ì¹´ë“œì—ëŠ” 1ì¤„ë¡œ ì¬ìš”ì•½í•´ì„œ ì €ì¥
weekly_focus_insight_card = summarize_insight_for_archive(weekly_focus_insight_full)

today_item = {
    "label": f"{WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°",
    "date_str": NEWSLETTER_DATE,
    "url": MAIN_PAGE_URL,
    # âœ… ì•„ì¹´ì´ë¸Œ ì¹´ë“œì—ì„œ ë°”ë¡œ ì“°ëŠ” ê°’(1ì¤„)
    "insight": weekly_focus_insight_card,
}

# âœ… ê°™ì€ URLì´ ìˆìœ¼ë©´ "ìŠ¤í‚µ"ì´ ì•„ë‹ˆë¼ "ì—…ë°ì´íŠ¸" (ê¸°ì¡´ ì¹´ë“œì— insightë¥¼ ì±„ì›Œ ë„£ê¸°)
found = None
for item in archive_items:
    if item.get("url") == today_item["url"]:
        found = item
        break

if found:
    # ê¸°ì¡´ í•­ëª©ì„ ìµœì‹  ê°’ìœ¼ë¡œ ê°±ì‹  (insight í¬í•¨)
    found.update(today_item)

    # (ì„ íƒ) ìµœì‹  í•­ëª©ì´ ë¦¬ìŠ¤íŠ¸ ë§¨ ì•ì— ì˜¤ë„ë¡ ì •ë ¬ ìœ ì§€
    archive_items = [found] + [x for x in archive_items if x is not found]
else:
    archive_items.insert(0, today_item)

# archive.html ìƒì„±
archive_html = build_archive_page_html(archive_items)

# ë¡œì»¬ ì €ì¥ (HTML)
with open("archive.html", "w", encoding="utf-8") as f:
    f.write(archive_html)

# GitHub ì—…ë¡œë“œ (HTML)
upload_file_to_github(
    ARCHIVE_PAGE_PATH,
    archive_html,
    f"Update archive page: {NEWSLETTER_DATE}"
)

# JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”
archive_json_str = json.dumps(archive_items, ensure_ascii=False, indent=2)

# ë¡œì»¬ ì €ì¥ (JSON)
with open("archive.json", "w", encoding="utf-8") as f:
    f.write(archive_json_str)

# GitHub ì—…ë¡œë“œ (JSON)
upload_file_to_github(
    ARCHIVE_JSON_PATH,
    archive_json_str,
    f"Update archive data: {NEWSLETTER_DATE}"
)



# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ ì—…ë¡œë“œ
for topic_num, filename in TOPIC_MORE_FILENAMES.items():
    html = more_pages_html.get(topic_num)
    if not html:
        continue

    more_repo_path = f"docs/{FOLDER_PATH}/{filename}"
    commit_msg_more = f"Add newsletter more (topic{topic_num}): {FOLDER_PATH}"
    upload_file_to_github(more_repo_path, html, commit_msg_more)

# ğŸ’¡ ì—°êµ¬ë™í–¥ more í˜ì´ì§€ ì—…ë¡œë“œ
if research_more_html:
    research_repo_path = f"docs/{FOLDER_PATH}/{RESEARCH_MORE_FILENAME}"
    commit_msg_research = f"Add newsletter research more: {FOLDER_PATH}"
    upload_file_to_github(research_repo_path, research_more_html, commit_msg_research)


print("GitHub Pages ì—…ë¡œë“œ ìš”ì²­ ì™„ë£Œ")
print("ë©”ì¸ í˜ì´ì§€ URL:", MAIN_PAGE_URL)
for topic_num, url in TOPIC_MORE_URLS.items():
    if topic_num in more_pages_html:
        print(f"ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ URL (topic{topic_num}):", url)


# # **09 ì´ë©”ì¼ ìë™ ë°œì†¡**
# ### **(Colabì—ì„œ ì‹¤í–‰í•˜ë©´ í…ŒìŠ¤íŠ¸ ì´ë©”ì¼ë¡œ, Github ì‹¤í–‰ ì‹œ, ì‹¤ì œ ìˆ˜ì‹ ìì—ê²Œ)**

# In[60]:


SEND_EMAIL = os.environ.get("SEND_EMAIL", "true").lower() == "true"

GMAIL_USER = os.environ.get("GMAIL_USER")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")
TO_EMAIL = None  # ê¸°ë³¸ê°’

if SEND_EMAIL:
    # ğŸ”¹ Colabì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë©”ì¼ë¡œë§Œ ì „ì†¡
    if IN_COLAB:
        TO_EMAIL = os.environ.get("TO_EMAIL_TEST")
        if not TO_EMAIL:
            print("[ë©”ì¼ ì „ì†¡ X] Colab í…ŒìŠ¤íŠ¸ìš© ì´ë©”ì¼(TO_EMAIL_TEST)ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì¼ì€ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            SEND_EMAIL = False  # ë©”ì¼ ì „ì†¡ ê°•ì œ OFF
        else:
            print(f"[ë©”ì¼ ì „ì†¡] Colab í…ŒìŠ¤íŠ¸ ëª¨ë“œ â†’ {TO_EMAIL}")
    else:
        # ğŸ”¹ GitHub Actions / ë¡œì»¬ì—ì„œëŠ” ì‹¤ì œ ìˆ˜ì‹ ì
        TO_EMAIL = os.environ.get("TO_EMAIL")
        if not TO_EMAIL:
            print("[ë©”ì¼ ì „ì†¡ X] í”„ë¡œë•ì…˜ ì´ë©”ì¼(TO_EMAIL)ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì¼ì€ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            SEND_EMAIL = False
        else:
            print(f"[ë©”ì¼ ì „ì†¡] í”„ë¡œë•ì…˜ ëª¨ë“œ â†’ {TO_EMAIL}")

# ğŸ”¹ ì‹¤ì œ ë©”ì¼ ì „ì†¡ì€ ì—¬ê¸°ì„œ ìµœì¢…ì ìœ¼ë¡œ í•œ ë²ˆ ë” ì²´í¬
if SEND_EMAIL and TO_EMAIL:
    SUBJECT = f"í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„° | {NEWSLETTER_DATE}"

    with open("newsletter.html", "r", encoding="utf-8") as f:
        html_content = f.read()

    msg = MIMEMultipart("alternative")
    msg["From"] = GMAIL_USER
    msg["To"] = TO_EMAIL
    msg["Subject"] = SUBJECT
    msg.attach(MIMEText(html_content, "html", _charset="utf-8"))

    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
        server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
        server.send_message(msg)

    print("ë©”ì¼ ë³´ëƒˆìŠµë‹ˆë‹¤!")
elif not SEND_EMAIL:
    print("SEND_EMAIL=False ìƒíƒœì´ê±°ë‚˜, ìœ„ ì¡°ê±´ì— ì˜í•´ ë©”ì¼ ì „ì†¡ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("TO_EMAILì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ë©”ì¼ì„ ë³´ë‚´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# # **10. ìµœì¢… í†µê³„ ì¶œë ¥**

# In[61]:


# ============================
# 10. ìµœì¢… í†µê³„ ì¶œë ¥
# ============================
print("\n" + "="*70)
print("ğŸ“Š ë‰´ìŠ¤ë ˆí„° ìƒì„± ìµœì¢… í†µê³„")
print("="*70)

# ============================
# 1. OpenAI í† í° ì‚¬ìš©ëŸ‰
# ============================
print("\nğŸ’° OpenAI API ì‚¬ìš©ëŸ‰:")
print("-" * 50)

try:
    # ì‹¤ì œ ì‚¬ìš©í•œ ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ì¶”ì •
    total_articles = len(df_final) if 'df_final' in globals() else 0
    research_articles = len(research_processed_articles) if 'research_processed_articles' in globals() else 0

    # GPT-4.1-mini í‰ê·  í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
    # - ì¼ë°˜ ê¸°ì‚¬ ìš”ì•½: ì•½ 800 í† í°/ê¸°ì‚¬ (ì…ë ¥ 500 + ì¶œë ¥ 300)
    # - ì—°êµ¬ë…¼ë¬¸ ìš”ì•½: ì•½ 1200 í† í°/ë…¼ë¬¸ (ì…ë ¥ 800 + ì¶œë ¥ 400)

    estimated_article_tokens = total_articles * 800
    estimated_research_tokens = research_articles * 1200
    estimated_total_tokens = estimated_article_tokens + estimated_research_tokens

    print(f"  â€¢ ì¼ë°˜ ê¸°ì‚¬ ì²˜ë¦¬: ì•½ {total_articles}ê±´ Ã— 800 í† í° = {estimated_article_tokens:,} í† í°")
    print(f"  â€¢ ì—°êµ¬ë…¼ë¬¸ ì²˜ë¦¬: ì•½ {research_articles}ê±´ Ã— 1,200 í† í° = {estimated_research_tokens:,} í† í°")
    print(f"  â€¢ ì´ ì˜ˆìƒ í† í°: ì•½ {estimated_total_tokens:,} í† í°")

    # ë¹„ìš© ê³„ì‚° (GPT-4.1-mini ê¸°ì¤€: $0.15/1M input tokens, $0.60/1M output tokens)
    # í‰ê· ì ìœ¼ë¡œ input:output = 65:35 ë¹„ìœ¨ë¡œ ê°€ì •
    input_tokens = int(estimated_total_tokens * 0.65)
    output_tokens = int(estimated_total_tokens * 0.35)

    input_cost = (input_tokens / 1_000_000) * 0.15
    output_cost = (output_tokens / 1_000_000) * 0.60
    total_cost = input_cost + output_cost

    print(f"\n  ğŸ’µ ì˜ˆìƒ ë¹„ìš©:")
    print(f"     - Input í† í°: {input_tokens:,} Ã— $0.15/1M = ${input_cost:.4f}")
    print(f"     - Output í† í°: {output_tokens:,} Ã— $0.60/1M = ${output_cost:.4f}")
    print(f"     - ì´ ë¹„ìš©: ${total_cost:.4f} (ì•½ â‚©{int(total_cost * 1300):,}ì›)")

except Exception as e:
    print(f"  âš ï¸ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚° ì‹¤íŒ¨: {e}")
    total_cost = 0

# ============================
# 2. ë©”ì¸ vs ì¶”ê°€ ê¸°ì‚¬ ë¶„í¬
# ============================
print("\nğŸ“‘ ë©”ì¸ vs ì¶”ê°€ ê¸°ì‚¬:")
print("-" * 50)

if 'topic_main_articles' in globals() and 'topic_extra_articles' in globals():
    for topic_num in [4, 3, 2, 1]:  # ì—­ìˆœìœ¼ë¡œ ì¶œë ¥
        main_count = len(topic_main_articles.get(topic_num, []))
        extra_count = len(topic_extra_articles.get(topic_num, []))

        topic_icon = TOPIC_ICON.get(topic_num, "")
        print(f"{topic_icon} í† í”½ {topic_num}: ë©”ì¸ {main_count}ê°œ | ì¶”ê°€ {extra_count}ê°œ")

# ============================
# 3. í† í”½ë³„ í•œê¸€/ì˜ë¬¸ ê¸°ì‚¬ ìˆ˜
# ============================
print("\nğŸ“° í† í”½ë³„ ê¸°ì‚¬ ë¶„í¬:")
print("-" * 50)

# ì „ì²´ í•œê¸€/ì˜ë¬¸ ê°œìˆ˜ ì§‘ê³„ë¥¼ ìœ„í•œ ë³€ìˆ˜
total_korean_count = 0
total_english_count = 0

if 'df_final' in globals() and not df_final.empty:
    for topic_num in [4, 3, 2, 1]:  # ì—­ìˆœìœ¼ë¡œ ì¶œë ¥
        topic_articles = df_final[df_final["topic_final"] == topic_num]

        if len(topic_articles) == 0:
            continue

        korean_articles = []
        english_articles = []

        for _, row in topic_articles.iterrows():
            # ì›ë¬¸ ì œëª©(original_title)ë¡œ íŒë‹¨
            original_title = str(row.get("original_title") or "")

            # ì›ë¬¸ ì œëª©ì— í•œê¸€ì´ ìˆìœ¼ë©´ í•œê¸€ ê¸°ì‚¬
            has_korean = any('\uac00' <= ch <= '\ud7a3' for ch in original_title)

            if has_korean:
                korean_articles.append(row)
            else:
                english_articles.append(row)

        ko_count = len(korean_articles)
        en_count = len(english_articles)
        total = ko_count + en_count

        # ì „ì²´ ì§‘ê³„ì— ì¶”ê°€
        total_korean_count += ko_count
        total_english_count += en_count

        ko_pct = (ko_count / total * 100) if total > 0 else 0
        en_pct = (en_count / total * 100) if total > 0 else 0

        topic_icon = TOPIC_ICON.get(topic_num, "")
        topic_desc = TOPIC_DESC.get(topic_num, "")

        print(f"\n{topic_icon} í† í”½ {topic_num}: {topic_desc}")
        print(f"  â”œâ”€ ğŸ‡°ğŸ‡· í•œê¸€ ê¸°ì‚¬: {ko_count}ê°œ ({ko_pct:.1f}%)")
        print(f"  â”œâ”€ ğŸ‡ºğŸ‡¸ ì˜ë¬¸ ê¸°ì‚¬: {en_count}ê°œ ({en_pct:.1f}%)")
        print(f"  â””â”€ ğŸ“Š ì´ ê¸°ì‚¬: {total}ê°œ")

# ============================
# 4. ì—°êµ¬ë™í–¥ í†µê³„
# ============================
print("\nğŸ”¬ ì—°êµ¬ë™í–¥ ë…¼ë¬¸:")
print("-" * 50)

if 'research_processed_articles' in globals():
    print(f"  â€¢ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜: {len(research_processed_articles)}ê°œ")

    # ì €ë„ë³„ ë¶„í¬
    if research_processed_articles:
        from collections import Counter
        journal_counts = Counter(
            article.get('journal_name', 'Unknown')
            for article in research_processed_articles
        )

        print(f"  â€¢ ì €ë„ë³„ ë¶„í¬:")
        for journal, count in journal_counts.most_common():
            print(f"     - {journal}: {count}ê°œ")

# ============================
# 5. ìµœì¢… ìš”ì•½ (ìˆ˜ì •ë¨)
# ============================
print()  # ğŸ”¥ ë¹ˆ ì¤„ ì¶”ê°€
print()  # ğŸ”¥ ë¹ˆ ì¤„ ì¶”ê°€
print()  # ğŸ”¥ ë¹ˆ ì¤„ ì¶”ê°€

print("="*70)

total_news = len(df_final) if 'df_final' in globals() else 0
total_research = len(research_processed_articles) if 'research_processed_articles' in globals() else 0

# ğŸ”¥ ìˆ˜ì •: ì¼ë°˜ ë‰´ìŠ¤ì— í•œê¸€/ì˜ë¬¸ ê°œìˆ˜ í‘œì‹œ
print(f"ğŸ“° ì¼ë°˜ ë‰´ìŠ¤: {total_news}ê°œ (ì˜ë¬¸: {total_english_count}ê°œ, í•œê¸€: {total_korean_count}ê°œ)")
print(f"ğŸ”¬ ì—°êµ¬ë™í–¥: {total_research}ê°œ")
print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${total_cost:.4f} (ì•½ â‚©{int(total_cost * 1300):,}ì›)" if total_cost > 0 else "ğŸ’° ì˜ˆìƒ ë¹„ìš©: ê³„ì‚° ë¶ˆê°€")
print("="*70)
print("âœ… ë‰´ìŠ¤ë ˆí„° ìƒì„± ì™„ë£Œ!")
print("="*70 + "\n")


# # **11. Colabì—ì„œë§Œ ì‹¤í–‰: í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ GitHubì— ìë™ ì—…ë¡œë“œ**

# In[ ]:


# ============================================================
# Colab ì „ìš©: í˜„ì¬ ë…¸íŠ¸ë¶ ì „ì²´ë¥¼ .pyë¡œ ë³€í™˜í•´ GitHubì˜ newsletter.pyë¥¼ êµì²´
# + (ì¶”ê°€) newsletter_history/ í´ë”ì— íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ë¡œ ìŠ¤ëƒ…ìƒ· ì €ì¥
# ============================================================

def _in_colab() -> bool:
    try:
        import google.colab  # noqa: F401
        return True
    except Exception:
        return False

if _in_colab():
    import os
    import json
    import base64
    import requests

    # 1) GitHub ì„¤ì • (í•„ìˆ˜)
    GITHUB_OWNER = os.environ.get("GITHUB_OWNER", "HANCOM-InSpace")
    GITHUB_REPO  = os.environ.get("GITHUB_REPO", "Weekly-Newsletter")
    BRANCH       = os.environ.get("GITHUB_BRANCH", "main")
    TARGET_PATH  = "newsletter.py"

    # Colab Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ í† í° ì œê³µ (ë‘˜ ì¤‘ í•˜ë‚˜)
    GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
    if not GITHUB_TOKEN:
        try:
            from google.colab import userdata
            GITHUB_TOKEN = userdata.get("GITHUB_TOKEN")
        except Exception:
            GITHUB_TOKEN = None

    if not GITHUB_TOKEN:
        raise RuntimeError("GITHUB_TOKENì´ ì—†ìŠµë‹ˆë‹¤. Colab Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")

    # 2) í˜„ì¬ ë…¸íŠ¸ë¶(.ipynb)ì„ 'íŒŒì¼ ì—†ì´' ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (Colab API)
    from google.colab import _message
    nb = _message.blocking_request("get_ipynb", timeout_sec=30)
    nb_json = nb["ipynb"]

    # 3) ë…¸íŠ¸ë¶ JSON -> Python(.py) ë³€í™˜ (nbconvert)
    import nbformat
    from nbconvert.exporters import PythonExporter

    nb_node = nbformat.reads(json.dumps(nb_json), as_version=4)
    exporter = PythonExporter()
    py_text, _ = exporter.from_notebook_node(nb_node)

    # 4) GitHub APIë¡œ ê¸°ì¡´ newsletter.py SHA ì¡°íšŒ í›„ ì—…ë°ì´íŠ¸(PUT)
    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{TARGET_PATH}"
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    # ê¸°ì¡´ íŒŒì¼ ì¡°íšŒ(sha í•„ìš”)
    r = requests.get(api_url, headers=headers, params={"ref": BRANCH})
    if r.status_code == 200:
        sha = r.json()["sha"]
    elif r.status_code == 404:
        sha = None
    else:
        raise RuntimeError(f"GitHub GET ì‹¤íŒ¨: {r.status_code} / {r.text}")

    content_b64 = base64.b64encode(py_text.encode("utf-8")).decode("utf-8")

    # ì»¤ë°‹ ë©”ì‹œì§€ (ì›í•˜ë©´ ìˆ˜ì •)
    commit_msg = "Update newsletter.py from Colab notebook"

    payload = {
        "message": commit_msg,
        "content": content_b64,
        "branch": BRANCH,
    }
    if sha:
        payload["sha"] = sha

    u = requests.put(api_url, headers=headers, json=payload)
    if u.status_code not in (200, 201):
        raise RuntimeError(f"GitHub PUT ì‹¤íŒ¨: {u.status_code} / {u.text}")

    print(f"âœ… GitHubì— {TARGET_PATH} ì—…ë°ì´íŠ¸ ì™„ë£Œ: {GITHUB_OWNER}/{GITHUB_REPO}@{BRANCH}")

    # ============================================================
    # (ì¶”ê°€) íˆìŠ¤í† ë¦¬ íŒŒì¼ ì €ì¥:
    # newsletter_history/newsletter_YYYY_MM_DD_HH_MM.py
    # ============================================================
    HISTORY_DIR = "newsletter_history"

    # KST íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ê°™ì€ ë¶„ì— 2ë²ˆ ì €ì¥í•˜ë©´ ì¶©ëŒí•  ìˆ˜ ìˆì–´ ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    from datetime import datetime, timezone, timedelta
    KST = timezone(timedelta(hours=9))
    ts = datetime.now(KST).strftime("%Y_%m_%d_%H_%M")

    history_path = f"{HISTORY_DIR}/newsletter_{ts}.py"
    api_url_hist = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{history_path}"

    payload_hist = {
        "message": f"Archive snapshot: {history_path}",
        "content": content_b64,
        "branch": BRANCH,
    }

    resp = requests.put(api_url_hist, headers=headers, json=payload_hist)

    if resp.status_code in (200, 201):
        print(f"âœ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {history_path}")
    else:
        # ê°™ì€ ë¶„ì— ì¤‘ë³µ ì €ì¥ ë“±ìœ¼ë¡œ ì‹¤íŒ¨ ì‹œ -> ì´ˆê¹Œì§€ ë¶™ì—¬ ì¬ì‹œë„
        ts2 = datetime.now(KST).strftime("%Y_%m_%d_%H_%M_%S")
        history_path2 = f"{HISTORY_DIR}/newsletter_{ts2}.py"
        api_url_hist2 = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{history_path2}"

        payload_hist["message"] = f"Archive snapshot: {history_path2}"
        resp2 = requests.put(api_url_hist2, headers=headers, json=payload_hist)

        if resp2.status_code in (200, 201):
            print(f"âœ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ(ì´ˆ í¬í•¨): {history_path2}")
        else:
            raise RuntimeError(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {resp2.status_code} / {resp2.text}")

else:
    # Colabì´ ì•„ë‹Œ í™˜ê²½(ì˜ˆ: GitHub Actions)ì—ì„œëŠ” ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    pass

