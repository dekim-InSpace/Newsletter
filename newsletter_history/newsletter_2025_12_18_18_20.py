#!/usr/bin/env python
# coding: utf-8

# # **ë‰´ìŠ¤ë ˆí„° ì‹¤í–‰ ë°©ë²•**
# **ì¢Œì¸¡ìƒë‹¨ "ëª¨ë‘ ì‹¤í–‰" í´ë¦­**
# <br>
# ![image.png](data:image/png;base64,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)
# 

# # **01-1 ì„¤ì¹˜ & import**

# In[ ]:


# ============================
# ğŸ”— GitHub ì—°ë™ ì„¤ì • (Colab ì „ìš©)
# ============================
try:
    from google.colab import userdata
    IN_COLAB = True
    print("âœ… Google Colab í™˜ê²½ ê°ì§€ë¨")
except:
    IN_COLAB = False
    print("âš ï¸ ë¡œì»¬/GitHub Actions í™˜ê²½")

if IN_COLAB:
    import os

    # Colab Secretsì—ì„œ API í‚¤ ì½ê¸°
    os.environ["GITHUB_TOKEN"] = userdata.get('GITHUB_TOKEN')
    os.environ["OPENAI_API_KEY"] = userdata.get('OPENAI_API_KEY')
    os.environ["NEWSAPI_KEY"] = userdata.get('NEWSAPI_KEY')
    os.environ["NEWSDATA_API_KEY"] = userdata.get('NEWSDATA_API_KEY')
    os.environ["GNEWS_API_KEY"] = userdata.get('GNEWS_API_KEY')
    os.environ["MEDIASTACK_API_KEY"] = userdata.get('MEDIASTACK_API_KEY')
    os.environ["SERPAPI_KEY"] = userdata.get('SERPAPI_KEY')
    os.environ["CURRENTS_API_KEY"] = userdata.get('CURRENTS_API_KEY')
    os.environ["NAVER_CLIENT_ID"] = userdata.get('NAVER_CLIENT_ID')
    os.environ["NAVER_CLIENT_SECRET"] = userdata.get('NAVER_CLIENT_SECRET')

    os.environ["GMAIL_USER"] = userdata.get('GMAIL_USER')
    os.environ["GMAIL_APP_PASSWORD"] = userdata.get('GMAIL_APP_PASSWORD')
    os.environ["TO_EMAIL"] = userdata.get('TO_EMAIL')
    os.environ["TO_EMAIL_TEST"] = userdata.get('TO_EMAIL_TEST')


    print("âœ… Colab Secretsì—ì„œ API í‚¤ ë¡œë“œ ì™„ë£Œ")


# # **01-2 ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜**

# In[ ]:


# ============================
# 1. ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„¤ì¹˜
# ============================
if IN_COLAB:
    get_ipython().system('pip install -q requests pandas python-dateutil openai beautifulsoup4 feedparser')

import os
import time
import json
import requests
import pandas as pd
import difflib
import re

from datetime import datetime, timedelta, timezone

import feedparser
from dateutil import parser as dateparser  # ì´ë¯¸ ìˆë‹¤ë©´ ìƒëµ

from openai import OpenAI
from bs4 import BeautifulSoup
from urllib.parse import urljoin
from html import escape as h
# IPythonì€ Colabì—ì„œë§Œ í•„ìš”
try:
    from IPython.display import HTML, display
except ImportError:
    # GitHub Actions í™˜ê²½ìš© ë”ë¯¸ í•¨ìˆ˜
    def display(obj):
        pass
    def HTML(text):
        return text
    pass
import smtplib
from email.mime.multipart import MIMEMultipart
from email.mime.text import MIMEText
from concurrent.futures import ThreadPoolExecutor, as_completed


# # **02-1 ì„¤ì • (API í‚¤)**

# In[ ]:


# ============================================================
# 2. API í‚¤ & ê¸°ë³¸ ì„¤ì •
# ============================================================

OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
NEWSAPI_KEY = os.environ.get("NEWSAPI_KEY")
NEWSDATA_API_KEY = os.environ.get("NEWSDATA_API_KEY")
GNEWS_API_KEY = os.environ.get("GNEWS_API_KEY")
MEDIASTACK_API_KEY = os.environ.get("MEDIASTACK_API_KEY")
SERPAPI_KEY = os.environ.get("SERPAPI_KEY")
CURRENTS_API_KEY = os.environ.get("CURRENTS_API_KEY")
NAVER_CLIENT_ID = os.environ.get("NAVER_CLIENT_ID")
NAVER_CLIENT_SECRET = os.environ.get("NAVER_CLIENT_SECRET")



NEWSDATA_BASE_URL_LATEST = "https://newsdata.io/api/1/latest"



# # **02-2 ì„¤ì • (ë‚ ì§œ, ì£¼ì œ, í‚¤ì›Œë“œ, ìƒìˆ˜)**

# In[ ]:


# ì‚¬ìš©í•  GPT mini ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gpt-4.1-mini", ë‚˜ì¤‘ì— "gpt-5.1-mini"ë¡œ êµì²´ ê°€ëŠ¥)
MODEL_NAME = "gpt-4.1-mini"

os.environ["OPENAI_API_KEY"] = OPENAI_API_KEY
client = OpenAI()

# ============================================================
# GitHub Pages ì„¤ì •
# ============================================================
# GitHub ê³„ì • ì•„ì´ë””ì™€ ë ˆí¬ ì´ë¦„
GITHUB_OWNER = "hancom-inspace"              # ì˜ˆ: "junwoo0920"
GITHUB_REPO  = "Weekly-Newsletter"    # GitHubì—ì„œ ë§Œë“  ë ˆí¬ ì´ë¦„

# GitHub Pages ìµœì¢… URL (Settings â†’ Pagesì— í‘œì‹œë˜ëŠ” ì£¼ì†Œ)
# ë³´í†µ í˜•íƒœ: https://{owner}.github.io/{repo}
BASE_URL = f"https://{GITHUB_OWNER}.github.io/{GITHUB_REPO}"

# GitHub Personal Access Token (Colab ì…€ì—ì„œ os.environ["GITHUB_TOKEN"] ë¡œ ë¯¸ë¦¬ ë„£ì–´ë‘” ê°’)
GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")


# í—¤ë” ì´ë¯¸ì§€ / ë¡œê³  / ê¸°ë³¸ ì¸ë„¤ì¼
HEADER_BACKGROUND = "https://static.wixstatic.com/media/b749e1_60701dc82eb542149d95637ce7a34802~mv2_d_5568_3712_s_4_2.jpg/v1/fill/w_1553,h_1071,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/b749e1_60701dc82eb542149d95637ce7a34802~mv2_d_5568_3712_s_4_2.jpg"
LOGO_URL = "https://static.wixstatic.com/media/0c4f16_c8132c52e3ef485286a1f0ec457a5cdb~mv2.png/v1/fill/w_111,h_44,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/logotype_vert_rgb_v2__w__w500.png%201x,%20https://static.wixstatic.com/media/0c4f16_c8132c52e3ef485286a1f0ec457a5cdb~mv2.png/v1/fill/w_222,h_88,al_c,q_85,usm_0.66_1.00_0.01,enc_avif,quality_auto/logotype_vert_rgb_v2__w__w500.png%202x"
DEFAULT_THUMB = "https://e0.pxfuel.com/wallpapers/597/629/desktop-wallpaper-fun-funny-funny-funny-space.jpg"

CONTENT_WIDTH = 700

# ============================
# 2. ë‚ ì§œ ìë™ ì„¤ì • (UTC ê¸°ì¤€ 7ì¼ ì „ ~ ì˜¤ëŠ˜)
# ============================
# KST ì‹œê°„ëŒ€ ì •ì˜
KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST)

# ê¸°ì¤€ì¼ (ì‹¤í–‰ì¼)
anchor_date = now_kst.date()

# ìˆ˜ì§‘ ì¢…ë£Œì¼: ì „ë‚ 
end_date_kst = anchor_date - timedelta(days=1)

# ìˆ˜ì§‘ ì‹œì‘ì¼: ì¢…ë£Œì¼ ê¸°ì¤€ 6ì¼ ì „
start_date_kst = end_date_kst - timedelta(days=6)

# KST â†’ UTC ë³€í™˜
date_from_utc = datetime.combine(
    start_date_kst, datetime.min.time()
).replace(tzinfo=KST).astimezone(timezone.utc)

date_to_utc = datetime.combine(
    end_date_kst, datetime.max.time()
).replace(tzinfo=KST).astimezone(timezone.utc)

DATE_FROM = date_from_utc.strftime("%Y-%m-%d")
DATE_TO   = date_to_utc.strftime("%Y-%m-%d")


print("=" * 60)
print(f"ğŸ• í˜„ì¬ KST ì‹œê°„: {now_kst.strftime('%Y-%m-%d %H:%M:%S')}")
print(f"ğŸ“… ê²€ìƒ‰ ë²”ìœ„ (KST): {start_date_kst} ~ {end_date_kst}")
print(f"ğŸ“… ê²€ìƒ‰ ë²”ìœ„ (UTC): {DATE_FROM} ~ {DATE_TO}")
print("=" * 60)


# ì˜ì–´ + í•œêµ­ì–´ ê¸°ì‚¬ ê°™ì´ ì‚¬ìš©
LANGUAGES = ["en", "ko"]

# ============================
# 2. ì£¼ì œ ì •ì˜ & í‚¤ì›Œë“œ
# ============================
TOPIC_DESC = {
    1: "GeoINT / ì§€ë¦¬ê³µê°„ ì •ë³´, ìœ„ì„±Â·ì§€ë„ ê¸°ë°˜ ì •ë³´ ë¶„ì„",
    2: "í•­ê³µ / ë“œë¡ Â·ë¬´ì¸ê¸°(UAV/UAS) ë™í–¥",
    3: "AI ë°ì´í„°Â·ë¶„ì„ í”Œë«í¼ / MLOpsÂ·ìë™í™”",
    4: "ìœ„ì„± ì˜ìƒ / SARÂ·ê´‘í•™Â·í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´ ì´ë¯¸ì§€ ë¶„ì„"
}

TOPIC_ICON = {
    1: "ğŸŒ",   # GEOINT
    2: "âœˆï¸",   # í•­ê³µ
    3: "ğŸ¤–",   # AI í”Œë«í¼
    4: "ğŸ›°ï¸",   # ìœ„ì„±ì˜ìƒ
}

# ì˜ì–´ + í•œê¸€ í‚¤ì›Œë“œ í˜¼í•© (âœ… ì •ë¦¬ë³¸ + í•œê¸€ ë³´ê°•)
TOPIC_KEYWORDS = {
    1: [
        # GeoINT / Geospatial / GIS / Location Intelligence
        "geospatial intelligence", "GeoINT",
        "geospatial analytics", "location intelligence",
        "geospatial data", "geospatial mapping",
        "GIS platform", "geospatial security",
        "OSINT geospatial", "satellite intelligence",
        "geospatial surveillance", "geospatial AI",
        "spatial analytics", "geospatial platform",
        "GIS software", "geospatial data platform",
        "digital twin GIS", "smart city GIS",

        # KR ë³´ê°•
        "ì§€ë¦¬ê³µê°„ ì •ë³´", "ì§€ë¦¬ê³µê°„ì •ë³´", "ê³µê°„ì •ë³´",
        "ê³µê°„ì •ë³´ì‚°ì—…", "ê³µê°„ì •ë³´ì‹œìŠ¤í…œ", "ì§€ë¦¬ì •ë³´ì‹œìŠ¤í…œ",
        "GIS", "GIS í”Œë«í¼", "ê³µê°„ ë¶„ì„", "ê³µê°„ë¶„ì„",
        "ì§€ë¦¬ê³µê°„ ë°ì´í„°", "ê³µê°„ ë°ì´í„°", "ê³µê°„ë°ì´í„°",
        "ìœ„ì¹˜ ê¸°ë°˜ ì„œë¹„ìŠ¤", "ìœ„ì¹˜ê¸°ë°˜ ì„œë¹„ìŠ¤", "ìœ„ì¹˜ì •ë³´",
        "ì§€ë„ ì„œë¹„ìŠ¤", "ì§€ë„ ë°ì´í„°", "êµ­í† ì •ë³´",
        "êµ­í† ì§€ë¦¬ì •ë³´", "êµ­í† ì§€ë¦¬ì •ë³´ì›",
        "ì¸¡ëŸ‰", "ì¸¡ëŸ‰ ê¸°ìˆ ", "ì§€ì ", "ë””ì§€í„¸ íŠ¸ìœˆ", "ë””ì§€í„¸íŠ¸ìœˆ",
        "ìŠ¤ë§ˆíŠ¸ì‹œí‹°", "ê³µê°„ ë¹…ë°ì´í„°", "ê³µê°„ë¹…ë°ì´í„°",
        "ì˜¤í”ˆì†ŒìŠ¤ GIS",
    ],

    2: [
        # Drone / UAV-UAS / UAM + regulation + operations
        "drone industry",
        "UAV technology", "UAS technology",
        "urban air mobility", "UAM",
        "drone regulation", "BVLOS drone",
        "autonomous drone",
        "drone delivery", "drone inspection",
        "drone surveillance",
        "drone traffic management", "UTM",
        "counter drone", "anti-drone",
        "eVTOL", "AAM",
        "drone swarm", "drone security",

        # KR ë³´ê°•
        "ë“œë¡ ", "ë“œë¡  ì‚°ì—…", "ë“œë¡ ì‚°ì—…",
        "ë¬´ì¸ê¸°", "ë¬´ì¸í•­ê³µê¸°", "ë¬´ì¸í•­ê³µ",
        "UAV", "UAS",
        "ë„ì‹¬ í•­ê³µ ëª¨ë¹Œë¦¬í‹°", "ë„ì‹¬í•­ê³µëª¨ë¹Œë¦¬í‹°",
        "UAM ê·œì œ", "UAM", "AAM", "ë¯¸ë˜í•­ê³µëª¨ë¹Œë¦¬í‹°",
        "ë“œë¡  ê·œì œ", "ë“œë¡  ì•ˆì „", "ë“œë¡  ì¸ì¦",
        "BVLOS", "ì‹œê³„ì™¸ ë¹„í–‰", "ì‹œê³„ì™¸ë¹„í–‰",
        "ë“œë¡  ë°°ì†¡", "ë“œë¡ íƒë°°",
        "ë“œë¡  ì ê²€", "ë“œë¡  ê²€ì‚¬", "ì‹œì„¤ë¬¼ ì ê²€ ë“œë¡ ",
        "ë“œë¡  ê°ì‹œ", "ë“œë¡  ì •ì°°", "ë“œë¡  ë°©ì‚°", "ë“œë¡  ë°©ì–´",
        "ë“œë¡  ê´€ì œ", "ë“œë¡ êµí†µê´€ë¦¬", "UTM",
        "K-ë“œë¡ ", "ë“œë¡  ì‹¤ì¦", "ë“œë¡ íŠ¹ë³„ììœ í™”",
    ],

    3: [
        # Enterprise AI / Data+Analytics Platform / MLOps / GenAI Ops
        "AI data platform", "data analytics platform",
        "MLOps platform",
        "enterprise AI", "enterprise AI platform",
        "generative AI platform", "LLM platform",
        "RAG platform", "retrieval augmented generation",
        "LLMOps",
        "vector database", "feature store",
        "model deployment", "model serving",
        "AI infrastructure", "AI cloud platform",
        "data lakehouse", "data lake", "data warehouse",
        "data pipeline", "orchestration",
        "model monitoring", "model governance",

        # KR ë³´ê°•
        "AI ë°ì´í„° í”Œë«í¼", "ë°ì´í„° ë¶„ì„ í”Œë«í¼", "ë¶„ì„ í”Œë«í¼",
        "ë°ì´í„° ë ˆì´í¬", "ë°ì´í„°ë ˆì´í¬", "ë ˆì´í¬í•˜ìš°ìŠ¤", "ë°ì´í„° ì›¨ì–´í•˜ìš°ìŠ¤", "ë°ì´í„°ì›¨ì–´í•˜ìš°ìŠ¤",
        "ë°ì´í„° íŒŒì´í”„ë¼ì¸", "ë°ì´í„°íŒŒì´í”„ë¼ì¸", "ETL", "ELT", "ì›Œí¬í”Œë¡œ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜",
        "MLOps", "LLMOps", "ëª¨ë¸ì˜µìŠ¤",
        "ì—”í„°í”„ë¼ì´ì¦ˆ AI", "ê¸°ì—…ìš© AI", "ì—…ë¬´ìš© AI",
        "ìƒì„±í˜• AI", "ìƒì„±í˜•AI", "ìƒì„±í˜• AI í”Œë«í¼", "LLM í”Œë«í¼",
        "RAG", "RAG í”Œë«í¼", "ê²€ìƒ‰ì¦ê°•ìƒì„±",
        "ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤", "ë²¡í„°DB", "ì„ë² ë”©", "í”¼ì²˜ ìŠ¤í† ì–´", "íŠ¹ì„± ì €ì¥ì†Œ",
        "ëª¨ë¸ ë°°í¬", "ëª¨ë¸ì„œë¹™", "ëª¨ë¸ ì„œë¹™",
        "ëª¨ë¸ ëª¨ë‹ˆí„°ë§", "ëª¨ë¸ ê±°ë²„ë„ŒìŠ¤", "AI ê±°ë²„ë„ŒìŠ¤",
        "AI ì¸í”„ë¼", "AI í´ë¼ìš°ë“œ í”Œë«í¼", "GPU ì¸í”„ë¼",
        "AI ì—…ë¬´ ìë™í™”", "ë””ì§€í„¸ ì „í™˜", "DX",
    ],

    4: [
        # Satellite imagery / EO / SAR-Optical-Hyperspectral / Remote Sensing
        "satellite imagery analysis", "satellite imagery processing",
        "SAR satellite analytics",
        "optical satellite imagery analysis",
        "hyperspectral satellite data",
        "earth observation data", "EO data platform",
        "satellite data analytics",
        "commercial satellite imagery",
        "remote sensing analytics",
        "synthetic aperture radar", "SAR imagery",
        "multispectral imagery", "hyperspectral imagery",
        "change detection satellite", "satellite image segmentation",

        # KR ë³´ê°•
        "ìœ„ì„± ì˜ìƒ", "ìœ„ì„±ì˜ìƒ", "ìœ„ì„±ì˜ìƒ ë¶„ì„", "ìœ„ì„± ì˜ìƒ ë¶„ì„",
        "ìœ„ì„±ì˜ìƒ ì²˜ë¦¬", "ìœ„ì„± ì´ë¯¸ì§€", "ìœ„ì„± ì´ë¯¸ì§€ ë¶„ì„",
        "ì§€êµ¬ê´€ì¸¡", "ì§€êµ¬ ê´€ì¸¡", "ì§€êµ¬ê´€ì¸¡ ë°ì´í„°", "EO", "ì§€êµ¬ê´€ì¸¡ ìœ„ì„±",
        "ì›ê²©íƒì‚¬", "ì›ê²© íƒì‚¬", "ì›ê²©íƒì‚¬ ë°ì´í„°",
        "SAR", "SAR ìœ„ì„±", "í•©ì„±ê°œêµ¬ë ˆì´ë‹¤", "í•©ì„±ê°œêµ¬ ë ˆì´ë”", "ë ˆì´ë” ìœ„ì„±",
        "ê´‘í•™ ìœ„ì„±", "ê´‘í•™ ìœ„ì„± ì˜ìƒ", "ê´‘í•™ìœ„ì„±",
        "ì´ˆë¶„ê´‘", "í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´", "í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´ ì˜ìƒ", "ì´ˆë¶„ê´‘ ì˜ìƒ",
        "ë‹¤ì¤‘ë¶„ê´‘", "ë©€í‹°ìŠ¤í™íŠ¸ëŸ´", "ë‹¤ì¤‘ë¶„ê´‘ ì˜ìƒ",
        "ìœ„ì„± ë°ì´í„°", "ìœ„ì„±ë°ì´í„°", "ìœ„ì„± ë°ì´í„° ë¶„ì„", "ìœ„ì„±ë°ì´í„° ë¶„ì„",
        "ìœ„ì„± ë°ì´í„° í”Œë«í¼", "ìœ„ì„±ë°ì´í„° í”Œë«í¼",
        "ìœ„ì„± ì„œë¹„ìŠ¤", "ì§€í‘œ ë³€í™” íƒì§€", "ë³€í™” íƒì§€",
    ],
}



# 1ì°¨ í›„ë³´ ê°œìˆ˜ (í† í”½ë‹¹ NewsAPIì—ì„œ ë„‰ë„‰íˆ ê°€ì ¸ì˜¤ê¸°)
ARTICLES_PER_TOPIC_RAW = 100

# ì–¸ì–´ë³„ ëª©í‘œ ê°œìˆ˜ (í•œê¸€ 30% : ì˜ì–´ 70%)
ARTICLES_PER_LANG_KO = 30   # í•œê¸€: 30%
ARTICLES_PER_LANG_EN = 70   # ì˜ì–´: 70%

# ìµœì¢… ë‰´ìŠ¤ë ˆí„°ì— ë°˜ë“œì‹œ ë³´ì—¬ì¤„ ê°œìˆ˜
ARTICLES_PER_TOPIC_FINAL = 3

# [ì¶”ê°€] í•œ í† í”½ì—ì„œ "ë©”ì¸ 3 + ì¶”ê°€ ìµœì†Œ 6"ì„ í™•ë³´í•˜ê¸° ìœ„í•œ ì „ì²´ ìµœì†Œ ê°œìˆ˜
MIN_TOTAL_PER_TOPIC = ARTICLES_PER_TOPIC_FINAL + 6  # 3 + 6 = 9


# # **03 NewsAPIë¡œ ê¸°ì‚¬ ìˆ˜ì§‘**

# In[ ]:


# ============================
# 3. NewsAPIì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘
# ============================
def search_news_newsapi(query, from_date, to_date, language=None, page_size=50):
    """
    NewsAPI (2ìˆœìœ„ ë³´ì¡° ì†ŒìŠ¤ 1)
    """
    url = "https://newsapi.org/v2/everything"
    params = {
        "q": query,
        "from": from_date,
        "to": to_date,
        "sortBy": "publishedAt",
        "pageSize": page_size,
        "apiKey": NEWSAPI_KEY,
    }
    if language:
        params["language"] = language

    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    return r.json().get("articles", [])


def search_news_mediastack(query, from_date, to_date, language=None, page_size=30):
    """
    MediaStack (2ìˆœìœ„ ë³´ì¡° ì†ŒìŠ¤ 2)
    - ë¬´ë£Œ í”Œëœì´ë©´ HTTPSê°€ ì•ˆë  ìˆ˜ë„ ìˆì–´ì„œ httpë¡œ ì¨ì•¼ í•  ë•Œë„ ìˆìŒ. (ë¬¸ì„œ ì°¸ê³ )
    """
    url = "http://api.mediastack.com/v1/news"
    limit = min(page_size, 50)

    params = {
        "access_key": MEDIASTACK_API_KEY,
        "keywords": query,
        "date_from": from_date,
        "date_to": to_date,
        "sort": "published_desc",
        "limit": limit,
    }
    if language:
        # mediastackëŠ” ì–¸ì–´ ì½”ë“œê°€ en, ko ë“± (ê³µì‹ ë¬¸ì„œ í™•ì¸)
        params["languages"] = language

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] MediaStack error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    articles = []
    for item in data.get("data", []):
        articles.append({
            "source": {"name": item.get("source")},
            "author": item.get("author"),
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("description"),
            "url": item.get("url"),
            "publishedAt": item.get("published_at"),  # ë˜ëŠ” published_at/created_at í™•ì¸ í•„ìš”
        })
    return articles

def search_news_serpapi(query, from_date, to_date, language=None, page_size=30):
    """
    SerpAPI Google News (3ìˆœìœ„ ë°±ì—… ì†ŒìŠ¤)
    """
    url = "https://serpapi.com/search"
    num = min(page_size, 20)

    params = {
        "engine": "google_news",
        "q": query,
        "api_key": SERPAPI_KEY,
        "num": num,
    }
    if language:
        params["hl"] = language  # hl=ko, hl=en ë“±

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] SerpAPI error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    results = data.get("news_results") or []
    articles = []
    for item in results:
        # published_dateëŠ” ì‚¬ëŒì´ ì½ê¸° ì¢‹ì€ ë¬¸ìì—´ì¼ ìˆ˜ ìˆìŒ ("3 hours ago" ì´ëŸ° ì‹ì´ë©´ ë‚˜ì¤‘ì— í•„í„°ì—ì„œ ê±¸ëŸ¬ì§ˆ ìˆ˜ ìˆìŒ)
        articles.append({
            "source": {"name": item.get("source")},
            "author": None,
            "title": item.get("title"),
            "description": item.get("snippet"),
            "content": item.get("snippet"),
            "url": item.get("link"),
            "publishedAt": item.get("date") or item.get("published_date"),
        })
    return articles


def search_news_currents(query, from_date, to_date, language=None, page_size=30):
    """
    Currents API (3ìˆœìœ„ ë°±ì—… ì†ŒìŠ¤)
    """
    url = "https://api.currentsapi.services/v1/search"
    limit = min(page_size, 50)

    params = {
        "apiKey": CURRENTS_API_KEY,
        "keywords": query,
        "limit": limit,
    }
    if language:
        params["language"] = language

    # ë‚ ì§œ í•„í„°ëŠ” í”Œëœì— ë”°ë¼ ë™ì‘ ë°©ì‹ì´ ë‹¬ë¼ì„œ, í•„ìš”ì‹œ ë¬¸ì„œ ë³´ê³  ë§ê²Œ ì¡°ì •
    params["start_date"] = from_date
    params["end_date"] = to_date

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] Currents error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    results = data.get("news") or []
    articles = []
    for item in results:
        articles.append({
            "source": {"name": item.get("author")},
            "author": item.get("author"),
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("description"),
            "url": item.get("url"),
            "publishedAt": item.get("published"),  # ISO8601ì¸ì§€ í™•ì¸ í•„ìš”
        })
    return articles

import xml.etree.ElementTree as ET
from html import unescape

def search_news_google_rss_kr(query, from_date, to_date, language=None, page_size=30):
    """
    Google News RSS Search (KR/KO fallback)
    - hl=ko, gl=KR, ceid=KR:ko ê³ ì •
    - RSSëŠ” date íŒŒë¼ë¯¸í„°ê°€ ì•½í•˜ë¯€ë¡œ, ë°›ì•„ì˜¨ ë’¤ from/toë¡œ ë¡œì»¬ í•„í„°
    """
    # Google News RSS search endpoint
    # qëŠ” URL íŒŒë¼ë¯¸í„°ë¡œ ë“¤ì–´ê°€ë¯€ë¡œ requests paramsë¥¼ ì‚¬ìš©
    url = "https://news.google.com/rss/search"

    params = {
        "q": query,
        "hl": "ko",
        "gl": "KR",
        "ceid": "KR:ko",
    }

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] GoogleRSS error (q={query}): {e}")
        return []

    # XML íŒŒì‹±
    try:
        root = ET.fromstring(r.text)
    except Exception as e:
        print(f"[WARN] GoogleRSS XML parse error: {e}")
        return []

    # ë‚ ì§œ í•„í„° ì¤€ë¹„
    try:
        dt_from = dateparser.parse(from_date).date()
        dt_to = dateparser.parse(to_date).date()
    except Exception:
        dt_from = None
        dt_to = None

    items = root.findall(".//item")
    articles = []

    for it in items:
        title = it.findtext("title") or ""
        link = it.findtext("link") or ""
        pub_date = it.findtext("pubDate") or ""
        desc = it.findtext("description") or ""

        # descriptionì€ HTMLì´ ì„ì´ë¯€ë¡œ unescape + íƒœê·¸ ëŒ€ì¶© ì œê±°
        desc = unescape(desc)
        desc = re.sub(r"<[^>]+>", " ", desc)
        desc = re.sub(r"\s+", " ", desc).strip()

        # pubDate íŒŒì‹± & ë²”ìœ„ í•„í„°
        if pub_date:
            try:
                d = dateparser.parse(pub_date)
                if d is not None:
                    d_date = d.date()
                    if dt_from and d_date < dt_from:
                        continue
                    if dt_to and d_date > dt_to:
                        continue
            except Exception:
                pass

        if not link:
            continue

        articles.append({
            "source": {"name": "Google News RSS (KR)"},
            "author": None,
            "title": unescape(title).strip(),
            "description": desc,
            "content": "",
            "url": link.strip(),
            "publishedAt": pub_date,
        })

        if len(articles) >= page_size:
            break

    return articles


def _normalize_naver_query(q: str) -> str:
    """
    Naver Search APIìš© query ì •ë¦¬.
    - NaverëŠ” Googleì²˜ëŸ¼ boolean OR/AND/NOTì„ ì•ˆì •ì ìœ¼ë¡œ ì§€ì›í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì–´ ì œê±°(ê³µë°± ì¹˜í™˜)
    - ê´„í˜¸/ë”°ì˜´í‘œ/ì¤‘ë³µ ê³µë°± ì •ë¦¬
    - ë„ˆë¬´ ê¸´ ì¿¼ë¦¬ëŠ” ì¬í˜„ìœ¨/í’ˆì§ˆì— ë¶ˆë¦¬í•  ìˆ˜ ìˆì–´ ë³´ìˆ˜ì ìœ¼ë¡œ ê¸¸ì´ ì œí•œ
    """
    q = (q or "").strip()
    if not q:
        return ""

    # boolean í† í° ì œê±°(ëŒ€/ì†Œë¬¸ì ëª¨ë‘)
    q = re.sub(r"\b(OR|AND|NOT)\b", " ", q, flags=re.IGNORECASE)

    # ê´„í˜¸ ì œê±°
    q = q.replace("(", " ").replace(")", " ")

    # ë”°ì˜´í‘œ ì œê±°(ê¸¸ì´ë§Œ ëŠ˜ê³  íš¨ê³¼ê°€ ì ì€ ê²½ìš°ê°€ ë§ìŒ)
    q = q.replace('"', " ")

    # ê³µë°± ì •ë¦¬
    q = re.sub(r"\s+", " ", q).strip()

    # ê¸¸ì´ ì œí•œ
    if len(q) > 120:
        q = q[:120].rsplit(" ", 1)[0].strip() or q[:120].strip()

    return q


def search_news_naver(query, from_date, to_date, language=None, page_size=30):
    """
    Naver Search API - News (KR fallback)
    - query: UTF-8
    - sort=date (ìµœì‹ ìˆœ)
    - display: ìµœëŒ€ 100 (ë„¤ì´ë²„ ì œí•œ)
    """
    if not NAVER_CLIENT_ID or not NAVER_CLIENT_SECRET:
        # í‚¤ ì—†ìœ¼ë©´ ì¡°ìš©íˆ ìŠ¤í‚µ (ë¡œê·¸ë§Œ)
        print("[WARN] Naver API keys missing: NAVER_CLIENT_ID / NAVER_CLIENT_SECRET")
        return []

    # NaverëŠ” boolean ORì´ ì•ˆì •ì ìœ¼ë¡œ ë™ì‘í•˜ì§€ ì•Šì„ ìˆ˜ ìˆì–´ queryë¥¼ ì •ê·œí™”
    query = _normalize_naver_query(query)
    if not query:
        return []

    url = "https://openapi.naver.com/v1/search/news.json"

    display = max(1, min(int(page_size), 100))
    params = {
        "query": query,
        "display": display,
        "start": 1,
        "sort": "date",
    }

    headers = {
        "X-Naver-Client-Id": NAVER_CLIENT_ID,
        "X-Naver-Client-Secret": NAVER_CLIENT_SECRET,
    }

    try:
        r = requests.get(url, params=params, headers=headers, timeout=10)
        r.raise_for_status()
        data = r.json()
    except Exception as e:
        print(f"[WARN] Naver error (q={query}): {e}")
        return []

    # ë‚ ì§œ í•„í„° ì¤€ë¹„
    try:
        dt_from = dateparser.parse(from_date).date()
        dt_to = dateparser.parse(to_date).date()
    except Exception:
        dt_from = None
        dt_to = None

    items = data.get("items") or []
    articles = []

    for it in items:
        # title/descriptionì€ HTML íƒœê·¸ê°€ ë“¤ì–´ìˆìŒ
        title = unescape(it.get("title") or "")
        title = re.sub(r"<[^>]+>", " ", title)
        title = re.sub(r"\s+", " ", title).strip()

        desc = unescape(it.get("description") or "")
        desc = re.sub(r"<[^>]+>", " ", desc)
        desc = re.sub(r"\s+", " ", desc).strip()

        link = (it.get("originallink") or it.get("link") or "").strip()
        pub_date = it.get("pubDate") or ""

        # pubDate í•„í„°
        if pub_date:
            try:
                d = dateparser.parse(pub_date)
                if d is not None:
                    d_date = d.date()
                    if dt_from and d_date < dt_from:
                        continue
                    if dt_to and d_date > dt_to:
                        continue
            except Exception:
                pass

        if not link:
            continue

        articles.append({
            "source": {"name": "Naver News Search"},
            "author": None,
            "title": title,
            "description": desc,
            "content": "",
            "url": link,
            "publishedAt": pub_date,
        })


        if len(articles) >= page_size:
            break

    return articles

def search_news_gnews(query, from_date, to_date, language=None, page_size=30):
    """
    GNews API (1ìˆœìœ„ ë©”ì¸ ì†ŒìŠ¤)
    - ê³µì‹ ë¬¸ì„œ ë³´ê³  íŒŒë¼ë¯¸í„° ì´ë¦„/ì œí•œì€ í•„ìš”ì‹œ ì¡°ì •
    """
    url = "https://gnews.io/api/v4/search"
    max_size = min(page_size, 50)  # ë¬´ë£Œ í”Œëœì€ ë³´í†µ 10~50 ì œí•œ

    params = {
        "q": query,
        "token": GNEWS_API_KEY,
        "max": max_size,
        "from": from_date,
        "to": to_date,
        "sortby": "publishedAt",
    }
    if language:
        # GNewsëŠ” ì–¸ì–´ì½”ë“œê°€ en, ko ë“±ì„ ì§€ì› (ë¬¸ì„œ í™•ì¸í•´ì„œ ë§ì¶°ì•¼ í•¨)
        params["lang"] = language

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except Exception as e:
        print(f"[WARN] GNews error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    articles = []
    for item in data.get("articles", []):
        articles.append({
            "source": {"name": item.get("source", {}).get("name")},
            "author": item.get("author"),
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("content"),
            "url": item.get("url"),
            # GNewsëŠ” publishedAtì´ ISO8601 í˜•ì‹
            "publishedAt": item.get("publishedAt"),
        })
    return articles


def search_news_newsdata(query, from_date, to_date, language=None, page_size=30):
    """
    NewsData.io 'latest' ì—”ë“œí¬ì¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ë²„ì „
    - ë¬´ë£Œ í”Œëœì—ì„œë„ ë™ì‘
    - latestëŠ” ê³¼ê±° nì¼ì´ ì•„ë‹ˆë¼, ìµœê·¼ ~48ì‹œê°„ ê¸°ì¤€ì´ì§€ë§Œ,
      ìš°ë¦¬ëŠ” ì–´ì°¨í”¼ NewsAPIì—ì„œ ì¼ì£¼ì¼ì¹˜ ì»¤ë²„í•˜ê³ ,
      NewsDataëŠ” "ì¶”ê°€ ì†ŒìŠ¤" ëŠë‚Œìœ¼ë¡œë§Œ ì“°ë©´ ë¨.
    """
    url = NEWSDATA_BASE_URL_LATEST

    # free í”Œëœì—ì„œ í•œ ë²ˆì— ê°€ì ¸ì˜¬ ìˆ˜ ìˆëŠ” ìµœëŒ€ sizeëŠ” 10ê°œ
    # (ê·¸ ì´ìƒ ë„£ìœ¼ë©´ ì—ëŸ¬) :contentReference[oaicite:1]{index=1}
    size = min(page_size, 10)

    params = {
        "apikey": NEWSDATA_API_KEY,
        "q": query,
        "size": size,
    }

    if language:
        params["language"] = language  # "en", "ko" ë“±

    try:
        r = requests.get(url, params=params, timeout=10)
        r.raise_for_status()
    except requests.exceptions.HTTPError as e:
        status = getattr(e.response, "status_code", None)
        print(f"[WARN] NewsData.io HTTP error (status={status}, q={query}, lang={language}): {e}")
        return []
    except Exception as e:
        print(f"[WARN] NewsData.io error (q={query}, lang={language}): {e}")
        return []

    data = r.json()
    results = data.get("results") or []

    articles = []

    for item in results:
        creator = item.get("creator")
        if isinstance(creator, list):
            author = creator[0] if creator else None
        else:
            author = creator

        articles.append({
            # ê¸°ì¡´ NewsAPI article êµ¬ì¡°ì— ë§ì¶° ë³€í™˜
            "source": {"name": item.get("source_id")},
            "author": author,
            "title": item.get("title"),
            "description": item.get("description"),
            "content": item.get("content"),
            "url": item.get("link"),
            "publishedAt": item.get("pubDate"),  # ì˜ˆ: "2025-12-08 10:30:00"
        })

    return articles


def search_news_topheadlines_kr(page_size=50):
    """
    NewsAPI 'top-headlines' ì—”ë“œí¬ì¸íŠ¸ë¡œ
    í•œêµ­(country=kr) ê¸°ì‚¬ë§Œ ê°€ì ¸ì˜¤ëŠ” í•¨ìˆ˜
    """
    url = "https://newsapi.org/v2/top-headlines"
    params = {
        "country": "kr",          # í•œêµ­
        "pageSize": page_size,    # í•œ ë²ˆì— ê°€ì ¸ì˜¬ ê¸°ì‚¬ ìˆ˜
        "apiKey": NEWSAPI_KEY,
    }

    r = requests.get(url, params=params, timeout=10)
    r.raise_for_status()
    return r.json().get("articles", [])


EXCLUDE_KEYWORDS = [
    "tutorial", "how to", "explained", "explainer", "what is", "history of",
    "advertisement", "sponsored", "review", "buy now", "product page"
]

def is_basic_newsworthy(article):
    """
    ë…¸ê³¨ì ì¸ íŠœí† ë¦¬ì–¼/ê´‘ê³ /ìƒí’ˆ í˜ì´ì§€ ë“± 1ì°¨ í•„í„°
    """
    title = (article.get("title") or "").lower()
    description = (article.get("description") or "").lower()
    content = (article.get("content") or "").lower()
    text = " ".join([title, description, content])
    for bad in EXCLUDE_KEYWORDS:
        if bad in text:
            return False
    return True

# ============================
# âœ… Rate-limit + Backoff helpers (ì¶”ê°€)
# ============================

# APIë³„ ìµœì†Œ í˜¸ì¶œ ê°„ê²©(ì´ˆ) - ë³´ìˆ˜ì ìœ¼ë¡œ ì‹œì‘í•´ì„œ ì•ˆì •í™” í›„ ì¤„ì´ì„¸ìš”.
_API_MIN_INTERVAL_SEC = {
    "gnews": 0.35,
    "newsapi_everything": 0.35,
    "mediastack": 0.5,
    "serpapi": 0.7,
    "currents": 0.5,
    "newsdata": 0.7,
    "newsapi_top": 0.35,
    "google_rss_kr": 0.5,
    "naver_news": 0.8,
}

def _rate_limit(api_name: str):
    """í”„ë¡œì„¸ìŠ¤ ë‹¨ìœ„(=ëŸ°íƒ€ì„)ë¡œ APIë³„ ìµœì†Œ í˜¸ì¶œ ê°„ê²©ì„ ê°•ì œ."""
    state = getattr(_rate_limit, "_state", None)
    if state is None:
        state = {"last_call": {}}
        setattr(_rate_limit, "_state", state)

    now = time.monotonic()
    last = state["last_call"].get(api_name, 0.0)
    min_itv = _API_MIN_INTERVAL_SEC.get(api_name, 0.3)

    wait = (last + min_itv) - now
    if wait > 0:
        time.sleep(wait)

    state["last_call"][api_name] = time.monotonic()


def _call_with_backoff(api_name, fn, *args, **kwargs):
    """
    429/ì¼ì‹œ ì˜¤ë¥˜(5xx/íƒ€ì„ì•„ì›ƒ/too many request ë¥˜) ì‹œ ì§€ìˆ˜ ë°±ì˜¤í”„.
    ìµœì¢… ì‹¤íŒ¨ ì‹œ raise í•´ì„œ call_api_guardedê°€ 403/429ë¥¼ ê°ì§€í•˜ê³ 
    circuit breakerë¥¼ ì—´ ìˆ˜ ìˆê²Œ í•¨.
    """
    max_tries = 5
    delay = 0.8

    for attempt in range(1, max_tries + 1):
        _rate_limit(api_name)
        try:
            return fn(*args, **kwargs) or []

        except requests.exceptions.HTTPError as e:
            resp = getattr(e, "response", None)
            status = getattr(resp, "status_code", None)
            msg = str(e).lower()

            retriable = (status in (429, 500, 502, 503, 504)) or ("too many" in msg)
            if retriable and attempt < max_tries:
                time.sleep(delay)
                delay *= 2
                continue

            print(f"[WARN] {api_name} HTTP error (status={status}): {e}")
            raise

        except Exception as e:
            msg = str(e).lower()
            retriable = ("too many" in msg) or ("429" in msg) or ("timeout" in msg) or ("temporar" in msg)
            if retriable and attempt < max_tries:
                time.sleep(delay)
                delay *= 2
                continue

            print(f"[WARN] {api_name} error: {e}")
            raise


# ============================
# âœ… (NEW) Query OR bundler + Run budgets + Circuit breaker (+ Topic budgets)
# ============================

from typing import List
from collections import defaultdict

def build_or_query(keywords: List[str], max_terms: int = 18) -> str:
    """
    OR ì¿¼ë¦¬ ìƒì„±.
    - ë„ˆë¬´ ê¸´ ì¿¼ë¦¬/ê³¼ë„í•œ termì„ í”¼í•˜ê¸° ìœ„í•´ max_termsë¡œ ì»·
    - ê³µë°± í¬í•¨ í‚¤ì›Œë“œëŠ” ë”°ì˜´í‘œë¡œ ê°ì‹¸ ì •í™•ë„ ìœ ì§€
    """
    terms = []
    for kw in keywords:
        kw = str(kw).strip()
        if not kw:
            continue
        if " " in kw:
            terms.append(f'"{kw}"')
        else:
            terms.append(kw)

    terms = terms[:max_terms]
    if not terms:
        return ""
    return " OR ".join(terms)

def chunk_keywords_for_1to2_calls(keywords: List[str], max_terms_per_call: int = 18) -> List[List[str]]:
    """
    "ì–¸ì–´ë‹¹ 1~2íšŒ í˜¸ì¶œ"ì„ ê°•ì œí•˜ê¸° ìœ„í•œ í‚¤ì›Œë“œ ì²­í‚¹.
    - max_terms_per_call ê¸°ì¤€ìœ¼ë¡œ 1~2ê°œ ì²­í¬ë§Œ ë°˜í™˜
    """
    keywords = [k for k in (keywords or []) if str(k).strip()]
    if not keywords:
        return []
    first = keywords[:max_terms_per_call]
    rest = keywords[max_terms_per_call:(max_terms_per_call * 2)]
    chunks = [first]
    if rest:
        chunks.append(rest)
    return chunks  # ìµœëŒ€ 2ê°œ


# ---- ì‹¤í–‰(run) ë‹¨ìœ„ Budget / Circuit Breaker ìƒíƒœ ----
_RUN_API_BUDGETS_DEFAULT = {
    # ìš”ì²­ ì£¼ì‹  ì‹¤í–‰ë‹¹ í˜¸ì¶œ ì˜ˆì‚° ì˜ˆì‹œ
    "mediastack": 25,
    "serpapi": 60,
    "gnews": 30,
    "newsapi_everything": 60,
    "newsapi_top": 10,
    "currents": 25,
    "newsdata": 25,
    "google_rss_kr": 30,
    "naver_news": 20,
}

# ---- í† í”½(topic) ë‹¨ìœ„ Budget (í˜¸ì¶œ ìˆ˜ ìƒí•œì„ â€œí† í”½ë³„ë¡œë„â€ ëª…í™•í™”) ----
_TOPIC_API_BUDGETS_DEFAULT = {
    # OR 1~2íšŒ/ì–¸ì–´(ko/en) ê¸°ì¤€: ëŒ€ëµ 4íšŒ + ë²„í¼
    "gnews": 6,
    "newsapi_everything": 6,

    # pass2 ë³´ì¡° ì†ŒìŠ¤ëŠ” ë‚®ê²Œ
    "mediastack": 3,
    "serpapi": 3,
    "currents": 3,
    "newsdata": 3,

    # í† í”½ë‹¹ 1íšŒë§Œ ì˜ë„
    "newsapi_top": 1,
}

_CB_DEFAULT = {
    # ì—°ì† 403/429ê°€ ì´ íšŸìˆ˜ ì´ìƒì´ë©´ í•´ë‹¹ ì‹¤í–‰ ë™ì•ˆ ì˜¤í”ˆ(open)
    "threshold": 2,
}

def _get_run_state():
    state = getattr(_get_run_state, "_state", None)
    if state is None:
        state = {
            "budget_left": dict(_RUN_API_BUDGETS_DEFAULT),
            "topic_budget_left": defaultdict(lambda: dict(_TOPIC_API_BUDGETS_DEFAULT)),
            "cb": {},  # api_name -> {"open": bool, "consec_403_429": int}
        }
        setattr(_get_run_state, "_state", state)
    return state

def reset_run_state():
    """ì‹¤í–‰(run) ì‹œì‘ ì‹œ í˜¸ì¶œ(ì„ íƒ)."""
    state = _get_run_state()
    state["budget_left"] = dict(_RUN_API_BUDGETS_DEFAULT)
    state["topic_budget_left"] = defaultdict(lambda: dict(_TOPIC_API_BUDGETS_DEFAULT))
    state["cb"] = {}

def _cb_get(api_name: str) -> dict:
    st = _get_run_state()
    cb = st["cb"].get(api_name)
    if cb is None:
        cb = {"open": False, "consec_403_429": 0}
        st["cb"][api_name] = cb
    return cb

def _cb_on_result(api_name: str, status_code, success: bool):
    cb = _cb_get(api_name)
    if success:
        cb["consec_403_429"] = 0
        return
    if status_code in (403, 429):
        cb["consec_403_429"] += 1
        if cb["consec_403_429"] >= _CB_DEFAULT["threshold"]:
            cb["open"] = True

def _budget_can_call(api_name: str) -> bool:
    st = _get_run_state()
    left = st["budget_left"].get(api_name)
    if left is None:
        return True
    return left > 0

def _budget_decrement(api_name: str, n: int = 1):
    st = _get_run_state()
    if api_name not in st["budget_left"]:
        return
    st["budget_left"][api_name] = max(0, st["budget_left"][api_name] - n)

def _topic_budget_can_call(topic_id: int, api_name: str) -> bool:
    st = _get_run_state()
    tb = st["topic_budget_left"][topic_id]
    left = tb.get(api_name)
    if left is None:
        return True
    return left > 0

def _topic_budget_decrement(topic_id: int, api_name: str, n: int = 1):
    st = _get_run_state()
    tb = st["topic_budget_left"][topic_id]
    if api_name not in tb:
        return
    tb[api_name] = max(0, tb[api_name] - n)

def call_api_guarded(api_name: str, fn, *args, topic_id=None, **kwargs):
    """
    - (1) circuit breaker openì´ë©´ ìŠ¤í‚µ
    - (2) run budget 0ì´ë©´ ìŠ¤í‚µ
    - (3) topic budget 0ì´ë©´ ìŠ¤í‚µ (topic_id ì œê³µ ì‹œ)
    - (4) í˜¸ì¶œ ì‹œë„í•˜ë©´ run/topic budget ì°¨ê°
    - (5) 403/429 ì—°ì†ì´ë©´ cb open
    """
    cb = _cb_get(api_name)
    if cb.get("open"):
        return []

    if not _budget_can_call(api_name):
        return []

    if topic_id is not None and not _topic_budget_can_call(topic_id, api_name):
        return []

    # top-level call ê¸°ì¤€ìœ¼ë¡œ 1 ì°¨ê°
    _budget_decrement(api_name, 1)
    if topic_id is not None:
        _topic_budget_decrement(topic_id, api_name, 1)

    try:
        out = _call_with_backoff(api_name, fn, *args, **kwargs) or []
        _cb_on_result(api_name, status_code=None, success=True)
        return out
    except requests.exceptions.HTTPError as e:
        status = getattr(getattr(e, "response", None), "status_code", None)
        _cb_on_result(api_name, status_code=status, success=False)
        return []
    except Exception:
        return []


# ============================
# âœ… collect_articles_for_topic (êµì²´)
#   - 2-pass ìœ ì§€
#   - NewsAPI OR: ì–¸ì–´ë‹¹ 1~2íšŒ
#   - GNewsë„ OR: ì–¸ì–´ë‹¹ 1~2íšŒ (ê¸°ì¡´ í‚¤ì›Œë“œë³„ í˜¸ì¶œ ì œê±°)
#   - top-headlines: í† í”½ë‹¹ 1íšŒ + guarded + pass2ì—ì„œë§Œ ë³´ê°•
#   - run budget + topic budget + circuit breaker
# ============================

def _build_ko_query_for_local_sources(topic_keywords, fallback_query):
    """
    KO ì „ìš© ì†ŒìŠ¤(Naver/Google RSS)ìš© ì¿¼ë¦¬ ì¶•ì†Œ:
    - í•œê¸€ í‚¤ì›Œë“œë§Œ ìµœëŒ€ 5ê°œ ORë¡œ ë¬¶ì–´ ê¸¸ì´/ì—ëŸ¬ ë¦¬ìŠ¤í¬ ì¤„ì„
    - í•œê¸€ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ fallback_query ì‚¬ìš©
    """
    ko_terms = []
    for k in topic_keywords:
        s = str(k)
        if any('\uac00' <= ch <= '\ud7a3' for ch in s):
            ko_terms.append(s)

    ko_terms = ko_terms[:5]
    if ko_terms:
        # ê³µë°± í¬í•¨ í‚¤ì›Œë“œëŠ” ë”°ì˜´í‘œ ì²˜ë¦¬
        parts = [f"\"{t}\"" if " " in t else t for t in ko_terms]
        return " OR ".join(parts)

    # fallbackë„ ë„ˆë¬´ ê¸¸ë©´ ì²« í† í°ë§Œ
    fb = str(fallback_query or "").strip()
    if len(fb) > 80:
        fb = fb.split(" OR ")[0].strip()
    return fb

def _split_keywords_by_lang(topic_keywords):
    """
    í† í”½ í‚¤ì›Œë“œë¥¼ en/koë¡œ ë¶„ë¦¬ (Hangul í¬í•¨ ì—¬ë¶€ ê¸°ì¤€).

    ì¤‘ìš”:
    - í‚¤ì›Œë“œê°€ '"A" OR "B" OR "í•œê¸€"' ì²˜ëŸ¼ í•œ ì¤„ì— ì„ì—¬ ìˆëŠ” ê²½ìš°ê°€ ë§ì•„ì„œ,
      ë¬¸ìì—´ ë‚´ë¶€ ORë¥¼ ë¨¼ì € ë¶„í•´í•œ ë’¤ ì¡°ê° ë‹¨ìœ„ë¡œ ì–¸ì–´ ë¶„ë¦¬í•œë‹¤.
    - ì´ë ‡ê²Œ í•´ì•¼ en_sources(GNews ë“±)ë¡œ í•œê¸€ì´ ì„ì—¬ ë“¤ì–´ê°€ 400ì´ ë‚˜ëŠ” ê²ƒì„ ë§‰ì„ ìˆ˜ ìˆë‹¤.
    """
    def _has_hangul(s: str) -> bool:
        return any('\uac00' <= ch <= '\ud7a3' for ch in (s or ""))

    def _split_or_expr(s: str):
        """
        ë§¤ìš° ë‹¨ìˆœí•œ OR ë¶„í•´:
        - ê³µë°± í¬í•¨ OR í† í° ê¸°ì¤€ìœ¼ë¡œ ìª¼ê° ë‹¤.
        - ê´„í˜¸/ê³µë°± ì •ë¦¬ë§Œ ìˆ˜í–‰ (ì™„ì „í•œ íŒŒì„œê°€ ì•„ë‹ˆë¼ 'ìµœì†Œ ë³€ê²½' ëª©ì )
        """
        s = (s or "").strip()
        if not s:
            return []
        parts = re.split(r"\s+OR\s+", s, flags=re.IGNORECASE)
        out = []
        for p in parts:
            p = (p or "").strip()
            p = p.strip("()").strip()
            if p:
                out.append(p)
        return out if out else [s]

    en_terms, ko_terms = [], []

    for k in (topic_keywords or []):
        s = str(k or "").strip()
        if not s:
            continue

        # âœ… "A OR B OR í•œê¸€" ê°™ì€ í˜¼í•© ë¬¸ìì—´ì€ ë¨¼ì € ORë¡œ ë¶„í•´í•´ì„œ ì¡°ê° ë‹¨ìœ„ë¡œ ë¶„ë¥˜
        pieces = _split_or_expr(s)
        for piece in pieces:
            if not piece:
                continue
            if _has_hangul(piece):
                ko_terms.append(piece)
            else:
                en_terms.append(piece)

    # ìˆœì„œ ìœ ì§€ dedup
    def _uniq(seq):
        out, seen = [], set()
        for x in seq:
            key = x.lower()
            if key in seen:
                continue
            seen.add(key)
            out.append(x)
        return out

    return _uniq(en_terms), _uniq(ko_terms)



def collect_articles_for_topic(topic_id, keywords):
    collected_ko = []
    collected_en = []
    seen_urls = set()

    def _need(lang):
        if lang == "ko":
            return max(0, ARTICLES_PER_LANG_KO - len(collected_ko))
        return max(0, ARTICLES_PER_LANG_EN - len(collected_en))

    def _target_list(lang):
        return collected_ko if lang == "ko" else collected_en

    # top-headlines í•„í„°ë§ìš© í† í°(ë„ˆë¬´ ì§§ì€ í† í°ì€ ì œì™¸)
    topic_tokens = set()
    for kw in keywords:
        for tok in str(kw).lower().split():
            if len(tok) >= 2:
                topic_tokens.add(tok)

    # -----------------------------
    # âœ… í† í”½ë‹¹ top-headlines 1íšŒë§Œ í˜¸ì¶œ(í•œê¸€ ë³´ê°•ìš©)
    #    - ë°˜ë“œì‹œ guardedë¡œ í˜¸ì¶œí•´ run/topic ì˜ˆì‚° + CB ì ìš©
    # -----------------------------
    top_headlines_cache = []
    top_headlines_cache = call_api_guarded(
        "newsapi_top",
        search_news_topheadlines_kr,
        page_size=20,
        topic_id=topic_id,
    )

    # âœ… í‚¤ì›Œë“œë¥¼ en/koë¡œ ë¶„ë¦¬í•´ì„œ ì–¸ì–´ë³„ ì†ŒìŠ¤ì— ì„ì´ì§€ ì•Šê²Œ í•¨
    keywords_en, keywords_ko = _split_keywords_by_lang(keywords)

    # enì´ ë¹„ë©´(ê·¹ë‹¨ ì¼€ì´ìŠ¤) ê¸°ì¡´ keywordsë¥¼ fallback
    if not keywords_en:
        keywords_en = list(keywords or [])


    PASS1_KW_N = 2

    # -------------------------------------------------
    # âœ… ì–¸ì–´ë³„ ì†ŒìŠ¤ ì™„ì „ ë¶„ë¦¬ + ì–¸ì–´ë³„ í‚¤ì›Œë“œ ì™„ì „ ë¶„ë¦¬
    #   - en: ê¸°ì¡´ 6ê°œ ë‰´ìŠ¤ APIë§Œ ì‚¬ìš© + en í‚¤ì›Œë“œë§Œ ì‚¬ìš©
    #   - ko: Google KR RSS + Naver + topheadlines_cacheë§Œ ì‚¬ìš© + ko í‚¤ì›Œë“œë§Œ ì‚¬ìš©
    # -------------------------------------------------
    pass_plan = [
        {
            "name": "pass1",
            "kws_en": keywords_en[:PASS1_KW_N],
            "kws_ko": keywords_ko,
            "en_sources": ("gnews", "newsapi_everything"),
            "ko_sources": ("google_rss_kr", "naver_news", "topheadlines_cache"),
        },
        {
            "name": "pass2",
            "kws_en": keywords_en,
            "kws_ko": keywords_ko,
            "en_sources": (
                "gnews",
                "newsapi_everything",
                "mediastack",
                "serpapi",
                "currents",
                "newsdata",
            ),
            "ko_sources": ("google_rss_kr", "naver_news", "topheadlines_cache"),
        },
    ]

    # enì´ ë¹„ë©´(ê·¹ë‹¨ ì¼€ì´ìŠ¤) "mixed keywords ì „ì²´"ë¥¼ enìœ¼ë¡œ ë˜ëŒë ¤ ë„£ì§€ ë§ê³ ,
    # í•œê¸€ì„ ì œê±°í•œ ë¬¸ìì—´ë§Œ en í›„ë³´ë¡œ ë§Œë“ ë‹¤ (GNews 400 ë°©ì§€)
    if not keywords_en:
        tmp_en = []
        for k in (keywords or []):
            s = str(k or "").strip()
            if not s:
                continue
            # í•œê¸€ë§Œ ì œê±° â†’ ë‚¨ì€ ì˜ë¬¸/ê¸°í˜¸ ê¸°ë°˜ìœ¼ë¡œ en ì¿¼ë¦¬ í›„ë³´ ìƒì„±
            s2 = re.sub(r"[\uac00-\ud7a3]+", " ", s)
            s2 = re.sub(r"\s+", " ", s2).strip()
            if s2:
                tmp_en.append(s2)
        keywords_en = tmp_en

    # koê°€ ë¹„ë©´(ê·¹ë‹¨ ì¼€ì´ìŠ¤) RSS/NaverëŠ” ì˜ë¬¸ë„ ì–´ëŠ ì •ë„ ê²€ìƒ‰ë˜ë¯€ë¡œ ê¸°ì¡´ fallback ìœ ì§€
    if not keywords_ko:
        keywords_ko = list(keywords or [])





    for plan in pass_plan:
        if _need("ko") <= 0 and _need("en") <= 0:
            break

        for lang in LANGUAGES:
            remaining = _need(lang)

            # âœ… ì–¸ì–´ë³„ ì‹¤í–‰ ì†ŒìŠ¤ ì„ íƒ (ì™„ì „ ë¶„ë¦¬)
            active_sources = plan["ko_sources"] if lang == "ko" else plan["en_sources"]

            if remaining <= 0:
                continue

            tier_articles = []

            # -------------------------------------------------
            # 1) âœ… GNews OR ë¬¶ìŒ: ì–¸ì–´ë‹¹ 1~2íšŒ í˜¸ì¶œ
            # -------------------------------------------------
            if "gnews" in active_sources and len(tier_articles) < remaining:
                gnews_chunks = chunk_keywords_for_1to2_calls(plan["kws_en"], max_terms_per_call=18)
                for chunk in gnews_chunks:
                    if len(tier_articles) >= remaining:
                        break
                    q_or = build_or_query(chunk, max_terms=18)
                    if not q_or:
                        continue

                    rem_chunk = remaining - len(tier_articles)
                    tier_articles.extend(
                        call_api_guarded(
                            "gnews",
                            search_news_gnews,
                            q_or,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem_chunk, 50),
                            topic_id=topic_id,
                        )
                    )

            # -------------------------------------------------
            # 2) âœ… NewsAPI everything OR ë¬¶ìŒ: ì–¸ì–´ë‹¹ 1~2íšŒ í˜¸ì¶œ
            # -------------------------------------------------
            if "newsapi_everything" in active_sources and len(tier_articles) < remaining:
                newsapi_chunks = chunk_keywords_for_1to2_calls(plan["kws_en"], max_terms_per_call=18)

                for chunk in newsapi_chunks:
                    if len(tier_articles) >= remaining:
                        break

                    q_or = build_or_query(chunk, max_terms=18)
                    if not q_or:
                        continue

                    rem_chunk = remaining - len(tier_articles)
                    tier_articles.extend(
                        call_api_guarded(
                            "newsapi_everything",
                            search_news_newsapi,
                            q_or,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem_chunk, 50),
                            topic_id=topic_id,
                        )
                    )

            # -------------------------------------------------
            # âœ… (ì´ë™ë¨) í•œêµ­ ì „ìš© ë°±ì—…: Google News RSS (KR) + Naver Search API
            #    - ko ìˆ˜ëŸ‰ ë¶€ì¡± ì‹œì—ë§Œ í˜¸ì¶œ
            #    - pass1ì—ì„œë„ ì‹¤í–‰ë˜ë„ë¡ pass2 ë¸”ë¡ ë°–ìœ¼ë¡œ ì´ë™
            # -------------------------------------------------
            if lang == "ko" and len(tier_articles) < remaining:
                # (a) Google News RSS (KR)
                rem_kr = remaining - len(tier_articles)
                if "google_rss_kr" in active_sources and rem_kr > 0:
                    ko_query = _build_ko_query_for_local_sources(
                        plan.get("kws_ko") or [],
                        fallback_query=((plan.get("kws_ko")[0] if plan.get("kws_ko") else "") or (plan.get("kws_en")[0] if plan.get("kws_en") else "")),

                    )

                    tier_articles.extend(
                        call_api_guarded(
                            "google_rss_kr",
                            search_news_google_rss_kr,
                            ko_query,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem_kr, 30),
                            topic_id=topic_id,
                        )
                    )


                # (b) Naver News Search API
                rem_kr2 = remaining - len(tier_articles)
                if "naver_news" in active_sources and rem_kr2 > 0:
                    ko_query = _build_ko_query_for_local_sources(
                        plan.get("kws_ko") or [],
                        fallback_query=((plan.get("kws_ko")[0] if plan.get("kws_ko") else "") or (plan.get("kws_en")[0] if plan.get("kws_en") else "")),

                    )

                    tier_articles.extend(
                        call_api_guarded(
                            "naver_news",
                            search_news_naver,
                            ko_query,
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem_kr2, 30),
                            topic_id=topic_id,
                        )
                    )

            # -------------------------------------------------
            # 3) ë³´ì¡° ì†ŒìŠ¤ë“¤(pass2ì—ì„œë§Œ) â€” budgetìœ¼ë¡œ ìƒí•œì´ ê°•í•˜ê²Œ ê±¸ë¦¼
            # -------------------------------------------------
            if plan["name"] == "pass2" and len(tier_articles) < remaining:
                # mediastack
                rem3 = remaining - len(tier_articles)
                if "mediastack" in active_sources and rem3 > 0:
                    tier_articles.extend(
                        call_api_guarded(
                            "mediastack",
                            search_news_mediastack,
                            plan.get("kws_en")[0] if plan.get("kws_en") else "",
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem3, 30),
                            topic_id=topic_id,
                        )
                    )

                # serpapi
                rem4 = remaining - len(tier_articles)
                if "serpapi" in active_sources and rem4 > 0:
                    tier_articles.extend(
                        call_api_guarded(
                            "serpapi",
                            search_news_serpapi,
                            plan.get("kws_en")[0] if plan.get("kws_en") else "",
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem4, 10),
                            topic_id=topic_id,
                        )
                    )

                # currents
                rem5 = remaining - len(tier_articles)
                if "currents" in active_sources and rem5 > 0:
                    tier_articles.extend(
                        call_api_guarded(
                            "currents",
                            search_news_currents,
                            plan.get("kws_en")[0] if plan.get("kws_en") else "",
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem5, 50),
                            topic_id=topic_id,
                        )
                    )

                # newsdata
                rem6 = remaining - len(tier_articles)
                if "newsdata" in active_sources and rem6 > 0:
                    tier_articles.extend(
                        call_api_guarded(
                            "newsdata",
                            search_news_newsdata,
                            plan.get("kws_en")[0] if plan.get("kws_en") else "",
                            DATE_FROM,
                            DATE_TO,
                            language=lang,
                            page_size=min(rem6, 10),
                            topic_id=topic_id,
                        )
                    )

            # -------------------------------------------------
            # 4) âœ… top-headlines ìºì‹œ ë³´ê°•: pass2ì—ì„œë§Œ
            # -------------------------------------------------
            if (
                plan["name"] == "pass2"
                and len(tier_articles) < remaining
                and lang == "ko"
                and "topheadlines_cache" in active_sources
                and top_headlines_cache
            ):
                for art in top_headlines_cache:
                    title_text = str(art.get("title", "")).lower()
                    if not title_text:
                        continue
                    if any(tok in title_text for tok in topic_tokens):
                        tier_articles.append(art)

            # -----------------------------
            # ê³µí†µ í›„ì²˜ë¦¬: URL ì¤‘ë³µ ì œê±° + ë‚ ì§œ í•„í„° + ê¸°ë³¸ í•„í„°
            # -----------------------------
            for a in tier_articles:
                if lang == "ko" and len(collected_ko) >= ARTICLES_PER_LANG_KO:
                    break
                if lang == "en" and len(collected_en) >= ARTICLES_PER_LANG_EN:
                    break

                url = a.get("url")
                if not url or url in seen_urls:
                    continue

                published_at_raw = a.get("publishedAt")

                if not published_at_raw:
                    published_dt = datetime.fromisoformat(DATE_TO).date()
                else:
                    try:
                        parsed = dateparser.parse(published_at_raw)
                        if parsed is None:
                            raise ValueError("parsed is None")
                        published_dt = parsed.date()
                    except Exception:
                        published_dt = datetime.fromisoformat(DATE_TO).date()

                from_dt = datetime.fromisoformat(DATE_FROM).date()
                to_dt = datetime.fromisoformat(DATE_TO).date()
                if not (from_dt <= published_dt <= to_dt):
                    continue

                if not is_basic_newsworthy(a):
                    continue

                seen_urls.add(url)
                article_data = {
                    "topic_seed": topic_id,
                    "source_name": a.get("source", {}).get("name"),
                    "author": a.get("author"),
                    "original_title": a.get("title"),
                    "description": a.get("description"),
                    "content": a.get("content"),
                    "url": url,
                    "published_at": str(published_dt),
                }
                _target_list(lang).append(article_data)

    collected = collected_ko + collected_en
    print(f"  â”” ì£¼ì œ {topic_id}: í•œê¸€ {len(collected_ko)}ê°œ, ì˜ì–´ {len(collected_en)}ê°œ ìˆ˜ì§‘ë¨")
    return collected





reset_run_state()
print("=== [1ë‹¨ê³„] NewsAPIì—ì„œ ê¸°ì‚¬ ìˆ˜ì§‘ ì¤‘ ===")

raw_articles = []

for t_id, kws in TOPIC_KEYWORDS.items():
    arts = collect_articles_for_topic(t_id, kws)
    print(f"ì£¼ì œ {t_id} ({TOPIC_DESC[t_id]}) : {len(arts)}ê°œ ê¸°ì‚¬ í›„ë³´ ìˆ˜ì§‘")
    raw_articles.extend(arts)


df_raw = pd.DataFrame(raw_articles)
print("\n[1ì°¨ ìˆ˜ì§‘ ê²°ê³¼ ê°œìˆ˜] :", len(df_raw))
if IN_COLAB:
    display(df_raw.head())


# # **03-1 ì–¸ì–´ë³„ ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜**

# In[ ]:


# ============================
# 3-1. ì–¸ì–´ë³„ ë¹„ìœ¨ ê³„ì‚° í•¨ìˆ˜ (ìƒˆë¡œ ì¶”ê°€)
# ============================

def calculate_language_ratio(articles):
    """ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ì—ì„œ í•œê¸€/ì˜ë¬¸ ë¹„ìœ¨ ê³„ì‚°"""
    total = len(articles)
    if total == 0:
        return 0, 0, 0, 0

    korean_count = sum(1 for art in articles if is_korean_article(art))
    english_count = total - korean_count

    korean_pct = (korean_count / total) * 100
    english_pct = (english_count / total) * 100

    return korean_count, english_count, korean_pct, english_pct


def is_korean_article(article_dict):
    """ê¸°ì‚¬ê°€ í•œê¸€ì¸ì§€ íŒë‹¨ (ì›ë¬¸ ê¸°ë°˜)

    title_ko / summary_ko ëŠ” GPTê°€ ìƒì„±í•œ í•œê¸€ ìš”ì•½ì´ë¯€ë¡œ
    ì–¸ì–´ íŒë³„ì— ì‚¬ìš©í•˜ë©´ ëª¨ë“  ê¸°ì‚¬ê°€ í•œê¸€ë¡œ ì˜¤íŒì •ë¨.
    ë°˜ë“œì‹œ 'ì›ë¬¸ í…ìŠ¤íŠ¸'ë§Œ ê¸°ì¤€ìœ¼ë¡œ íŒë³„í•œë‹¤.

    ë˜í•œ, ìµœì¢… dict ìŠ¤í‚¤ë§ˆ(df_row_to_article)ê°€ ì‚¬ìš©í•˜ëŠ”
    orig_title í‚¤ë„ ì§€ì›í•œë‹¤.
    """
    # âœ… (ìµœì†Œ ì•ˆì „ì¥ì¹˜) pandas Series ë“¤ì–´ì™€ë„ dictë¡œ ë³€í™˜
    if hasattr(article_dict, "to_dict") and not isinstance(article_dict, dict):
        article_dict = article_dict.to_dict()

    if not isinstance(article_dict, dict):
        return False

    # âœ… ì›ë¬¸ title í›„ë³´ í‚¤ë“¤ (ìš°ì„ ìˆœìœ„ ìˆœ)
    title = (
        article_dict.get("original_title")
        or article_dict.get("orig_title")
        or article_dict.get("title")
        or ""
    )

    # âœ… ì›ë¬¸ ë³´ì¡° í…ìŠ¤íŠ¸ í›„ë³´ í‚¤ë“¤
    description = article_dict.get("description") or ""
    content = article_dict.get("content") or ""

    text = " ".join([str(title), str(description), str(content)])
    return any('\uac00' <= ch <= '\ud7a3' for ch in text)


# # **04 GPT (ì—„ê²© í•„í„°ë§/ë¶„ë¥˜/ìš”ì•½)**

# In[ ]:


# ============================
# 4. GPT minië¡œ ìš”ì•½/ë¶„ë¥˜/í•„í„°ë§
# ============================

SYSTEM_PROMPT_STRICT = """
ë‹¹ì‹ ì€ B2B í…Œí¬ ë‰´ìŠ¤ë ˆí„° í¸ì§‘ìì…ë‹ˆë‹¤.
ì•„ë˜ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©° ê¸°ì‚¬ë¥¼ ì„ ë³„í•˜ê³  í•œêµ­ì–´ë¡œ ì •ë¦¬í•˜ì„¸ìš”.

[ì£¼ì œ ë²”ìœ„]
1) GeoINT / ì§€ë¦¬ê³µê°„ ì •ë³´ / ìœ„ì„±ì •ë³´ / ì§€ë„ë°ì´í„° ë¶„ì„
2) ë“œë¡  / UAV / UAM ì‚°ì—… ë° ê¸°ìˆ 
3) AI ë°ì´í„°Â·ë¶„ì„ í”Œë«í¼ / MLOps / ìë™í™”Â·ì—”í„°í”„ë¼ì´ì¦ˆ AI ì¸í”„ë¼
   ë¿ë§Œ ì•„ë‹ˆë¼, ë‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ë„ ëª¨ë‘ 3ë²ˆìœ¼ë¡œ í¬í•¨í•©ë‹ˆë‹¤.
   - ì—”í„°í”„ë¼ì´ì¦ˆ/ì‚°ì—… ë¶„ì•¼ì˜ AI ë„ì…Â·ì „ëµÂ·ê±°ë²„ë„ŒìŠ¤
   - ìƒì„±í˜• AI(GenAI) ë° LLM í™œìš© ì‚¬ë¡€, AI ê¸°ë°˜ ì—…ë¬´ ìë™í™”
   - AI í´ë¼ìš°ë“œ ì¸í”„ë¼, GPU/ê°€ì†ê¸°, ëª¨ë¸ í—ˆë¸Œ/ë§ˆì¼“í”Œë ˆì´ìŠ¤
   - ë°ì´í„°Â·ë¶„ì„ ì œí’ˆì— AI ê¸°ëŠ¥ì´ í¬ê²Œ ì¶”ê°€ë˜ëŠ” ê²½ìš°
   - ê¸°ì—…ìš© í˜‘ì—…íˆ´Â·ì—…ë¬´íˆ´ì—ì„œ AI ì—ì´ì „íŠ¸/ì½”íŒŒì¼ëŸ¿ì´ í•µì‹¬ì¸ ì—…ë°ì´íŠ¸

   ë‹¨, ìˆœìˆ˜ ì†Œë¹„ììš© ì„œë¹„ìŠ¤(ê²Œì„ ì¶”ì²œ, SNS í•„í„° ë“±)ë§Œ ë‹¤ë£¨ëŠ” ê¸°ì‚¬ëŠ” ë˜ë„ë¡ ì œì™¸í•©ë‹ˆë‹¤.

4) ìœ„ì„± ì˜ìƒ(SARÂ·ê´‘í•™Â·í•˜ì´í¼ìŠ¤í™íŠ¸ëŸ´) ë¶„ì„

[íŠ¹ë³„ ê·œì¹™: í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬]
- ê¸°ì‚¬ ì œëª©Â·ë³¸ë¬¸Â·ì„¤ëª…ì— 'í•œì»´', 'í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤', 'Hancom InSpace', 'InSpace' ë“±ì´ í¬í•¨ë˜ë©´
  -> ë°˜ë“œì‹œ keep = true ë¡œ ì„¤ì •í•˜ì„¸ìš”.
- ì£¼ì œ ë²”ìœ„ë¥¼ ì•½ê°„ ë²—ì–´ë‚˜ë„ ê´œì°®ìŠµë‹ˆë‹¤.
- topic_finalì€ 1~4 ì¤‘ ê°€ì¥ ê´€ë ¨ì„±ì´ ë†’ì€ ë²ˆí˜¸ë¡œ ì„ íƒí•˜ì„¸ìš”.
- reason í•„ë“œì— ë°˜ë“œì‹œ "[í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨]"ì„ í¬í•¨í•˜ì„¸ìš”.

[ë¶ˆí—ˆ ì»¨í…ì¸ ]
- ë‹¨ìˆœ ë¸”ë¡œê·¸ ê¸€, ì œí’ˆ ê´‘ê³ , ê¸°ì´ˆ ì„¤ëª…, íŠœí† ë¦¬ì–¼ì€ keep=false

[ì¶œë ¥ JSON í˜•ì‹]
{
  "keep": true ë˜ëŠ” false,
  "topic_final": 1~4,
  "title_ko": "...",
  "summary_ko": "...",
  "reason": "..."
}
"""

SYSTEM_PROMPT_LOOSE = SYSTEM_PROMPT_STRICT + """
[ì¶”ê°€ ì™„í™” ê·œì¹™]
- keep ì—¬ë¶€ë¥¼ íŒë‹¨í•  ë•Œ, ì£¼ì œì™€ì˜ ê´€ë ¨ì„±ì´ ì•½ê°„ ì• ë§¤í•˜ë”ë¼ë„ ê°€ëŠ¥í•œ í•œ keep=true ìª½ìœ¼ë¡œ íŒë‹¨í•˜ì„¸ìš”.
- íŠ¹íˆ ì •ë¶€ ì •ì±…, ê³µê³µê¸°ê´€, ëŒ€ê¸°ì—…ì˜ AIÂ·ìœ„ì„±Â·ë“œë¡ Â·ë°ì´í„° ê´€ë ¨ ë°œí‘œÂ·MOUÂ·ì‚¬ì—…ê³„íš ë“±ì€ í­ë„“ê²Œ í¬í•¨í•©ë‹ˆë‹¤.
- 'íŠœí† ë¦¬ì–¼/ìˆœìˆ˜ ê´‘ê³ /ì™„ì „ ë‹¤ë¥¸ ë¶„ì•¼(ì—°ì˜ˆÂ·ìŠ¤í¬ì¸  ë“±)'ê°€ ì•„ë‹Œ ì´ìƒ, ì—”í„°í”„ë¼ì´ì¦ˆ/ì‚°ì—…Â·ê³µê³µê³¼ ì¡°ê¸ˆì´ë¼ë„ ì ‘ì ì´ ë³´ì´ë©´ keep=trueë¡œ ë‘ì„¸ìš”.
"""


def build_user_prompt(row):
    topic_hint = TOPIC_DESC.get(row["topic_seed"], "")
    text = f"""
[seed topic ì •ë³´]
- id: {row['topic_seed']}
- desc: {topic_hint}

[ê¸°ì‚¬ ë©”íƒ€]
- source: {row.get('source_name')}
- url: {row.get('url')}
- published_at: {row.get('published_at')}

[ì œëª©]
{row.get('original_title')}

[description]
{row.get('description')}

[ë³¸ë¬¸ content]
{row.get('content')}
"""
    return text


# ì „ì—­ ë³€ìˆ˜ë¡œ í† í° ëˆ„ì 
if 'total_tokens_used' not in globals():
    total_tokens_used = {"input": 0, "output": 0}


def gpt_process_article(row, system_prompt=SYSTEM_PROMPT_STRICT, model=MODEL_NAME):
    user_prompt = build_user_prompt(row)
    response = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": system_prompt},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3  # ğŸ”¥ 0ì—ì„œ 0.3ìœ¼ë¡œ ë³€ê²½
    )

        # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
    if hasattr(response, 'usage'):
        total_tokens_used["input"] += response.usage.input_tokens
        total_tokens_used["output"] += response.usage.output_tokens

    text_out = response.output[0].content[0].text
    return json.loads(text_out)



print("\n=== [4ë‹¨ê³„] GPT ì—„ê²© í•„í„°ë§ + í•œì»´ ê°•ì œ í¬í•¨ ë¡œì§ ===")


# ============================================================
# âœ… (NEW) GPT ê¸°ë°˜ "ê°™ì€ ì‚¬ê±´/ê°™ì€ ë‚´ìš©" ì¤‘ë³µ íŒë³„
# - ì œëª©ì´ ë‹¬ë¼ë„ ìš”ì•½/ë‚´ìš©ì´ ê°™ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ê°„ì£¼
# - ë¹„ìš©/ì†ë„ ë•Œë¬¸ì— 1ì°¨(ë¬¸ì ìœ ì‚¬ë„) â†’ 2ì°¨(GPT) 2ë‹¨ê³„ë¡œ ì‚¬ìš© ê¶Œì¥
# ============================================================

SEMANTIC_DEDUP_ENABLE = True

# ì œëª© ìœ ì‚¬ë„ê°€ ì´ ê°’ ì´ìƒì´ë©´ GPT ì—†ì´ë„ ì¤‘ë³µ ì²˜ë¦¬(ë¹ ë¥´ê³  ì €ë ´)
FAST_TITLE_DUP_THRESHOLD = 0.88

# ì œëª© ìœ ì‚¬ë„ê°€ ì´ êµ¬ê°„ì´ë©´(ì• ë§¤) GPTì—ê²Œ ì˜ë¯¸ì¤‘ë³µ ì—¬ë¶€ë¥¼ ë¬¼ì–´ë´„
SEMANTIC_CHECK_MIN = 0.45
SEMANTIC_CHECK_MAX = 0.88

def gpt_is_same_story(a_title: str, a_summary: str, b_title: str, b_summary: str, model=MODEL_NAME) -> bool:
    """
    GPTê°€ ë‘ ê¸°ì‚¬ê°€ ê°™ì€ ì‚¬ê±´/ë‚´ìš©ì¸ì§€(ì˜ë¯¸ ì¤‘ë³µ) íŒë‹¨.
    ë°˜í™˜: True(ì¤‘ë³µ) / False(ì¤‘ë³µ ì•„ë‹˜)
    """
    # ì…ë ¥ì´ ë„ˆë¬´ ë¹„ë©´ íŒë‹¨ ë¶ˆê°€ â†’ ì¤‘ë³µ ì•„ë‹˜ ì²˜ë¦¬
    if not (a_title or a_summary) or not (b_title or b_summary):
        return False

    system = (
        "You are a strict news deduplication engine.\n"
        "Decide whether two news items refer to the same underlying event/story.\n"
        "Return ONLY valid JSON."
    )

    user = f"""
[Article A]
title: {a_title}
summary: {a_summary}

[Article B]
title: {b_title}
summary: {b_summary}

Rules:
- Consider them duplicates if they describe the same event/announcement/deal/report, even if wording differs.
- Consider them NOT duplicates if they are different events, different companies, different timeframes, or one is analysis/opinion while the other is a different news event.
Output JSON schema:
{{"duplicate": true/false, "confidence": 0-100}}
"""

    try:
        resp = client.responses.create(
            model=model,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.0,
        )
        out = resp.output[0].content[0].text
        data = json.loads(out)
        dup = bool(data.get("duplicate", False))
        conf = int(data.get("confidence", 0) or 0)

        # ë³´ìˆ˜ì ìœ¼ë¡œ: confidenceê°€ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¤‘ë³µìœ¼ë¡œ ì•ˆ ë´„
        if dup and conf >= 65:
            return True
        return False

    except Exception as e:
        print(f"[WARN] gpt_is_same_story ì‹¤íŒ¨: {e}")
        return False


def is_korean_article_row(row) -> bool:
    """
    df_rawì˜ í•œ í–‰ì´ í•œêµ­ì–´ ê¸°ì‚¬ì¸ì§€ ê°„ë‹¨íˆ íŒë³„:
    ì œëª©/description/content ì¤‘ í•˜ë‚˜ì— í•œê¸€ì´ ìˆìœ¼ë©´ True
    """
    text = " ".join([
        str(row.get("original_title") or ""),
        str(row.get("description") or ""),
        str(row.get("content") or ""),
    ])
    return any('\uac00' <= ch <= '\ud7a3' for ch in text)


strict_rows = []
hancom_keywords = [
    "í•œì»´", "í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤", "hancom", "hancom inspace", "inspace", "ì¸ìŠ¤í˜ì´ìŠ¤"
]

# ë³‘ë ¬ ì²˜ë¦¬ìš© í•¨ìˆ˜ ì •ì˜
def process_single_article(idx_row):
    """
    ë‹¨ì¼ ê¸°ì‚¬ë¥¼ GPTë¡œ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    idx, row = idx_row
    url = row.get("url")
    if not url:
        return None

    # 1) GPT ì²˜ë¦¬
    try:
        result = gpt_process_article(row, system_prompt=SYSTEM_PROMPT_STRICT)
    except Exception as e:
        print(f"[ERROR] GPT ì²˜ë¦¬ ì‹¤íŒ¨ (url={url}): {e}")
        return None

    # 2) í•œì»´ í‚¤ì›Œë“œ ì²´í¬
    title_lower = str(row.get("original_title") or "").lower()
    desc_lower = str(row.get("description") or "").lower()

    is_hancom = any(
        kw.lower() in title_lower or kw.lower() in desc_lower
        for kw in hancom_keywords
    )

    keep = result.get("keep", False)
    if is_hancom:
        keep = True
        result["keep"] = True
        result["reason"] = "[í•œì»´ ê´€ë ¨] " + result.get("reason", "")

    # 3) keep=Falseì´ë©´ None ë°˜í™˜
    if not keep:
        return None

    # GPT ê²°ê³¼ì—ì„œ topic_final ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ seed fallback
    topic_seed = row.get("topic_seed")

    topic_final = result.get("topic_final", topic_seed)
    try:
        topic_final = int(topic_final)
    except Exception:
        topic_final = topic_seed

    return {
        "topic_seed": topic_seed,
        "topic_final": topic_final,   # âœ… GPT ë¶„ë¥˜ ë°˜ì˜
        "korean_summary": result.get("summary_ko", ""),
        "english_summary": "",
        "title_ko": result.get("title_ko", ""),
        "summary_ko": result.get("summary_ko", ""),
        "title_en": "",
        "summary_en": "",
        "original_title": row.get("original_title"),
        "description": row.get("description"),
        "url": url,
        "published_at": row.get("published_at"),
        "source_name": row.get("source_name"),
        "reason": result.get("reason"),
    }


# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
print(f"\nì´ {len(df_raw)}ê°œ ê¸°ì‚¬ë¥¼ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë™ì‹œ ì²˜ë¦¬: 20ê°œ)...")
with ThreadPoolExecutor(max_workers=20) as executor:
    # ëª¨ë“  ê¸°ì‚¬ë¥¼ ë³‘ë ¬ë¡œ ì²˜ë¦¬í•˜ë„ë¡ ì œì¶œ
    futures = {executor.submit(process_single_article, (idx, row)): idx
               for idx, row in df_raw.iterrows()}

    # ì™„ë£Œë˜ëŠ” ëŒ€ë¡œ ê²°ê³¼ ìˆ˜ì§‘
    completed = 0
    for future in as_completed(futures):
        result = future.result()
        if result:
            strict_rows.append(result)

        completed += 1
        # ì§„í–‰ ìƒí™© ì¶œë ¥ (20ê°œë§ˆë‹¤)
        if completed % 20 == 0:  # â† ì—¬ê¸°! for ë£¨í”„ ì•ˆì— ìˆì–´ìš”
            print(f"  ì§„í–‰: {completed}/{len(df_raw)} ({completed*100//len(df_raw)}%) - ì„ ë³„ëœ ê¸°ì‚¬: {len(strict_rows)}ê°œ")

print(f"\në³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(strict_rows)}ê°œ ê¸°ì‚¬ ì„ ë³„ë¨")


df_strict = pd.DataFrame(strict_rows)
print("\n[GTP í•„í„°ë§ í›„ strict_rows ê°œìˆ˜]:", len(df_strict))
if IN_COLAB:
    display(df_strict.head())

# ---------- URL ê¸°ì¤€ ì¤‘ë³µ ì •ë¦¬ ----------
dedup_rows = []

if not df_strict.empty:
    for url, group in df_strict.groupby("url"):
        # topic_final ë¹ˆë„ìˆ˜ ê¸°ë°˜ ì„ íƒ
        topic_counts = group["topic_final"].value_counts()
        best_topic = int(topic_counts.index[0])

        # ìµœì‹  ê¸°ì‚¬ ì„ íƒ
        best_row = group[group["topic_final"] == best_topic] \
            .sort_values("published_at", ascending=False) \
            .iloc[0]

        dedup_rows.append(best_row.to_dict())

df_final = pd.DataFrame(dedup_rows).reset_index(drop=True)

print("\n[URL ì¤‘ë³µ ì œê±° í›„ df_final ê°œìˆ˜]:", len(df_final))
if IN_COLAB:
    display(df_final)


# # **05 ë¶€ì¡±í•œ í† í”½ì€ ë°±ì—… í”„ë¡¬í”„íŠ¸ë¡œ ì±„ìš°ê¸° + í† í”½ë‹¹ 3ê°œ ë§ì¶”ê¸°**

# In[ ]:


# ============================
# 5. ì£¼ì œë³„ ìµœì†Œ 3ê°œ ê°•ì œ: ë¶€ì¡±ë¶„ì€ ë°±ì—… í”„ë¡¬í”„íŠ¸ë¡œ ì±„ìš°ê¸°
#    + í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
# ============================

def detect_hancom_priority(row):
    """
    í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬ì´ë©´ priority=1, ì•„ë‹ˆë©´ 0
    - ì œëª© / í•œê¸€ì œëª© / í•œê¸€ìš”ì•½ / ì†ŒìŠ¤ëª…ì— 'í•œì»´', 'InSpace' ë“±ì´ í¬í•¨ë˜ë©´ ìš°ì„ ìˆœìœ„ ë¶€ì—¬
    """
    text = " ".join([
        str(row.get("original_title", "") or ""),
        str(row.get("title_ko", "") or ""),
        str(row.get("summary_ko", "") or ""),
        str(row.get("source_name", "") or "")
    ]).lower()

    keywords = [
        "í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤",
        "í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤",
        "hancominspace",
        "hancom inspace",
        "hancom",
        "inspace",
    ]

    return 1 if any(k in text for k in keywords) else 0


def fill_topic_with_backup(topic_id, needed, df_final, df_raw):
    """
    topic_idì— ëŒ€í•´ df_finalì— ë¶€ì¡±í•œ ê°œìˆ˜(needed)ë§Œí¼
    df_rawì—ì„œ ì•„ì§ ì•ˆ ì“´ URLì„ ê³¨ë¼
    'ëŠìŠ¨í•œ' í”„ë¡¬í”„íŠ¸ë¡œ ìš”ì•½í•´ì„œ df_finalì— ì¶”ê°€.
    """
    used_urls = set(df_final["url"].tolist())
    cand = df_raw[(df_raw["topic_seed"] == topic_id) & (~df_raw["url"].isin(used_urls))]
    cand = cand.sort_values("published_at", ascending=False)

    added_rows = []
    for _, row in cand.iterrows():
        if needed <= 0:
            break
        try:
            result = gpt_process_article(row, system_prompt=SYSTEM_PROMPT_LOOSE)
        except Exception as e:
            print(f"[ë°±ì—… ì—ëŸ¬] topic {topic_id}, url={row.get('url')}: {e}")
            continue

        # ì–´ì°¨í”¼ seed topicìš©ì´ë¯€ë¡œ ê°•ì œë¡œ topic_final/keep ë³´ì •
        result_topic = topic_id
        title_ko = result.get("title_ko")
        summary_ko = result.get("summary_ko")
        reason = result.get("reason", "")

        if not title_ko or not summary_ko:
            # ë„ˆë¬´ ë¶€ì‹¤í•˜ë©´ ìŠ¤í‚µ
            continue

        added_rows.append({
            "topic_final": result_topic,
            "keep": True,
            "title_ko": title_ko,
            "summary_ko": summary_ko,
            "original_title": row.get("original_title"),
            "url": row.get("url"),
            "published_at": row.get("published_at"),
            "source_name": row.get("source_name"),
            "reason": f"[ë°±ì—…ê¸°ì‚¬] {reason}",
        })
        needed -= 1
        time.sleep(0.05)

    if added_rows:
        df_add = pd.DataFrame(added_rows)
        df_final = pd.concat([df_final, df_add], ignore_index=True)

    return df_final


# í˜„ì¬ í† í”½ë³„ ê°œìˆ˜ í™•ì¸ í›„ ë¶€ì¡±í•˜ë©´ ì±„ìš°ê¸°
for t in [1, 2, 3, 4]:
    cur = df_final[df_final["topic_final"] == t]
    cnt = len(cur)

    # [ìˆ˜ì •] ì˜ˆì „ì—ëŠ” "3ê°œ ì´ìƒì´ë©´ í†µê³¼"ì˜€ëŠ”ë°,
    # ì´ì œëŠ” "ìµœì†Œ 9ê°œ(MIN_TOTAL_PER_TOPIC) ì´ìƒ"ì´ì–´ì•¼ í†µê³¼
    if cnt >= MIN_TOTAL_PER_TOPIC:
        continue

    needed = MIN_TOTAL_PER_TOPIC - cnt
    print(
        f"í† í”½ {t} ë³´ê°• í•„ìš”: {cnt}ê°œ â†’ {MIN_TOTAL_PER_TOPIC}ê°œë¡œ, "
        f"{needed}ê°œ ë°±ì—… ìˆ˜ì§‘ ì‹œë„"
    )
    df_final = fill_topic_with_backup(t, needed, df_final, df_raw)

# ê·¸ë˜ë„ í˜¹ì‹œ ë¶€ì¡±í•˜ë©´(NewsAPIì— ê¸°ì‚¬ ìì²´ê°€ ì—†ëŠ” ê²½ìš°) ê²½ê³ ë§Œ ì°ê³  ë„˜ì–´ê°
for t in [1, 2, 3, 4]:
    cnt = len(df_final[df_final["topic_final"] == t])
    if cnt < ARTICLES_PER_TOPIC_FINAL:
        print(f"[ê²½ê³ ] í† í”½ {t}ëŠ” NewsAPIì— ê¸°ì‚¬ê°€ ë¶€ì¡±í•´ì„œ {cnt}ê°œë§Œ í‘œì‹œë©ë‹ˆë‹¤.")

# ---------- í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ì—¬ë¶€ í”Œë˜ê·¸ ----------
df_final["hancom_priority"] = df_final.apply(detect_hancom_priority, axis=1)
print("í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤ ê´€ë ¨ ê¸°ì‚¬ ìˆ˜:", int(df_final["hancom_priority"].sum()))

# ì—¬ê¸°ì„œëŠ” ì •ë ¬í•˜ì§€ ì•Šê³ , ë’¤ì—ì„œ 'ì‚¬íšŒì  ê´€ì‹¬ ê¸°ë°˜ priority'ë¥¼ ê³„ì‚°í•œ ë‹¤ìŒì— ì •ë ¬í•©ë‹ˆë‹¤.


print("\n[ë°±ì—… í¬í•¨ ìµœì¢… ê¸°ì‚¬ ê°œìˆ˜] :", len(df_final))
if IN_COLAB:
    display(df_final)

df_final.to_csv("newsletter_articles.csv", index=False)
print("CSV ì €ì¥ ì™„ë£Œ: newsletter_articles.csv")


# # **06 ë©”ì¸(3ê°œ) + ë”ë³´ê¸° ê¸°ì‚¬ ë¶„ë¦¬**

# In[ ]:


# ============================
# 6. ë©”ì¸(3ê°œ) + ë”ë³´ê¸°(ìµœëŒ€ 10ê°œ) ê¸°ì‚¬ ë¶„ë¦¬
# ============================

from collections import defaultdict
import difflib
import re

# ---------- ì‚¬íšŒì  ê´€ì‹¬ ê¸°ë°˜ ì¤‘ìš”ë„(priority) ê³„ì‚° ----------

# 1) ë§¤ì²´ ê°€ì¤‘ì¹˜: ê¸°ë³¸ê°’ 1.0, ì£¼ìš” ë§¤ì²´ëŠ” ë” ë†’ê²Œ
SOURCE_WEIGHTS = {
    "ì—°í•©ë‰´ìŠ¤": 2.0,
    "ì¡°ì„ ë¹„ì¦ˆ": 2.0,
    "í•œêµ­ê²½ì œ": 2.0,
    "ë§¤ì¼ê²½ì œ": 2.0,
    "ì „ìì‹ ë¬¸": 1.5,
    "ì„œìš¸ê²½ì œ": 1.5,
    # í•„ìš”í•˜ë©´ ì—¬ê¸° ê³„ì† ì¶”ê°€
}

def _get_source_weight(source_name):
    name = str(source_name or "").strip()
    return SOURCE_WEIGHTS.get(name, 1.0)  # ê¸°ë³¸ê°’ 1.0

# 2) í›„ì†/ë¶„ì„ ê¸°ì‚¬ ì—¬ë¶€ (followup_score)
FOLLOWUP_KEYWORDS = [
    "ë¶„ì„", "ì „ë§", "ì˜í–¥", "ì˜ë¯¸",
    "íŒŒê¸‰", "í•´ì„¤", "ì™œ", "ì´ìœ "
]

def _get_followup_score(title_ko, original_title):
    text = f"{title_ko or ''} {original_title or ''}"
    return 1 if any(k in text for k in FOLLOWUP_KEYWORDS) else 0

# 3) í´ëŸ¬ìŠ¤í„°(ë¹„ìŠ·í•œ ê¸°ì‚¬ ìˆ˜) + í™•ì‚° ì†ë„(velocity) ê³„ì‚°
#    - ì „ì²´ df_final ê¸°ì¤€ìœ¼ë¡œ ê³„ì‚°
import pandas as pd  # ì´ë¯¸ ìœ„ì— ìˆì„ ê°€ëŠ¥ì„±ì´ í¬ì§€ë§Œ, í•œ ë²ˆ ë” ì¨ë„ ë¬¸ì œ ì—†ìŒ

# published_atì„ datetimeìœ¼ë¡œ ë³€í™˜ (ì†ë„ ê³„ì‚°ìš©)
df_final["published_at_dt"] = pd.to_datetime(df_final["published_at"], errors="coerce")

# ë¹„êµì— ì‚¬ìš©í•  í…ìŠ¤íŠ¸ (ì œëª© + í•œê¸€ì œëª© + ìš”ì•½)
texts = (
    df_final["original_title"].fillna("") + " || " +
    df_final["title_ko"].fillna("") + " || " +
    df_final["summary_ko"].fillna("")
).tolist()

times = df_final["published_at_dt"].tolist()
n = len(df_final)

# ië²ˆì§¸ ê¸°ì‚¬ì™€ ë¹„ìŠ·í•œ ê¸°ì‚¬ ì¸ë±ìŠ¤ ëª¨ìŒ
similar_indices = [set() for _ in range(n)]

for i in range(n):
    for j in range(i + 1, n):
        a = texts[i].lower().strip()
        b = texts[j].lower().strip()
        if not a or not b:
            continue
        sim = difflib.SequenceMatcher(None, a, b).ratio()
        # ìœ ì‚¬ë„ thresholdëŠ” 0.75 ì •ë„ë¡œ ì„¤ì •
        if sim >= 0.75:
            similar_indices[i].add(j)
            similar_indices[j].add(i)

cluster_scores = []
velocity_scores = []

for i in range(n):
    cnt = len(similar_indices[i]) + 1

    # (1) cluster_score: ë¹„ìŠ·í•œ ê¸°ì‚¬ ê°œìˆ˜ì— ë”°ë¼ ì ìˆ˜ ë¶€ì—¬
    if cnt <= 1:
        cluster = 0
    elif cnt <= 3:
        cluster = 1
    elif cnt <= 6:
        cluster = 2
    else:
        cluster = 3
    cluster_scores.append(cluster)

    # (2) velocity_score: ì§§ì€ ì‹œê°„ ë™ì•ˆ ì–¼ë§ˆë‚˜ ë§ì´ ë‚˜ì™”ëŠ”ì§€
    if cnt <= 1:
        velocity = 0
    else:
        idxs = [i] + list(similar_indices[i])
        ts = [times[k] for k in idxs if pd.notnull(times[k])]
        if len(ts) >= 2:
            span_hours = (max(ts) - min(ts)).total_seconds() / 3600.0
            if span_hours <= 48:
                velocity = 2
            elif span_hours <= 168:
                velocity = 1
            else:
                velocity = 0
        else:
            velocity = 0
    velocity_scores.append(velocity)

# 4) df_finalì— ì»¬ëŸ¼ìœ¼ë¡œ ë¶™ì´ê¸°
df_final["cluster_score"]   = cluster_scores
df_final["velocity_score"]  = velocity_scores
df_final["source_weight"]   = df_final["source_name"].apply(_get_source_weight)
df_final["followup_score"]  = df_final.apply(
    lambda r: _get_followup_score(r.get("title_ko"), r.get("original_title")),
    axis=1
)

# 5) priority ê³„ì‚°
df_final["priority"] = (
    df_final["hancom_priority"] * 100 +
    df_final["cluster_score"] * 10 +
    df_final["velocity_score"] * 5 +
    df_final["source_weight"] * 3 +
    df_final["followup_score"] * 2
)

# âœ… ìƒˆë¡œ ì¶”ê°€: ì–¸ì–´ ê· í˜• ë³´ë„ˆìŠ¤
print("\n" + "="*60)
print("ğŸ¯ ì–¸ì–´ ê· í˜• ì¡°ì • ì¤‘...")
print("="*60)

TARGET_ENGLISH_RATIO = 70  # ëª©í‘œ ì˜ë¬¸ ë¹„ìœ¨
BALANCE_BONUS = 8          # ê· í˜• ë³´ë„ˆìŠ¤ ì ìˆ˜ (ë„ˆë¬´ í¬ì§€ ì•Šê²Œ)

for topic_num in [1, 2, 3, 4]:
    topic_mask = df_final["topic_final"] == topic_num
    topic_articles = df_final[topic_mask].to_dict('records')

    if len(topic_articles) == 0:
        continue

    ko_count, en_count, ko_pct, en_pct = calculate_language_ratio(topic_articles)

    # ì˜ë¬¸ì´ ë¶€ì¡±í•˜ë©´ ì˜ë¬¸ ê¸°ì‚¬ì— ë³´ë„ˆìŠ¤
    if en_pct < TARGET_ENGLISH_RATIO - 10:  # 60% ë¯¸ë§Œì´ë©´
        english_bonus = BALANCE_BONUS
        korean_bonus = 0
        print(f"[í† í”½ {topic_num}] ì˜ë¬¸ ë¶€ì¡±({en_pct:.1f}%) â†’ ì˜ë¬¸ ê¸°ì‚¬ ë³´ë„ˆìŠ¤ +{english_bonus}")

    # í•œê¸€ì´ ë¶€ì¡±í•˜ë©´ í•œê¸€ ê¸°ì‚¬ì— ë³´ë„ˆìŠ¤
    elif ko_pct < (100 - TARGET_ENGLISH_RATIO) - 10:  # 20% ë¯¸ë§Œì´ë©´
        english_bonus = 0
        korean_bonus = BALANCE_BONUS
        print(f"[í† í”½ {topic_num}] í•œê¸€ ë¶€ì¡±({ko_pct:.1f}%) â†’ í•œê¸€ ê¸°ì‚¬ ë³´ë„ˆìŠ¤ +{korean_bonus}")

    # ê· í˜•ì´ ì ë‹¹í•˜ë©´ ë³´ë„ˆìŠ¤ ì—†ìŒ
    else:
        english_bonus = 0
        korean_bonus = 0
        print(f"[í† í”½ {topic_num}] í˜„ì¬ ë¹„ìœ¨: ì˜ë¬¸ {en_pct:.1f}%, í•œê¸€ {ko_pct:.1f}% (ë³´ë„ˆìŠ¤ ì—†ìŒ)")

    # ë³´ë„ˆìŠ¤ ì ìš©
    if english_bonus > 0 or korean_bonus > 0:
        topic_indices = df_final[topic_mask].index

        for idx in topic_indices:
            row = df_final.loc[idx]
            is_korean = is_korean_article(row.to_dict())

            bonus = korean_bonus if is_korean else english_bonus
            df_final.at[idx, "priority"] += bonus

# ë³´ë„ˆìŠ¤ ì ìš© í›„ ì¬ì •ë ¬
df_final = df_final.sort_values(
    ["priority", "published_at_dt"],
    ascending=[False, False]
).reset_index(drop=True)

print("\nâœ… ì–¸ì–´ ê· í˜• ì¡°ì • ì™„ë£Œ\n")

print("\n[ì¤‘ìš”ë„(priority) ê³„ì‚° í›„ ìƒìœ„ 5ê°œ]:")
if IN_COLAB:
    display(df_final.head())


topic_main_articles = {}
topic_extra_articles = {}

def _normalize_text(s):
    if s is None:
        return ""
    s = s.lower()
    # í•œê¸€(ê°€-í£), ì˜ë¬¸, ìˆ«ìë§Œ ë‚¨ê¸°ê³  ë‚˜ë¨¸ì§€ëŠ” ê³µë°± ì²˜ë¦¬
    s = re.sub(r"[^0-9a-zA-Z\uAC00-\uD7A3]+", " ", s)
    s = re.sub(r"\s+", " ", s)
    return s.strip()

def is_near_duplicate(text_a, text_b, threshold=0.8):
    """
    ë‘ í…ìŠ¤íŠ¸ê°€ ê±°ì˜ ê°™ì€ì§€ íŒë‹¨.
    thresholdëŠ” 0.8~0.9 ì‚¬ì´ì—ì„œ ì¡°ì ˆí•˜ë©´ ë¨.
    """
    a = _normalize_text(text_a)
    b = _normalize_text(text_b)
    if not a or not b:
        return False
    return difflib.SequenceMatcher(None, a, b).ratio() >= threshold

def df_row_to_article(row, topic_num):
    topic_title = f"{topic_num}) {TOPIC_DESC[topic_num]}"
    return {
        "topic_title": topic_title,
        "ko_title": row["title_ko"],
        "orig_title": row["original_title"],
        "topic_num": str(topic_num),
        "url": row["url"],
        "date": row["published_at"],
        "summary": row["summary_ko"],
        "priority": row.get("priority", 0),
    }


# ============================
# í† í”½ë³„ ì–¸ì–´ ì¿¼í„° ê°•ì œ ì„ íƒ ì„¤ì •
# ============================
TARGET_ENGLISH_RATIO_QUOTA = 70     # EN 70% / KO 30%
MAX_ARTICLES_PER_TOPIC_LANG_QUOTA = 40  # í† í”½ë‹¹ ìµœëŒ€ ê¸°ì‚¬ ìˆ˜

def apply_language_quota_per_topic(
    topic_df: pd.DataFrame,
    target_english_ratio: int = TARGET_ENGLISH_RATIO_QUOTA,
    max_total: int = MAX_ARTICLES_PER_TOPIC_LANG_QUOTA
) -> pd.DataFrame:
    """
    priority ê¸°ì¤€ìœ¼ë¡œ í† í”½ ë‚´ ê¸°ì‚¬ë“¤ì„ EN/KO ì¿¼í„°ë¡œ ê°•ì œ ì„ íƒí•œë‹¤.
    """

    if topic_df is None or len(topic_df) == 0:
        return topic_df

    # 1) priority ê¸°ì¤€ ì •ë ¬
    topic_df = topic_df.sort_values(
        ["priority", "published_at_dt"],
        ascending=[False, False]
    ).copy()

    # 2) ìµœëŒ€ ê°œìˆ˜ ì œí•œ
    if max_total is not None and len(topic_df) > max_total:
        topic_df = topic_df.head(max_total).copy()

    # 3) ì–¸ì–´ íŒë³„ (ì›ë¬¸ ê¸°ë°˜)
    topic_df["_is_ko"] = topic_df.apply(
        lambda r: is_korean_article(r.to_dict()), axis=1
    )

    ko_df = topic_df[topic_df["_is_ko"]].copy()
    en_df = topic_df[~topic_df["_is_ko"]].copy()

    total = len(topic_df)
    en_target = int(round(total * (target_english_ratio / 100.0)))
    ko_target = total - en_target

    # 4) ëª©í‘œ ìˆ˜ë§Œí¼ ìš°ì„  ì„ íƒ
    en_sel = en_df.head(en_target)
    ko_sel = ko_df.head(ko_target)

    # 5) ë¶€ì¡±í•œ ìª½ ë³´ì¶©
    if len(en_sel) < en_target:
        ko_sel = pd.concat([
            ko_sel,
            ko_df.iloc[len(ko_sel):len(ko_sel) + (en_target - len(en_sel))]
        ])

    if len(ko_sel) < ko_target:
        en_sel = pd.concat([
            en_sel,
            en_df.iloc[len(en_sel):len(en_sel) + (ko_target - len(ko_sel))]
        ])

    selected = pd.concat([en_sel, ko_sel], ignore_index=True)

    # 6) ìµœì¢… ì •ë ¬
    selected = selected.sort_values(
        ["priority", "published_at_dt"],
        ascending=[False, False]
    )

    # 7) ì„ì‹œ ì»¬ëŸ¼ ì œê±°
    selected.drop(columns=["_is_ko"], inplace=True, errors="ignore")

    return selected


for topic_num in [1, 2, 3, 4]:
    topic_df = df_final[df_final["topic_final"] == topic_num].copy()

    # 1) priority ìˆœìœ¼ë¡œë§Œ ì •ë ¬ (ë‚ ì§œ ì •ë ¬ ì œê±°)
    topic_df = topic_df.sort_values(
        "priority",
        ascending=False
    )

    # 2) ê±°ì˜ ë™ì¼í•œ ë‚´ìš© ê¸°ì‚¬ ì œê±° (âœ… GPT ì˜ë¯¸ ê¸°ë°˜ dedup)
    dedup_rows = []
    for _, row in topic_df.iterrows():
        cur_title_raw = row.get("title_ko") or row.get("original_title") or ""
        cur_title = _normalize_text(cur_title_raw)
        cur_summary = (row.get("summary_ko") or "").strip()

        is_dup = False

        for kept in dedup_rows:
            kept_title_raw = kept.get("title_ko") or kept.get("original_title") or ""
            kept_title = _normalize_text(kept_title_raw)
            kept_summary = (kept.get("summary_ko") or "").strip()

            # â‘  ì™„ì „ ë™ì¼ ì œëª©ì´ë©´ ì¦‰ì‹œ ì¤‘ë³µ
            if cur_title and kept_title and cur_title == kept_title:
                is_dup = True
                break

            # â‘¡ ì œëª© ìœ ì‚¬ë„(ë¹ ë¥¸ 1ì°¨)
            title_sim_dup = is_near_duplicate(cur_title, kept_title, threshold=FAST_TITLE_DUP_THRESHOLD)
            if title_sim_dup:
                is_dup = True
                break

            # â‘¢ ì• ë§¤í•œ êµ¬ê°„ì´ë©´ GPTë¡œ "ì˜ë¯¸ ì¤‘ë³µ" íŒë³„
            if SEMANTIC_DEDUP_ENABLE:
                # ì œëª© ìœ ì‚¬ë„ ìˆ˜ì¹˜ê°€ í•„ìš”í•˜ë¯€ë¡œ SequenceMatcher ì§ì ‘ ê³„ì‚°
                a = _normalize_text(cur_title)
                b = _normalize_text(kept_title)
                sim = difflib.SequenceMatcher(None, a, b).ratio() if a and b else 0.0

                if SEMANTIC_CHECK_MIN <= sim < SEMANTIC_CHECK_MAX:
                    if gpt_is_same_story(
                        a_title=cur_title_raw, a_summary=cur_summary,
                        b_title=kept_title_raw, b_summary=kept_summary,
                    ):
                        is_dup = True
                        break

        if not is_dup:
            dedup_rows.append(row)

    topic_df = pd.DataFrame(dedup_rows)

    # í† í”½ë³„ EN/KO ë¹„ìœ¨ ì¿¼í„° ê°•ì œ ì ìš©
    topic_df = apply_language_quota_per_topic(topic_df)

    if len(topic_df) == 0:
        print(f"[ê²½ê³ ] í† í”½ {topic_num}ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        topic_main_articles[topic_num] = []
        topic_extra_articles[topic_num] = []
        continue



    if len(topic_df) == 0:
        print(f"[ê²½ê³ ] í† í”½ {topic_num}ì— ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.")
        topic_main_articles[topic_num] = []
        topic_extra_articles[topic_num] = []
        continue

    # 3) ë©”ì¸ 3ê°œ
    main_df = topic_df.head(ARTICLES_PER_TOPIC_FINAL)

    # 4) ì¶”ê°€ ê¸°ì‚¬
    extra_df = topic_df.iloc[ARTICLES_PER_TOPIC_FINAL :]

    main_list = [df_row_to_article(row, topic_num) for _, row in main_df.iterrows()]
    extra_list = [df_row_to_article(row, topic_num) for _, row in extra_df.iterrows()]

    topic_main_articles[topic_num] = main_list
    topic_extra_articles[topic_num] = extra_list

# ë””ë²„ê·¸ ì¶œë ¥
total_main = sum(len(v) for v in topic_main_articles.values())
total_extra = sum(len(v) for v in topic_extra_articles.values())
print("ë©”ì¸ ê¸°ì‚¬ ì´ ê°œìˆ˜:", total_main)
print("ì¶”ê°€ ê¸°ì‚¬ ì´ ê°œìˆ˜:", total_extra)
for t in [1, 2, 3, 4]:
    print(f"í† í”½ {t}: ë©”ì¸ {len(topic_main_articles[t])}ê°œ, ì¶”ê°€ {len(topic_extra_articles[t])}ê°œ")

# âœ… ìµœì¢… ì–¸ì–´ ë¹„ìœ¨ ëª¨ë‹ˆí„°ë§
print("\n" + "="*60)
print("ğŸ“Š í† í”½ë³„ ìµœì¢… ê¸°ì‚¬ ì–¸ì–´ ë¹„ìœ¨")
print("="*60)

for topic_num in [1, 2, 3, 4]:
    main_list = topic_main_articles.get(topic_num, [])
    extra_list = topic_extra_articles.get(topic_num, [])

    all_articles = main_list + extra_list
    ko_count, en_count, ko_pct, en_pct = calculate_language_ratio(all_articles)

    print(f"\ní† í”½ {topic_num} ({TOPIC_DESC[topic_num]}):")
    print(f"  ì˜ë¬¸: {en_count}ê°œ ({en_pct:.1f}%)")
    print(f"  í•œê¸€: {ko_count}ê°œ ({ko_pct:.1f}%)")
    print(f"  ì´: {len(all_articles)}ê°œ")

    # ê²½ê³  í‘œì‹œ
    if en_pct < 50:
        print(f"  âš ï¸ ì˜ë¬¸ ê¸°ì‚¬ ë¶€ì¡± ({en_pct:.1f}%)")
    elif ko_pct < 20:
        print(f"  âš ï¸ í•œê¸€ ê¸°ì‚¬ ë¶€ì¡± ({ko_pct:.1f}%)")
    else:
        print(f"  âœ… ê· í˜• ì–‘í˜¸")

print("\n" + "="*60 + "\n")


# # **07 ìµœì‹  ì—°êµ¬ë™í–¥ (í•™ìˆ ì§€ ì„¹ì…˜) ì„¤ì •**

# In[ ]:


# ============================================
# 7. ìµœì‹  ì—°êµ¬ë™í–¥ (í•™ìˆ ì§€ ì„¹ì…˜) ì„¤ì •
# ============================================

RESEARCH_SECTION_TITLE = "ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥"

# ì €ë„ë³„ ìµœëŒ€ ê°€ì ¸ì˜¬ ë…¼ë¬¸ ìˆ˜ (ì €ë„ 2ê°œ Ã— 250í¸ = ìµœëŒ€ 500í¸)
RESEARCH_MAX_PER_JOURNAL = 50

# ì—°êµ¬ë™í–¥ ì „ìš© ê³ ì • ì¸ë„¤ì¼
RESEARCH_THUMB = "https://img.freepik.com/premium-vector/laboratory-research-flat-style-design-vector-illustration-stock-illustration_357500-507.jpg"

# CrossRefì—ì„œ ì‚¬ìš©í•  ì €ë„ ëª©ë¡ (ISSN ê¸°ë°˜)
CROSSREF_JOURNALS = [
    {
        "journal_name": "Remote Sensing (MDPI)",
        "issn": "2072-4292",
        "logo_url": "https://www.mdpi.com/img/journals/rs-logo.png",
        "rss_url": "https://www.mdpi.com/rss/journal/remotesensing",
    },
    {
        "journal_name": "Journal of Applied Remote Sensing (SPIE)",
        "issn": "1931-3195",
        "logo_url": "https://www.spiedigitallibrary.org/by-type/journals/journal-of-applied-remote-sensing/~/media/Images/Journals/JARS/JARS_Cover.ashx",
        # SPIE RSSëŠ” í™•ì‹¤í•œ ê³µì‹ ë§í¬ê°€ ì—†ì–´ì„œ ì¼ë‹¨ ë¹„ì›Œë‘ê³  CrossRefì— ë§¡ê¹€
        "rss_url": None,
    },
]

# ============================================
# OpenAlex ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ìˆ˜ì§‘ (1ìˆœìœ„)
# ============================================

def _openalex_rebuild_abstract(inv_idx: dict) -> str:
    """
    OpenAlexì˜ abstract_inverted_indexë¥¼ í‰ë¬¸ ë¬¸ìì—´ë¡œ ë³µì›í•˜ëŠ” ìœ í‹¸ í•¨ìˆ˜.
    inv_idx ì˜ˆì‹œ:
      {
        "remote": [0, 10],
        "sensing": [1],
        ...
      }
    â†’ ìœ„ì¹˜ ì¸ë±ìŠ¤ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‹¨ì–´ ë°°ì—´ì„ ë§Œë“¤ì–´ í•©ì¹œë‹¤.
    """
    if not inv_idx:
        return ""
    # pos â†’ token ë§¤í•‘
    pos_map = {}
    for token, positions in inv_idx.items():
        for p in positions:
            pos_map[p] = token
    if not pos_map:
        return ""
    max_pos = max(pos_map.keys())
    tokens = []
    for i in range(max_pos + 1):
        tokens.append(pos_map.get(i, ""))
    return " ".join(t for t in tokens if t).strip()


def collect_research_articles_from_openalex(
    max_per_journal: int = RESEARCH_MAX_PER_JOURNAL,
):
    """
    OpenAlex APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ë„ë³„ ìµœì‹  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    - MDPI, SPIE ë‘˜ ë‹¤ OpenAlexë¥¼ 1ìˆœìœ„ë¡œ ì‚¬ìš©
    - ë°˜í™˜ í˜•ì‹ì€ collect_research_articles_from_crossrefì™€ ë™ì¼í•˜ê²Œ ë§ì¶˜ë‹¤.
    """
    base_url = "https://api.openalex.org/works"
    headers = {
        "User-Agent": "InspaceNewsletterBot/1.0 (mailto:newsletter@example.com)"
    }

    collected = []

    for j in CROSSREF_JOURNALS:
        journal_name = j["journal_name"]
        issn = j["issn"]
        logo_url = j.get("logo_url", RESEARCH_THUMB)

        # OpenAlex filter:
        #  - primary_location.source.issn : ì €ë„ ISSN
        #  - from_publication_date / to_publication_date : ì´ë²ˆ ì£¼ ë‚ ì§œ ë²”ìœ„
        oa_filter = ",".join([
            f"primary_location.source.issn:{issn}",
            f"from_publication_date:{DATE_FROM}",
            f"to_publication_date:{DATE_TO}",
        ])

        params = {
            "filter": oa_filter,
            "sort": "publication_date:desc",
            "per_page": max_per_journal,
        }

        print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name} ë…¼ë¬¸ ëª©ë¡ ìˆ˜ì§‘ (ISSN={issn})")

        try:
            resp = requests.get(base_url, params=params, headers=headers, timeout=10)
            resp.raise_for_status()
            data = resp.json()
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name} ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        items = data.get("results") or []
        print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name} ì‘ë‹µ ë…¼ë¬¸ ìˆ˜:", len(items))

        used = 0
        for it in items[:max_per_journal]:
            # 1) ì œëª©
            original_title = (it.get("title") or it.get("display_name") or "").strip()
            if not original_title:
                continue

            # 2) ì´ˆë¡ (abstract_inverted_index ìš°ì„  ì‚¬ìš©)
            inv = it.get("abstract_inverted_index") or {}
            if inv:
                abstract_clean = _openalex_rebuild_abstract(inv)
            else:
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ OpenAlexê°€ ì œê³µí•˜ëŠ” ê°„ë‹¨ description ë˜ëŠ” ë¹ˆ ë¬¸ìì—´
                abstract_clean = (it.get("abstract") or "").strip()

            # 3) ë§í¬ (landing_page_url â†’ pdf_url â†’ doi_url ìˆœì„œ)
            primary_loc = it.get("primary_location") or {}
            landing_url = primary_loc.get("landing_page_url")
            pdf_url = primary_loc.get("pdf_url")

            doi = it.get("doi")  # ë³´í†µ 'https://doi.org/...' í˜•íƒœì¼ ìˆ˜ë„ ìˆìŒ
            if doi and not doi.startswith("http"):
                doi_url = f"https://doi.org/{doi}"
            else:
                doi_url = doi

            click_url = landing_url or pdf_url or doi_url
            if not click_url:
                # ë§ˆì§€ë§‰ìœ¼ë¡œ OpenAlex ìì²´ ë§í¬ë¼ë„ ì‚¬ìš©
                click_url = it.get("id")

            # 4) ë°œí–‰ì¼
            published_at = (it.get("publication_date") or "")[:10]
            if not published_at:
                # ê·¸ë˜ë„ ì—†ìœ¼ë©´ ì´ë²ˆ ì£¼ ë ë‚ ì§œë¡œ ëŒ€ì²´
                published_at = DATE_TO

            collected.append({
                "journal_name": journal_name,
                "logo_url": logo_url,
                "original_title": original_title,
                "summary_en": abstract_clean,
                "url": click_url,          # âœ… ë‰´ìŠ¤ë ˆí„°ì—ì„œ í´ë¦­ë˜ëŠ” ë§í¬
                "published_at": published_at,
                # CrossRefì™€ í¬ë§· ë§ì¶”ê¸° ìœ„í•´ ë³´ë„ˆìŠ¤ í•„ë“œ
                "doi_url": doi_url or click_url,
                "publisher_url": landing_url or pdf_url or doi_url,
            })
            used += 1

        print(f"[ì—°êµ¬ë™í–¥-OpenAlex] {journal_name}: ì‹¤ì œ ìˆ˜ì§‘ {used}í¸")

    # ë‚ ì§œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ (CrossRef í•¨ìˆ˜ì™€ ë™ì¼í•œ ì •ë ¬ ë°©ì‹ ìœ ì§€)
    def _parse_date_safe(s):
        try:
            return datetime.strptime(s, "%Y-%m-%d")
        except Exception:
            return datetime.now()

    collected = sorted(
        collected,
        key=lambda x: _parse_date_safe(x["published_at"]),
        reverse=True,
    )

    return collected


def collect_research_articles_from_rss(max_per_journal=RESEARCH_MAX_PER_JOURNAL):
    """
    CrossRefê°€ í„°ì¡Œì„ ë•Œë¥¼ ìœ„í•œ ë°±ì—… ì†ŒìŠ¤:
    MDPI / SPIE RSSì—ì„œ ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•´ì„œ
    7-2 ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒê³¼ ë™ì¼í•œ í˜•íƒœì˜ ë¦¬ìŠ¤íŠ¸ë¥¼ ë°˜í™˜í•œë‹¤.
    """
    all_articles = []

    for j in CROSSREF_JOURNALS:
        rss_url = j.get("rss_url")
        if not rss_url:
            # ì´ ì €ë„ì€ RSS ì£¼ì†Œë¥¼ ì•„ì§ ì•ˆ ë„£ì—ˆìœ¼ë©´ ìŠ¤í‚µ
            continue

        print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSS ìˆ˜ì§‘ ì‹œë„: {rss_url}")
        try:
            feed = feedparser.parse(rss_url)
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSS íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

        if not feed.entries:
            print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSS ì—”íŠ¸ë¦¬ ì—†ìŒ")
            continue

        # ìµœì‹  ìˆœìœ¼ë¡œ ë˜ì–´ ìˆë‹¤ê³  ê°€ì •í•˜ê³  ì•ì—ì„œ max_per_journalê°œë§Œ ì‚¬ìš©
        for entry in feed.entries[:max_per_journal]:
            title = getattr(entry, "title", "").strip()
            link = getattr(entry, "link", "").strip()

            # pubDate â†’ YYYY-MM-DD ë¡œ ë³€í™˜ ì‹œë„
            pub_date_str = ""
            if hasattr(entry, "published"):
                try:
                    dt = dateparser.parse(entry.published)
                    if dt:
                        pub_date_str = dt.date().isoformat()
                except Exception:
                    pass

            # ìš”ì•½ (summary / description)
            summary = getattr(entry, "summary", "") or getattr(entry, "description", "") or ""
            summary = " ".join(str(summary).split())

            # 7-2 ë‹¨ê³„ì—ì„œ ì‚¬ìš©í•˜ëŠ” í•„ë“œ ì´ë¦„ì— ë§ì¶°ì„œ ë¦¬í„´
            all_articles.append({
                "journal_name": j["journal_name"],
                "logo_url": j["logo_url"],
                "original_title": title,
                "summary_en": summary,
                "url": link,                # âœ… ì—¬ê¸°ì„œ ë°”ë¡œ ì €ë„ ì›ë¬¸ ë§í¬
                "published_at": pub_date_str or DATE_TO,  # ë‚ ì§œ ëª» íŒŒì‹±í•˜ë©´ ì¼ë‹¨ ì´ë²ˆ ì£¼ ë ë‚ ì§œë¡œ
            })

        print(f"[ì—°êµ¬ë™í–¥-RSS] {j['journal_name']} RSSì—ì„œ {min(len(feed.entries), max_per_journal)}í¸ ìˆ˜ì§‘")

    return all_articles


SYSTEM_PROMPT_RESEARCH = """
ë‹¹ì‹ ì€ ì›ê²©íƒì‚¬(Remote Sensing) / ìœ„ì„± ì˜ìƒ ë¶„ì„ ë¶„ì•¼ì˜ ì—°êµ¬ë™í–¥ì„ ì •ë¦¬í•˜ëŠ” ì—ë””í„°ì…ë‹ˆë‹¤.

ì…ë ¥ìœ¼ë¡œ í•™ìˆ  ë…¼ë¬¸ì˜ ë©”íƒ€ë°ì´í„°(ì €ë„ëª…, ì˜ë¬¸ ì œëª©, ì˜ë¬¸ ì´ˆë¡/ìš”ì•½, ë°œí–‰ì¼, ë§í¬)ê°€ ì£¼ì–´ì§‘ë‹ˆë‹¤.
ì´ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ, ì•„ë˜ í˜•ì‹ì˜ JSONë§Œ ë°˜í™˜í•˜ì„¸ìš”.

ìš”êµ¬ì‚¬í•­:
- ë…¼ë¬¸ ë‚´ìš©ì„ í•œêµ­ì–´ë¡œ 3~4ë¬¸ì¥ ì •ë„ë¡œ ìš”ì•½í•˜ì„¸ìš”.
- í•œêµ­ì–´ ì œëª©ì€ ì‚¬ë‚´ ë‰´ìŠ¤ë ˆí„°ìš©ìœ¼ë¡œ, ë„ˆë¬´ ê¸¸ì§€ ì•Šê²Œ 30ì ë‚´ì™¸ë¡œ ì‘ì„±í•˜ì„¸ìš”.
- ê°€ëŠ¥í•œ í•œ, ë¹„ì „ë¬¸ê°€ë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ ë¶€ë“œëŸ¬ìš´ í•œêµ­ì–´ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.

ì¶œë ¥ JSON í˜•ì‹:
{
  "title_ko": "í•œêµ­ì–´ ì œëª©",
  "summary_ko": "í•œêµ­ì–´ ìš”ì•½ (3~4ë¬¸ì¥)"
}
"""

def summarize_research_article_to_ko(meta: dict, model: str = MODEL_NAME):
    global total_tokens_used

    """
    CrossRefì—ì„œ ê°€ì ¸ì˜¨ ë…¼ë¬¸ ë©”íƒ€ ì •ë³´ë¥¼ í•œêµ­ì–´ ì œëª©/ìš”ì•½ìœ¼ë¡œ ë³€í™˜
    meta ì˜ˆì‹œ:
      {
        "journal_name": "...",
        "original_title": "...",
        "summary_en": "...",
        "published_at": "YYYY-MM-DD",
        "url": "...",
      }
    """
    user_prompt = f"""
[ì €ë„ëª…]
{meta.get('journal_name')}

[ë…¼ë¬¸ ì œëª©]
{meta.get('original_title')}

[ë…¼ë¬¸ ìš”ì•½(ì˜ë¬¸)]
{meta.get('summary_en')}

[ë°œí–‰ì¼]
{meta.get('published_at')}

[ë§í¬]
{meta.get('url')}
"""

    resp = client.responses.create(
        model=model,
        input=[
            {"role": "system", "content": SYSTEM_PROMPT_RESEARCH},
            {"role": "user", "content": user_prompt},
        ],
        temperature=0.3,
    )

    # í† í° ì‚¬ìš©ëŸ‰ ëˆ„ì 
    if hasattr(resp, 'usage'):
        total_tokens_used["input"] += resp.usage.input_tokens
        total_tokens_used["output"] += resp.usage.output_tokens

    text_out = resp.output[0].content[0].text
    try:
        data = json.loads(text_out)
    except Exception:
        # JSON íŒŒì‹± ì‹¤íŒ¨ ì‹œ ê°„ë‹¨íˆ fallback
        data = {
            "title_ko": (meta.get("original_title") or "")[:30],
            "summary_ko": (meta.get("summary_en") or "")[:400],
        }
    return data

def collect_research_articles_from_crossref(
    max_per_journal: int = RESEARCH_MAX_PER_JOURNAL
):
    """
    CrossRef APIë¥¼ ì‚¬ìš©í•˜ì—¬ ì €ë„ë³„ ìµœì‹  ë…¼ë¬¸ ë©”íƒ€ë°ì´í„°ë¥¼ ìˆ˜ì§‘í•œë‹¤.
    ë°˜í™˜: ë¦¬ìŠ¤íŠ¸[dict]
      dict = {
        "journal_name": str,
        "logo_url": str,
        "original_title": str,
        "summary_en": str,
        "url": str,
        "published_at": "YYYY-MM-DD",
      }
    """
    collected = []

    headers = {
        "User-Agent": "InspaceNewsletterBot/1.0 (mailto:newsletter@example.com)"
    }
    base_url = "https://api.crossref.org/journals"

    for j in CROSSREF_JOURNALS:
        journal_name = j["journal_name"]
        issn = j["issn"]
        logo_url = j.get("logo_url", RESEARCH_THUMB)

        api_url = f"{base_url}/{issn}/works?sort=published&order=desc&rows={max_per_journal}"
        print(f"[ì—°êµ¬ë™í–¥] {journal_name} ë…¼ë¬¸ ëª©ë¡ ìˆ˜ì§‘ (CrossRef, ISSN={issn})")

        try:
            resp = requests.get(api_url, headers=headers, timeout=15)
            resp.raise_for_status()
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥] {journal_name} CrossRef ìš”ì²­ ì‹¤íŒ¨: {e}")
            continue

        try:
            data = resp.json()
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥] {journal_name} CrossRef JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
            continue

        items = (data.get("message") or {}).get("items") or []
        print(f"[ì—°êµ¬ë™í–¥] {journal_name} CrossRef ì‘ë‹µ ë…¼ë¬¸ ìˆ˜:", len(items))

        used = 0
        for it in items[:max_per_journal]:
            title_list = it.get("title") or []
            original_title = title_list[0] if title_list else ""
            if not original_title:
                continue

            abstract_raw = it.get("abstract") or ""
            if abstract_raw:
                try:
                    abstract_clean = BeautifulSoup(abstract_raw, "html.parser").get_text(" ", strip=True)
                except Exception:
                    abstract_clean = abstract_raw
            else:
                abstract_clean = ""

            # --- (1) DOI URL (í´ë¦­ìš©) ---
            doi = it.get("DOI")
            doi_url = f"https://doi.org/{doi}" if doi else (it.get("URL") or "")

            # --- (2) publisher URL (í¬ë¡¤ë§ìš©, ìˆìœ¼ë©´ ì‚¬ìš©) ---
            publisher_url = None
            for link in it.get("link") or []:
                u = link.get("URL")
                if not u:
                    continue
                # ìš°ì„ ìˆœìœ„: ì‹¤ì œ ì €ë„ ë„ë©”ì¸
                if "spiedigitallibrary.org" in u or "mdpi.com" in u:
                    publisher_url = u
                    break
                if not publisher_url:
                    publisher_url = u

            # --- (3) ë°œí–‰ì¼ ---
            pub = it.get("published-print") or it.get("published-online") or {}
            date_parts = (pub.get("date-parts") or [[]])[0]
            if date_parts:
                year = date_parts[0]
                month = date_parts[1] if len(date_parts) > 1 else 1
                day = date_parts[2] if len(date_parts) > 2 else 1
                published_at = f"{year:04d}-{month:02d}-{day:02d}"
            else:
                published_at = datetime.now(timezone.utc).date().isoformat()

            collected.append({
                "journal_name": journal_name,
                "logo_url": logo_url,
                "original_title": original_title,
                "summary_en": abstract_clean,

                # âœ… ë‰´ìŠ¤ë ˆí„°ì—ì„œ ì‚¬ìš©ìê°€ í´ë¦­í•  ë§í¬ëŠ” DOI ì£¼ì†Œ(ë˜ëŠ” CrossRef URL)ë¡œ
                "url": doi_url,

                "published_at": published_at,

                # âœ… ë‚˜ì¤‘ì— fallback í¬ë¡¤ë§í•  ë•Œ ì“°ë ¤ê³  ë”°ë¡œ ë³´ê´€
                "doi_url": doi_url,
                "publisher_url": publisher_url,
            })
            used += 1

        print(f"[ì—°êµ¬ë™í–¥] {journal_name}: ì‹¤ì œ ìˆ˜ì§‘ {used}í¸")

    def _parse_date_safe(s):
        try:
            return datetime.strptime(s, "%Y-%m-%d")
        except Exception:
            return datetime.now()

    collected = sorted(
        collected,
        key=lambda x: _parse_date_safe(x["published_at"]),
        reverse=True,
    )
    return collected


# # **07-2 ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€**

# In[ ]:


# ============================================
# 7-2. ìµœì‹  ì—°êµ¬ë™í–¥(í•™ìˆ ì§€) ìˆ˜ì§‘ & ìš”ì•½
# ============================================
import requests
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlsplit  # urlsplit ì¶”ê°€

def clean_and_shorten_summary(text, max_chars=220):
    """
    CrossRefì—ì„œ ê°€ì ¸ì˜¨ ì˜ì–´ ì´ˆë¡ì„ ì •ë¦¬í•˜ê³ ,
    ë„ˆë¬´ ê¸¸ë©´ max_chars ê¸°ì¤€ìœ¼ë¡œ ì˜ë¼ì„œ '... 'ì„ ë¶™ì—¬ì¤€ë‹¤.
    """
    if not text:
        return ""

    # ì¤„ë°”ê¿ˆ/ê³µë°± ì •ë¦¬
    text = " ".join(str(text).split())

    # ì´ë¯¸ ì¶©ë¶„íˆ ì§§ìœ¼ë©´ ê·¸ëŒ€ë¡œ ì‚¬ìš©
    if len(text) <= max_chars:
        return text

    # ê¸€ì ìˆ˜ ê¸°ì¤€ìœ¼ë¡œ ìë¥´ë˜, ë‹¨ì–´ ì¤‘ê°„ì—ì„œ ëŠê¸°ì§€ ì•Šê²Œ ë§ˆì§€ë§‰ ê³µë°± ê¸°ì¤€ìœ¼ë¡œ ìë¥´ê¸°
    cut = text[:max_chars]
    last_space = cut.rfind(" ")
    if last_space > 0:
        cut = cut[:last_space]

    return cut + "..."

def fetch_publisher_abstract(url, timeout=8):
    """
    CrossRef ìš”ì•½ì´ ë„ˆë¬´ ì§§ì„ ë•Œ, ì›ë¬¸ í˜ì´ì§€ì—ì„œ abstractë¥¼ í•œ ë²ˆ ë” ë½‘ì•„ë³´ëŠ” í•¨ìˆ˜.
    - ì‹¤íŒ¨í•˜ë©´ ""ì„ ë°˜í™˜ (ì ˆëŒ€ ì˜ˆì™¸ë¥¼ ìœ„ë¡œ ì˜¬ë¦¬ì§€ ì•ŠìŒ)
    """
    if not url:
        return ""

    # SPIE ê°™ì€ ë°ì„œ 403 ë§‰ëŠ” ê±¸ í”¼í•˜ë ¤ê³  ë¸Œë¼ìš°ì € User-Agent ì„¸íŒ…
    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0 Safari/537.36"
        ),
        "Accept-Language": "en-US,en;q=0.9",
    }

    try:
        resp = requests.get(url, timeout=timeout, headers=headers)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ì—°êµ¬ë™í–¥] fallback ìš”ì•½ ìˆ˜ì§‘ ì‹¤íŒ¨({url}): {e}")
        return ""

    html = resp.text
    soup = BeautifulSoup(html, "html.parser")

    # ì‚¬ì´íŠ¸ë§ˆë‹¤ êµ¬ì¡°ê°€ ë‹¤ë¥´ì§€ë§Œ, ê°€ì¥ í”í•œ abstract ì˜ì—­ ëª‡ êµ°ë°ë¥¼ ìˆœì„œëŒ€ë¡œ ì‹œë„
    selectors = [
        "section.abstract",
        "section#abstract",
        "div.abstract",
        "div#abstract",
        "div.article__body",
    ]

    abstract_el = None
    for sel in selectors:
        abstract_el = soup.select_one(sel)
        if abstract_el:
            break

    if not abstract_el:
        return ""

    text = " ".join(abstract_el.get_text(" ", strip=True).split())
    return text


def fetch_article_abstract_fallback(url, timeout=15):
    """
    CrossRef ìš”ì•½ì´ ì—†ê±°ë‚˜ ë„ˆë¬´ ì§§ì„ ë•Œ,
    ì‹¤ì œ ì €ë„ í˜ì´ì§€(SPIE, MDPI ë“±)ì— ë“¤ì–´ê°€ì„œ abstract/descriptionì„ ìµœëŒ€í•œ ë½‘ì•„ì˜¤ëŠ” í•¨ìˆ˜.
    """
    if not url:
        return ""

    try:
        headers = {
            "User-Agent": "InspaceNewsletterBot/1.0 (mailto:newsletter@example.com)"
        }
        resp = requests.get(url, headers=headers, timeout=timeout)
        resp.raise_for_status()
    except Exception as e:
        print(f"[ì—°êµ¬ë™í–¥] fallback ìš”ì•½ ìˆ˜ì§‘ ì‹¤íŒ¨({url}): {e}")
        return ""

    soup = BeautifulSoup(resp.text, "html.parser")

    # 1) meta íƒœê·¸ ìª½ì—ì„œ abstract/description ìš°ì„  ì‹œë„
    for meta in soup.find_all("meta"):
        name = (meta.get("name") or "").lower()
        prop = (meta.get("property") or "").lower()
        content = (meta.get("content") or "").strip()

        if not content:
            continue

        # SPIE/ì €ë„ ì‚¬ì´íŠ¸ë“¤ì´ ìì£¼ ì“°ëŠ” íŒ¨í„´ë“¤
        if name in ["description", "dc.description", "citation_abstract"] or prop in ["og:description"]:
            content = " ".join(content.split())
            if len(content) > 40:
                return content

    # 2) 'abstract' ê´€ë ¨ ë¸”ë¡ì„ ì§ì ‘ ì°¾ì•„ë³´ê¸°
    candidates = []
    selectors = [
        "div.abstract",
        "section.abstract",
        "div#abstract",
        "section#abstract",
        "div.article__abstract",
    ]
    for sel in selectors:
        for node in soup.select(sel):
            text = " ".join(node.get_text(" ", strip=True).split())
            if len(text) > 40:
                candidates.append(text)

    if candidates:
        # ê¸¸ì´ê°€ ì œì¼ ê¸´ ê±¸ í•˜ë‚˜ ê³ ë¦„
        return max(candidates, key=len)

    # 3) ê·¸ë˜ë„ ëª» ì°¾ìœ¼ë©´, ë³¸ë¬¸ <p> ì¤‘ ì•ì˜ ëª‡ ê°œë¥¼ ì´ì–´ë¶™ì—¬ì„œ pseudo-abstractë¡œ ì‚¬ìš©
    paras = []
    for p in soup.find_all("p"):
        t = " ".join(p.get_text(" ", strip=True).split())
        if len(t) < 40:
            continue
        paras.append(t)
        if len(paras) >= 3:
            break

    return " ".join(paras)

print("\n=== [7-2 ë‹¨ê³„] ìµœì‹  ì—°êµ¬ë™í–¥(í•™ìˆ ì§€) ìˆ˜ì§‘ & ìš”ì•½ ===")

# 1ìˆœìœ„: OpenAlex ê¸°ë°˜ ë…¼ë¬¸ ë©”íƒ€ ìˆ˜ì§‘
research_raw_articles = collect_research_articles_from_openalex(
    max_per_journal=RESEARCH_MAX_PER_JOURNAL
)
print("OpenAlex ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))

# OpenAlexì—ì„œ ì•„ë¬´ê²ƒë„ ëª» ê°€ì ¸ì™”ì„ ë•Œë§Œ CrossRefë¡œ í´ë°±
if not research_raw_articles:
    print("[ì—°êµ¬ë™í–¥] OpenAlex ê²°ê³¼ ì—†ìŒ â†’ CrossRefë¡œ í´ë°± ì‹œë„")
    research_raw_articles = collect_research_articles_from_crossref(
        max_per_journal=RESEARCH_MAX_PER_JOURNAL
    )
    print("CrossRef ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))

# (ì„ íƒ) ê·¸ë˜ë„ 0ê°œë©´, ë§ˆì§€ë§‰ìœ¼ë¡œ RSSê¹Œì§€ ì¨ë³´ê³  ì‹¶ë‹¤ë©´:
if not research_raw_articles:
    print("[ì—°êµ¬ë™í–¥] OpenAlex + CrossRef ëª¨ë‘ ì‹¤íŒ¨ â†’ RSSë¡œ í´ë°± ì‹œë„")
    research_raw_articles = collect_research_articles_from_rss(
        max_per_journal=RESEARCH_MAX_PER_JOURNAL
    )
    print("RSS ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))


# 2) CrossRef ê²°ê³¼ê°€ 0í¸ì´ë©´ â†’ RSS ë°±ì—… ì†ŒìŠ¤ ì‚¬ìš©
if not research_raw_articles:
    print("[ì—°êµ¬ë™í–¥] CrossRef ê²°ê³¼ 0í¸ â†’ RSS ë°±ì—… ì†ŒìŠ¤ ì‹œë„")
    research_raw_articles = collect_research_articles_from_rss(
        max_per_journal=RESEARCH_MAX_PER_JOURNAL
    )
    print("RSS ê¸°ë°˜ í•™ìˆ  ë…¼ë¬¸ ê°œìˆ˜:", len(research_raw_articles))


research_processed_articles = []

# ì—°êµ¬ ë…¼ë¬¸ ë³‘ë ¬ ì²˜ë¦¬ í•¨ìˆ˜
def process_single_research_article(meta):
    """
    ë‹¨ì¼ ì—°êµ¬ ë…¼ë¬¸ì„ ì²˜ë¦¬í•˜ëŠ” í•¨ìˆ˜
    """
    try:
        # 1) CrossRefì—ì„œ ê°€ì ¸ì˜¨ ì˜ì–´ ìš”ì•½ ë¨¼ì € êº¼ë‚´ê¸°
        raw_summary_en = (meta.get("summary_en") or "").strip()

        # ìš”ì•½ì´ ì™„ì „íˆ ë¹„ì—ˆëŠ”ì§€ ì—¬ë¶€
        has_any_crossref_text = bool(raw_summary_en)

        # ë„ˆë¬´ ì§§ê±°ë‚˜(= ê±°ì˜ ì—†ìŒ) ì•„ì˜ˆ ì—†ì–´ë„ í•œ ë²ˆì€ ì›ë¬¸ í˜ì´ì§€ì—ì„œ ë³´ì¶© ì‹œë„
        MIN_LEN_FOR_FALLBACK = 80

        fallback_summary = ""
        if len(raw_summary_en) < MIN_LEN_FOR_FALLBACK:
            target_url = meta.get("publisher_url") or meta.get("url")
            print(
                f"  - [fallback ì‹œë„] CrossRef ìš”ì•½ ë¶€ì¡± â†’ ì›ë¬¸ í˜ì´ì§€ íŒŒì‹± ì‹œë„: "
                f"{meta.get('original_title')} | {target_url}"
            )
            fallback_summary = fetch_publisher_abstract(target_url)

        # 2ë‹¨ê³„: fallback ìš”ì•½ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©, ì—†ìœ¼ë©´ CrossRef ìš”ì•½ ì‚¬ìš©
        final_summary_en = (fallback_summary or raw_summary_en).strip()

        # 3ë‹¨ê³„: ê·¸ë˜ë„ ë‘˜ ë‹¤ ë¹„ì–´ ìˆìœ¼ë©´, ìŠ¤í‚µí•˜ì§€ ë§ê³  ìµœì†Œ ì„¤ëª…ìœ¼ë¡œ ì‚´ë ¤ë‘ê¸°
        if not final_summary_en:
            title = (meta.get("original_title") or "").strip()
            journal = (meta.get("journal_name") or "").strip()
            final_summary_en = (
                f"No abstract available. This paper ({title}) was published in {journal}."
            )

        # ì—¬ê¸°ê¹Œì§€ ì˜¤ë©´ final_summary_enì—ëŠ” ë­”ê°€ í…ìŠ¤íŠ¸ê°€ ë“¤ì–´ìˆìŒ
        raw_summary_en = final_summary_en

        # 3) ìš”ì•½ ê¸¸ì´ ì •ë¦¬
        short_summary_en = clean_and_shorten_summary(raw_summary_en, max_chars=200)

        # 4) í•œê¸€ ìš”ì•½ (GPT)
        try:
            ko_data = summarize_research_article_to_ko({
                **meta,
                "summary_en": short_summary_en,
            })
        except Exception as e:
            print(f"[ì—°êµ¬ë™í–¥ ìš”ì•½ ì˜¤ë¥˜] {meta.get('url')}: {e}")
            ko_data = {}

        thumb_url = RESEARCH_THUMB

        return {
            "journal_name": meta["journal_name"],
            "logo_url": meta["logo_url"],
            "original_title": meta["original_title"],
            "summary_en": short_summary_en,
            "url": meta["url"],
            "published_at": meta["published_at"],
            "title_ko": ko_data.get("title_ko", meta["original_title"][:30]),
            "summary_ko": ko_data.get("summary_ko", short_summary_en),
            "thumbnail_url": thumb_url,
        }
    except Exception as e:
        print(f"[ì—°êµ¬ë™í–¥ ì²˜ë¦¬ ì˜¤ë¥˜] {meta.get('url')}: {e}")
        return None


# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
print(f"\nì´ {len(research_raw_articles)}ê°œ ì—°êµ¬ ë…¼ë¬¸ì„ ë³‘ë ¬ ì²˜ë¦¬í•©ë‹ˆë‹¤ (ë™ì‹œ ì²˜ë¦¬: 20ê°œ)...")
research_processed_articles = []

with ThreadPoolExecutor(max_workers=20) as executor:
    futures = {executor.submit(process_single_research_article, meta): idx
               for idx, meta in enumerate(research_raw_articles)}

    completed = 0
    for future in as_completed(futures):
        result = future.result()
        if result:
            research_processed_articles.append(result)

        completed += 1
        if completed % 5 == 0:
            print(f"  ì§„í–‰: {completed}/{len(research_raw_articles)} ({completed*100//len(research_raw_articles)}%) - ì„±ê³µ: {len(research_processed_articles)}ê°œ")

print(f"\nì—°êµ¬ ë…¼ë¬¸ ë³‘ë ¬ ì²˜ë¦¬ ì™„ë£Œ! ì´ {len(research_processed_articles)}ê°œ ë…¼ë¬¸ ì²˜ë¦¬ë¨")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# âœ… ì—¬ê¸°ë¶€í„° ìƒˆë¡œ ì¶”ê°€: ì—°êµ¬ë™í–¥ ë‚ ì§œ í•„í„°
#    (ë‰´ìŠ¤ ì„¹ì…˜ì— ì“°ëŠ” DATE_FROM ~ DATE_TOì™€ ë§ì¶”ê¸°)
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
try:
    from_dt = datetime.fromisoformat(DATE_FROM).date()
    to_dt = datetime.fromisoformat(DATE_TO).date()
except Exception as e:
    print("[ì—°êµ¬ë™í–¥] DATE_FROM/DATE_TO íŒŒì‹± ì˜¤ë¥˜, í•„í„°ë¥¼ ê±´ë„ˆëœë‹ˆë‹¤:", e)
else:
    def _parse_pubdate_safe(s: str):
        try:
            # CrossRefì—ì„œ ë§Œë“  published_atì€ 'YYYY-MM-DD' í˜•ì‹
            return datetime.strptime(s, "%Y-%m-%d").date()
        except Exception:
            return None

    filtered = []
    for art in research_processed_articles:
        d = _parse_pubdate_safe(art.get("published_at", ""))
        if d is None:
            continue  # ë‚ ì§œ ì´ìƒí•œ ê±´ ë²„ë¦¼
        if from_dt <= d <= to_dt:
            filtered.append(art)

    print(f"[ì—°êµ¬ë™í–¥] ë‚ ì§œ í•„í„° ì ìš© ì „: {len(research_processed_articles)}í¸")
    print(f"[ì—°êµ¬ë™í–¥] {DATE_FROM} ~ {DATE_TO} ë²”ìœ„ ë‚´: {len(filtered)}í¸")

    research_processed_articles = filtered

    # (ì˜µì…˜) ë‹¤ì‹œ ë‚ ì§œ ê¸°ì¤€ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    research_processed_articles = sorted(
        research_processed_articles,
        key=lambda x: datetime.strptime(x["published_at"], "%Y-%m-%d"),
        reverse=True,
    )

# ë©”ì¸ 3ê°œ + ì¶”ê°€ í•™ìˆ ì§€ + (ì¶”ê°€ í˜ì´ì§€ìš©) ë¦¬ìŠ¤íŠ¸ ë¶„ë¦¬
research_main_articles = []
research_extra_articles = []
research_more_articles = []

if research_processed_articles:
    # 1) ë©”ì¸ ì„¹ì…˜ì— ë³´ì—¬ì¤„ 3ê°œ
    research_main_articles = research_processed_articles[:3]

    # 2) ë©”ì¸ 3ê°œ ì´í›„ë¥¼ 'ì¶”ê°€ í•™ìˆ ì§€'ë¡œ ì‚¬ìš©
    if len(research_processed_articles) > 3:
        research_extra_articles = research_processed_articles[3:]
        # 3) ì¶”ê°€ ì—°êµ¬ë™í–¥ í˜ì´ì§€ì—ëŠ”
        #    "ë©”ì¸ 3ê°œë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ ì „ë¶€"ë§Œ ë„£ê¸°
        research_more_articles = research_extra_articles[:]   # âœ… ì—¬ê¸°ë§Œ ë³€ê²½
    else:
        research_extra_articles = []
        research_more_articles = []

    print("ì—°êµ¬ë™í–¥ ë©”ì¸ í•™ìˆ ì§€ ê°œìˆ˜:", len(research_main_articles))
    print("ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ ê°œìˆ˜:", len(research_extra_articles))
else:
    print("[ì•Œë¦¼] ì—°êµ¬ë™í–¥ (CrossRef)ì—ì„œ ê°€ì ¸ì˜¨ ë…¼ë¬¸ì´ ì—†ìŠµë‹ˆë‹¤.")


# # **07-1 ì¸ë„¤ì¼ ì¶”ì¶œ (ê¸°ë³¸ ì¸ë„¤ì¼ í¬í•¨)**

# In[ ]:


import re
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse, parse_qs, unquote

def _is_google_host(host: str) -> bool:
    host = (host or "").lower()
    return ("google." in host) or host.endswith("google.com") or host.endswith("gstatic.com")

def _extract_publisher_url_from_google_html(base_url: str, html: str) -> str:
    """
    news.google.com/rss/articles/... ë˜ëŠ” news.google.com/articles/... (HTTP 200) HTMLì—ì„œ
    ì‹¤ì œ í¼ë¸”ë¦¬ì…” ì›ë¬¸ ë§í¬ë¥¼ ìµœëŒ€í•œ ì°¾ì•„ ë°˜í™˜.
    """
    if not html:
        return ""

    def _clean_and_validate(candidate: str) -> str:
        if not candidate:
            return ""
        candidate = candidate.strip().strip("'\"")
        candidate = urljoin(base_url, candidate)
        parsed = urlparse(candidate)
        if not parsed.scheme.startswith("http"):
            return ""
        if not parsed.netloc or _is_google_host(parsed.netloc):
            return ""
        return candidate

    try:
        soup = BeautifulSoup(html, "html.parser")

        # 1) meta refresh (ìˆìœ¼ë©´ ìµœìš°ì„ )
        meta = soup.find("meta", attrs={"http-equiv": lambda v: v and v.lower() == "refresh"})
        if meta and meta.get("content"):
            content = meta["content"]
            if "url=" in content.lower():
                target = content.split("url=", 1)[-1]
                target = _clean_and_validate(target)
                if target:
                    return target

        # 2) rel=canonical (í¼ë¸”ë¦¬ì…”ë¡œ ë°•í˜€ìˆëŠ” ì¼€ì´ìŠ¤)
        link = soup.find("link", rel=lambda v: v and "canonical" in str(v).lower())
        if link and link.get("href"):
            target = _clean_and_validate(link["href"])
            if target:
                return target

        # 3) a[href]ë¥¼ í›‘ìœ¼ë©°:
        #    (a) ë°”ë¡œ ì™¸ë¶€ ì ˆëŒ€ URLì´ë©´ ë°˜í™˜
        #    (b) google.com/url?... ì— ìˆ¨ê²¨ì§„ url/q íŒŒë¼ë¯¸í„°ë¥¼ íŒŒì‹±í•´ì„œ ë°˜í™˜
        for a in soup.find_all("a", href=True):
            href = (a["href"] or "").strip()
            if not href:
                continue

            full = urljoin(base_url, href)
            p = urlparse(full)

            # (a) ì™¸ë¶€ ì ˆëŒ€ URL
            direct = _clean_and_validate(full)
            if direct:
                return direct

            # (b) google redirect URL íŒ¨í„´: https://www.google.com/url?url=...
            if _is_google_host(p.netloc) and p.path.startswith("/url"):
                qs = parse_qs(p.query)
                for key in ("url", "q"):
                    if key in qs and qs[key]:
                        cand = unquote(qs[key][0])
                        cand = _clean_and_validate(cand)
                        if cand:
                            return cand

        # 4) HTML ë³¸ë¬¸ì—ì„œ url= ë¡œ ë°•í˜€ìˆëŠ” í¼ë¸”ë¦¬ì…” URLì„ ì •ê·œì‹ìœ¼ë¡œë¼ë„ ì°¾ê¸° (ë§ˆì§€ë§‰ ë³´ë£¨)
        #    ë„ˆë¬´ ê³µê²©ì ìœ¼ë¡œ ì¡ì§€ ì•Šê²Œ http(s)ë§Œ ëŒ€ìƒìœ¼ë¡œ.
        for m in re.finditer(r"https?%3A%2F%2F[^\"'<>\\s]+", html):
            cand = unquote(m.group(0))
            cand = _clean_and_validate(cand)
            if cand:
                return cand

        for m in re.finditer(r"https?://[^\"'<>\\s]+", html):
            cand = m.group(0)
            cand = _clean_and_validate(cand)
            if cand:
                return cand

    except Exception:
        return ""

    return ""



from urllib.parse import urlparse, parse_qs, unquote

# --- (A) URL resolve ìºì‹œ (ë™ì¼ URL ë°˜ë³µ í˜¸ì¶œ ë°©ì§€) ---
_URL_RESOLVE_CACHE = {}

def resolve_final_url(url: str, headers: dict, timeout: int = 6) -> str:
    if not url:
        return url
    if url in _URL_RESOLVE_CACHE:
        return _URL_RESOLVE_CACHE[url]

    final = url
    html = ""

    try:
        resp = requests.get(
            url,
            headers=headers or {},
            timeout=timeout,
            allow_redirects=True,
        )
        if resp is not None and getattr(resp, "url", None):
            final = resp.url
        try:
            html = resp.text if resp is not None else ""
        except Exception:
            html = ""
    except Exception:
        _URL_RESOLVE_CACHE[url] = url
        return url

    # 1) ìµœì¢… URL ìì²´ê°€ google.com/url?url=... í˜•íƒœë©´ ì¦‰ì‹œ í•´ì œ
    try:
        p = urlparse(final)
        if _is_google_host(p.netloc) and p.path.startswith("/url"):
            qs = parse_qs(p.query)
            for key in ("url", "q"):
                if key in qs and qs[key]:
                    cand = unquote(qs[key][0]).strip()
                    if cand and not _is_google_host(urlparse(cand).netloc):
                        final = cand
                        _URL_RESOLVE_CACHE[url] = final
                        return final
    except Exception:
        pass

    # 2) ì—¬ì „íˆ google/news.googleë©´ HTMLì—ì„œ í¼ë¸”ë¦¬ì…” ë§í¬ ì¶”ì¶œ
    try:
        host = (urlparse(final).netloc or "").lower()
        if ("news.google.com" in host) or host.endswith("google.com"):
            pub = _extract_publisher_url_from_google_html(final, html)
            if pub:
                final = pub
    except Exception:
        pass

    _URL_RESOLVE_CACHE[url] = final
    return final




# ê´‘ê³ /íŠ¸ë˜í‚¹ ì „ìš© "ì˜êµ¬ ë¸”ë™ë¦¬ìŠ¤íŠ¸" (ì´ê±´ ì¸ë„¤ì¼ì´ ë  ì¼ì´ ì—†ìŒ)
ALWAYS_BAD_HOST_SNIPPETS = [
    "doubleclick.net",
    "googlesyndication.com",
    "scorecardresearch.com",
    "pixel.wp.com",
    "adservice",
    "adsystem",
    "biztoc.com",  # âœ¨ ì• ê·¸ë¦¬ê²Œì´í„° ì¸ë„¤ì¼ì€ í•­ìƒ ê¸°ë³¸ ì´ë¯¸ì§€ë¡œ ëŒ€ì²´
]

# ì‹¤í–‰ ì¤‘ì— ìë™ìœ¼ë¡œ ìŒ“ì´ëŠ” "ë¬¸ì œ í˜¸ìŠ¤íŠ¸" ì¹´ìš´íŠ¸
HOST_FAIL_COUNTS = {}        # host -> fail count
HOST_FAIL_THRESHOLD = 3      # ì´ íšŸìˆ˜ ì´ìƒ ì‹¤íŒ¨í•˜ë©´ ê·¸ í˜¸ìŠ¤íŠ¸ëŠ” ë¬¸ì œ í˜¸ìŠ¤íŠ¸ë¡œ ê°„ì£¼

# "ì‚¬ì´ë“œë°”/ì¶”ì²œ ì˜ì—­"ìœ¼ë¡œ ê°„ì£¼í•  ë§Œí•œ í‚¤ì›Œë“œ
SIDEBAR_KEYWORDS = [
    "sidebar", "aside", "related", "recommend", "recom", "widget",
    "trending", "popular", "most-read", "mostread", "sponsor",
    "sponsored", "ads", "advert", "outbrain", "taboola",
]

# "ë³¸ë¬¸ ì˜ì—­"ìœ¼ë¡œ ì¸ì‹í•  ë§Œí•œ í‚¤ì›Œë“œ
MAIN_CONTENT_KEYWORDS = [
    "article", "post", "content", "entry", "story", "main", "body",
]


def _get_host(url: str) -> str:
    try:
        host = urlsplit(url).hostname or ""
        return host.lower()
    except Exception:
        return ""


def _mark_host_fail(host: str):
    if not host:
        return
    HOST_FAIL_COUNTS[host] = HOST_FAIL_COUNTS.get(host, 0) + 1


def is_bad_image_url(url, timeout=3):
    """
    'ì´ê±´ ì¸ë„¤ì¼ë¡œ ì“°ê¸°ì—” ìˆ˜ìƒí•˜ë‹¤' ì‹¶ì€ ì´ë¯¸ì§€ë§Œ ê³¨ë¼ì„œ ê±°ë¥¸ë‹¤.
    - ê´‘ê³ /íŠ¸ë˜í‚¹ ë„ë©”ì¸: ë¬´ì¡°ê±´ ì œì™¸
    - ê°™ì€ í˜¸ìŠ¤íŠ¸ì—ì„œ ì—¬ëŸ¬ ë²ˆ ì‹¤íŒ¨í•˜ë©´: ìë™ìœ¼ë¡œ "ë¬¸ì œ í˜¸ìŠ¤íŠ¸"ë¡œ ê°„ì£¼
    - ê·¸ ì™¸ì—ëŠ” HEAD ê¸°ì¤€ìœ¼ë¡œ ë„ˆë¬´ ì‘ì€ ì´ë¯¸ì§€ / ì´ë¯¸ì§€ ì•„ë‹˜ / ì—ëŸ¬ ë“±ë§Œ ì œì™¸
    """
    if not url:
        return True

    url = url.strip()
    lower = url.lower()
    host = _get_host(url)

    # data: URI, 1x1 pixel ë“±ì€ ì œì™¸
    if lower.startswith("data:"):
        return True

    if "1x1" in lower and (".gif" in lower or ".png" in lower or ".jpg" in lower):
        return True

    # 1) ê´‘ê³ /íŠ¸ë˜í‚¹ ë„ë©”ì¸ì€ ì˜êµ¬ì ìœ¼ë¡œ ì œì™¸
    if any(sn in host for sn in ALWAYS_BAD_HOST_SNIPPETS):
        return True

    # 2) ì´ë¯¸ ì—¬ëŸ¬ ë²ˆ ì‹¤íŒ¨í•œ "ë¬¸ì œ í˜¸ìŠ¤íŠ¸"ëŠ” ë¹ ë¥´ê²Œ ì œì™¸
    if HOST_FAIL_COUNTS.get(host, 0) >= HOST_FAIL_THRESHOLD:
        return True

    # 3) ê²½ë¡œ ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ íŒŒì¼ì²˜ëŸ¼ ë³´ì´ëŠ”ì§€ íŒë‹¨
    path = urlsplit(url).path.lower()
    looks_like_image = path.endswith((".jpg", ".jpeg", ".png", ".webp", ".gif"))

    try:
        resp = requests.head(url, timeout=timeout, allow_redirects=True)
        status = resp.status_code

        # HEADê°€ ì•„ì˜ˆ ì‹¤íŒ¨ê±°ë‚˜ 4xx/5xxë©´ ì¼ë‹¨ ì´ URLì€ ë‚˜ì¨
        # (ì´ ê²½ìš°ëŠ” í˜¸ìŠ¤íŠ¸ ì‹¤íŒ¨ ì¹´ìš´íŠ¸ì— ë°˜ì˜)
        if status < 200 or status >= 400:
            _mark_host_fail(host)
            # í˜¹ì‹œ 403/405/406ì¸ë° ì´ë¯¸ì§€ì²˜ëŸ¼ ë³´ì´ëŠ” ê²½ìš°ëŠ”
            # "ì´ í™˜ê²½ì—ì„œë§Œ ë§‰íŒ ê²ƒ"ì¼ ìˆ˜ ìˆì–´ì„œ, í•œ ë²ˆì€ ë´ì¤€ë‹¤.
            if looks_like_image and status in (403, 405, 406):
                return False
            return True

        ctype = resp.headers.get("content-type", "").lower()

        # content-typeì— image ê°€ ì—†ê³ , í™•ì‹¤íˆ ì´ë¯¸ì§€ í™•ì¥ìë„ ì•„ë‹ˆë©´ ì œì™¸
        if "image" not in ctype and not looks_like_image:
            _mark_host_fail(host)
            return True

        # content-typeì´ ë¹„ì–´ ìˆëŠ”ë° í™•ì‹¤íˆ ì´ë¯¸ì§€ í™•ì¥ìì¸ ê²½ìš°ëŠ” í—ˆìš©
        if not ctype and looks_like_image:
            return False

        # ë„ˆë¬´ ì‘ì€ content-length (í”½ì…€/ì•„ì´ì½˜ ë“±)ëŠ” ì œì™¸
        clen_header = resp.headers.get("content-length")
        try:
            clen = int(clen_header) if clen_header else 0
        except ValueError:
            clen = 0

        if 0 < clen < 4000:  # 4KB ë¯¸ë§Œ
            return True

        if lower.endswith(".gif") and 0 < clen < 8000:
            return True

    except Exception:
        # ë„¤íŠ¸ì›Œí¬ ì—ëŸ¬ ë“±ì€ host failë¡œ ê°„ì£¼
        _mark_host_fail(host)
        # ì´ë¯¸ì§€ì²˜ëŸ¼ ìƒê¸´ URLì´ë©´ í•œ ë²ˆì€ ë´ì£¼ê³ , ì´í›„ ì¹´ìš´íŠ¸ ëˆ„ì ë˜ë©´ ìë™ ì°¨ë‹¨
        return not looks_like_image

    return False


def is_sidebar_like(tag):
    """
    ì£¼ì–´ì§„ íƒœê·¸(ë˜ëŠ” ìƒìœ„ íŠ¸ë¦¬)ê°€ sidebar/related/recommend ì˜ì—­ì¸ì§€ íŒë‹¨
    """
    cur = tag
    depth = 0
    while cur is not None and depth < 6:  # ë„ˆë¬´ ê¹Šì´ ì•ˆ ì˜¬ë¼ê°€ë„ ë¨
        cid = (cur.get("id") or "").lower()
        cls = " ".join(cur.get("class", [])).lower()

        text = cid + " " + cls
        if any(key in text for key in SIDEBAR_KEYWORDS):
            return True

        cur = cur.parent
        depth += 1
    return False


def is_main_content_like(tag):
    """
    ì£¼ì–´ì§„ íƒœê·¸(ë˜ëŠ” ìƒìœ„ íŠ¸ë¦¬)ê°€ ë³¸ë¬¸(article/main/content) ì˜ì—­ì¸ì§€ ëŒ€ëµ íŒë‹¨
    """
    cur = tag
    depth = 0
    while cur is not None and depth < 6:
        cid = (cur.get("id") or "").lower()
        cls = " ".join(cur.get("class", [])).lower()
        text = cid + " " + cls

        if any(key in text for key in MAIN_CONTENT_KEYWORDS):
            return True

        cur = cur.parent
        depth += 1
    return False


def extract_image_candidates(page_url, html):
    """
    í˜ì´ì§€ì—ì„œ ì¸ë„¤ì¼ í›„ë³´ë¥¼ ëª¨ìœ¼ë˜,
    - 1ìˆœìœ„: og:image / twitter:image / ê¸°íƒ€ meta
    - 2ìˆœìœ„: "ë³¸ë¬¸ ì˜ì—­(article/main/content ë“±)" ë‚´ë¶€ì˜ <img> (sidebar/related ì œì™¸)
    - 3ìˆœìœ„: (ë³¸ë¬¸ ì˜ì—­ì„ ëª» ì°¾ì€ ê²½ìš°ì—ë§Œ) í˜ì´ì§€ ì „ì²´ <img>ì—ì„œ sidebar/related ì œì™¸ í›„ ì‚¬ìš©
    """
    soup = BeautifulSoup(html, "html.parser")
    candidates = []

    def add_candidate(src, score):
        if not src:
            return
        src = src.strip()
        if not src or src.startswith("data:"):
            return
        full = urljoin(page_url, src)
        candidates.append((score, full))

    # 1) og:image, twitter:image ë“± ë©”íƒ€ íƒœê·¸ ìš°ì„ 
    for meta in soup.find_all("meta"):
        prop = (meta.get("property") or "").lower()
        name = (meta.get("name") or "").lower()
        content = meta.get("content") or ""

        if prop in ("og:image", "og:image:url", "og:image:secure_url"):
            add_candidate(content, 100)
        elif name in ("twitter:image", "twitter:image:src"):
            add_candidate(content, 95)
        elif name == "image" or prop == "image":
            add_candidate(content, 90)

    # 2) "ë³¸ë¬¸ ì˜ì—­" í›„ë³´ ì°¾ê¸° (article/main/ë‚´ìš© div ë“±)
    content_areas = []

    # ëŒ€í‘œì ì¸ ë³¸ë¬¸ íƒœê·¸ë“¤
    for tag in soup.find_all(["article", "main", "section", "div"]):
        cid = (tag.get("id") or "").lower()
        cls = " ".join(tag.get("class", [])).lower()
        text = cid + " " + cls

        # MAIN_CONTENT_KEYWORDS ê¸°ì¤€ìœ¼ë¡œ ë³¸ë¬¸ì²˜ëŸ¼ ë³´ì´ëŠ” ì˜ì—­ë§Œ ì„ íƒ
        if any(k in text for k in MAIN_CONTENT_KEYWORDS):
            content_areas.append(tag)

    # ë³¸ë¬¸ ì˜ì—­ì„ í•˜ë‚˜ë„ ëª» ì°¾ìœ¼ë©´, fallbackìœ¼ë¡œ body(or ì „ì²´)ë¥¼ ì‚¬ìš©
    if not content_areas:
        if soup.body:
            content_areas = [soup.body]
        else:
            content_areas = [soup]

    # 3) ë³¸ë¬¸ ì˜ì—­ ë‚´ë¶€ì˜ <img>ë§Œ í›„ë³´ë¡œ ì‚¬ìš© (sidebar-like ì œì™¸)
    for area in content_areas:
        for img in area.find_all("img"):
            src = img.get("src") or ""
            if not src or src.startswith("data:"):
                continue

            # sidebar/related/recommend ì˜ì—­ì´ë©´ ì œì™¸
            if is_sidebar_like(img):
                continue

            score = 40  # ê¸°ë³¸ ì ìˆ˜

            # ë³¸ë¬¸(article/main/content) ìª½ì´ë©´ ì ìˆ˜ ê°€ì‚°
            if is_main_content_like(img):
                score += 20

            # width/heightë¡œ ëŒ€ì¶© í° ì´ë¯¸ì§€ì— ë³´ë„ˆìŠ¤
            w = img.get("width")
            h = img.get("height")
            try:
                w = int(w)
                h = int(h)
                if w >= 200 and h >= 150:
                    score += 10
            except Exception:
                pass

            # classì— hero/featured/main ë“±ì´ ìˆìœ¼ë©´ ì¶”ê°€ ê°€ì‚°
            cls = " ".join(img.get("class", [])).lower()
            if any(x in cls for x in ["hero", "featured", "main", "headline"]):
                score += 10

            add_candidate(src, score)

    # (ì˜µì…˜) ê·¸ë˜ë„ í›„ë³´ê°€ ì „í˜€ ì—†ìœ¼ë©´, ë§ˆì§€ë§‰ ìˆ˜ë‹¨ìœ¼ë¡œ í˜ì´ì§€ ì „ì²´ <img>ì—ì„œ sidebar ì œì™¸ í›„ ì‚¬ìš©
    if not candidates:
        for img in soup.find_all("img"):
            src = img.get("src") or ""
            if not src or src.startswith("data:"):
                continue
            if is_sidebar_like(img):
                continue
            add_candidate(src, 20)

    # ì ìˆ˜ ë†’ì€ ìˆœìœ¼ë¡œ ì •ë ¬ + ì¤‘ë³µ ì œê±°
    seen = set()
    unique = []
    for score, url in sorted(candidates, key=lambda x: x[0], reverse=True):
        if url in seen:
            continue
        seen.add(url)
        unique.append((score, url))

    return unique


def _maybe_follow_canonical(page_url, html, timeout=6, headers=None):
    """
    ì• ê·¸ë¦¬ê²Œì´í„°(biztoc ë“±) íŠ¹ì´ ì¼€ì´ìŠ¤ ì¡ê¸°:
    - í˜ì´ì§€ ì•ˆì— <link rel="canonical" href="..."> ê°€ ìˆê³ 
    - ê·¸ hostê°€ ê¸°ì¡´ page_url ê³¼ ë‹¤ë¥´ë©´
      â†’ ì›ë³¸ ê¸°ì‚¬ë¡œ ê°„ì£¼í•˜ê³  ê·¸ ìª½ HTMLì„ ë‹¤ì‹œ ê°€ì ¸ì™€ì„œ ì‚¬ìš©
    """
    try:
        soup = BeautifulSoup(html, "html.parser")
        link = soup.find("link", rel=lambda v: v and "canonical" in v.lower())
        if not link:
            return page_url, html

        href = (link.get("href") or "").strip()
        if not href:
            return page_url, html

        canonical_url = urljoin(page_url, href)
        orig_host = _get_host(page_url)
        canon_host = _get_host(canonical_url)

        # hostê°€ ë‹¤ë¥´ê³ , canonical ì´ ì¡´ì¬í•˜ë©´ ì›ë³¸ ê¸°ì‚¬ì¼ ê°€ëŠ¥ì„±ì´ ë†’ìŒ
        if canon_host and orig_host and canon_host != orig_host:
            try:
                resp2 = requests.get(
                    canonical_url,
                    timeout=timeout,
                    headers=headers or {}
                )
                if resp2.status_code == 200 and resp2.text:
                    # ì›ë³¸ ê¸°ì‚¬ HTMLë¡œ êµì²´
                    return canonical_url, resp2.text
            except Exception:
                # canonical ë”°ë¼ê°€ë‹¤ ì‹¤íŒ¨í•˜ë©´ ì›ë˜ ê²ƒ ìœ ì§€
                return page_url, html
    except Exception:
        return page_url, html

    return page_url, html


def fetch_thumbnail(url, timeout=6):
    if not url:
        return DEFAULT_THUMB

    headers = {
        "User-Agent": (
            "Mozilla/5.0 (Windows NT 10.0; Win64; x64) "
            "AppleWebKit/537.36 (KHTML, like Gecko) "
            "Chrome/123.0 Safari/537.36"
        )
    }

    # âœ… (1) Google RSS/News ì¤‘ê³„ URLì´ë©´ ìµœì¢… ì›ë¬¸ URLë¡œ ë¨¼ì € resolve
    resolved_url = resolve_final_url(url, headers=headers, timeout=timeout)

    try:
        resp = requests.get(resolved_url, timeout=timeout, headers=headers)
        if resp.status_code != 200:
            return DEFAULT_THUMB
    except Exception:
        return DEFAULT_THUMB

    html = resp.text

    # (2) ê¸°ì¡´ ë¡œì§ ìœ ì§€: canonical í•œ ë²ˆ ë” ì¶”ì (ì• ê·¸ë¦¬ê²Œì´í„° ì¼€ì´ìŠ¤)
    page_for_image, html_for_image = _maybe_follow_canonical(
        resolved_url, html, timeout=timeout, headers=headers
    )

    candidates = extract_image_candidates(page_for_image, html_for_image)

    for score, img_url in candidates:
        try:
            if not is_bad_image_url(img_url):
                return img_url
        except Exception:
            continue

    return DEFAULT_THUMB



# ë©”ì¸ + ì¶”ê°€ ê¸°ì‚¬ ëª¨ë‘ì— ëŒ€í•´ ì¸ë„¤ì¼ ì±„ìš°ê¸° (ë³‘ë ¬ ì²˜ë¦¬)
print("\nì¸ë„¤ì¼ ì¶”ì¶œ ì‹œì‘ (ë³‘ë ¬ ì²˜ë¦¬)...")

def fetch_thumbnail_for_article(article_data):
    """
    ë‹¨ì¼ ê¸°ì‚¬ì˜ ì¸ë„¤ì¼ì„ ì¶”ì¶œí•˜ëŠ” í•¨ìˆ˜
    """
    topic_num, art = article_data
    news_url = art.get("url", "")
    try:
        thumb = fetch_thumbnail(news_url)
        art["thumbnail_url"] = thumb or DEFAULT_THUMB
        return True
    except Exception as e:
        art["thumbnail_url"] = DEFAULT_THUMB
        return False

# ëª¨ë“  ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸ ìˆ˜ì§‘
all_articles = []
for topic_num in topic_main_articles:
    for art in topic_main_articles[topic_num]:
        all_articles.append((topic_num, art))

for topic_num in topic_extra_articles:
    for art in topic_extra_articles[topic_num]:
        all_articles.append((topic_num, art))

# ë³‘ë ¬ ì²˜ë¦¬ ì‹¤í–‰
total_articles = len(all_articles)
print(f"ì´ {total_articles}ê°œ ê¸°ì‚¬ì˜ ì¸ë„¤ì¼ì„ ì¶”ì¶œí•©ë‹ˆë‹¤ (ë™ì‹œ ì²˜ë¦¬: 15ê°œ)...")

with ThreadPoolExecutor(max_workers=20) as executor:
    futures = {executor.submit(fetch_thumbnail_for_article, art_data): idx
               for idx, art_data in enumerate(all_articles)}

    completed = 0
    success = 0
    for future in as_completed(futures):
        if future.result():
            success += 1
        completed += 1

        # ì§„í–‰ ìƒí™© ì¶œë ¥ (10ê°œë§ˆë‹¤)
        if completed % 10 == 0:
            print(f"  ì§„í–‰: {completed}/{total_articles} ({completed*100//total_articles}%) - ì„±ê³µ: {success}ê°œ")

print(f"\nì¸ë„¤ì¼ ì¶”ì¶œ ì™„ë£Œ! ì„±ê³µ: {success}/{total_articles}ê°œ")
print("(ë³¸ë¬¸ ì˜ì—­ ìœ„ì£¼ + sidebar/related ì œì™¸ + ìŠ¤ë§ˆíŠ¸ í•„í„° + canonical ì¶”ì )")


# 
# # **08 ì¹´ë“œ/ì„¹ì…˜ HTML + ìµœì¢… ë‰´ìŠ¤ë ˆí„° HTML ìƒì„±**

# In[67]:


# ============================
# 8. ì¹´ë“œ/ì„¹ì…˜ HTML + ë”ë³´ê¸° í˜ì´ì§€ + ìµœì¢… ë‰´ìŠ¤ë ˆí„° HTML
# ============================

# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ ì „ìš© í˜ì´ì§€ íŒŒì¼ëª…
# (ì›í•˜ëŠ” ì´ë¦„ìœ¼ë¡œ ë°”ê¿”ë„ ë˜ëŠ”ë°, ë©”ì¸ ë²„íŠ¼ ë§í¬ì™€ ê°™ì´ ì¨ì•¼ í•¨)
TOPIC_MORE_FILENAMES = {
    1: "more_geoint.html",        # GeoINT ì „ìš© ì¶”ê°€ ê¸°ì‚¬
    2: "more_aviation.html",      # í•­ê³µ/ë“œë¡  ì „ìš© ì¶”ê°€ ê¸°ì‚¬
    3: "more_ai_platform.html",   # AI ë°ì´í„° í”Œë«í¼ ì „ìš© ì¶”ê°€ ê¸°ì‚¬
    4: "more_satellite.html",     # ìœ„ì„± ì˜ìƒ ì „ìš© ì¶”ê°€ ê¸°ì‚¬
}

# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ í—¤ë” íƒ€ì´í‹€
TOPIC_MORE_TITLES = {
    1: "GeoINT - ì¶”ê°€ ê¸°ì‚¬",
    2: "í•­ê³µ / ë“œë¡ Â·ë¬´ì¸ê¸°(UAV/UAS) - ì¶”ê°€ ê¸°ì‚¬",
    3: "AI ë°ì´í„° í”Œë«í¼ - ì¶”ê°€ ê¸°ì‚¬",
    4: "ìœ„ì„± ì˜ìƒ - ì¶”ê°€ ê¸°ì‚¬",
}

# ì—°êµ¬ë™í–¥ more í˜ì´ì§€
RESEARCH_MORE_FILENAME = "more_research.html"


USE_KOREAN_FIRST = True  # Trueë©´ í•œê¸€ ì œëª© ìš°ì„ , Falseë©´ ì˜ë¬¸ ìš°ì„ 

# ============================
# ê³µí†µ ìœ í‹¸: ì €ë„/ì†ŒìŠ¤ ë¼ë²¨ ì •ë¦¬
# ============================
def format_journal_source_label(journal_name: str) -> str:
    """
    'Remote Sensing (MDPI)' -> 'MDPI - Remote Sensing'
    'Journal of Applied Remote Sensing (SPIE)' -> 'SPIE - Journal of Applied Remote Sensing'
    """
    if not journal_name:
        return ""

    journal_name = journal_name.strip()
    m = re.match(r"(.+?)\s*\((.+)\)", journal_name)
    if not m:
        return journal_name

    main = m.group(1).strip()
    site = m.group(2).strip()
    return f"{site} - {main}"

def make_card(article, use_korean_first=USE_KOREAN_FIRST):
    """
    ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œ (ì¸ë„¤ì¼ + ì œëª©/ë¶€ì œëª© + ë‚ ì§œ + ìš”ì•½)
    """
    thumb = article.get("thumbnail_url") or DEFAULT_THUMB

    ko = article.get("ko_title", "") or ""
    en = article.get("orig_title", "") or ""
    date = article.get("date", "") or ""
    summary = article.get("summary", "") or ""
    url = article.get("url", "") or ""

    # í•œ/ì˜ ì œëª© ì •ë¦¬
    if ko and en:
        main_title, sub_title = (ko, en) if use_korean_first else (en, ko)
    else:
        main_title = ko or en
        sub_title = ""

    sub_block = ""
    if sub_title:
        sub_block = f"""
            <div style="font-size:14px; color:#555; margin-bottom:4px; line-height:1.35;">
              {h(sub_title)}
            </div>"""

    return f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td style="padding:0; margin:0;">

      <table width="100%" cellpadding="0" cellspacing="0" border="0">

        <tr>

          <!-- ì¸ë„¤ì¼ -->
          <td valign="top" width="135" style="padding:0;">
            <a href="{h(url)}" target="_blank" style="text-decoration:none;">
              <img src="{h(thumb)}" width="120" height="120"
                   style="display:block; border-radius:14px; object-fit:cover;"
                   onerror="this.onerror=null; this.src='{DEFAULT_THUMB}';">
            </a>
          </td>

          <!-- ì œëª©/ë¶€ì œëª© -->
          <td valign="top" style="padding-left:6px;">

            <div style="font-size:18px; font-weight:700; color:#111;
                        margin-bottom:4px; line-height:1.35;">
              {h(main_title)}
            </div>

            {sub_block}

            <div style="font-size:12px; color:#888; margin-bottom:2px;">
              {h(date)}
            </div>

          </td>

        </tr>

        <!-- ìš”ì•½ -->
        <tr>
          <td colspan="2" style="padding-top:12px;">
            <div style="font-size:14px; color:#333; line-height:1.8;">
              {h(summary)}
            </div>
          </td>
        </tr>

        <!-- ì›ë¬¸ ë§í¬ -->
        <tr>
          <td colspan="2" style="padding-top:6px;">
            <a href="{h(url)}" target="_blank"
               style="font-size:13px; color:#0066cc; text-decoration:none;">
              ìì„¸íˆ ë³´ê¸° â†’
            </a>
          </td>
        </tr>

      </table>

    </td>
  </tr>
</table>
"""


def make_research_card_text_only(article):
    """
    ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ ì „ìš© í…ìŠ¤íŠ¸ ì¹´ë“œ
    - ì¸ë„¤ì¼ ì—†ìŒ
    - ì¹´ë“œ ì „ì²´ê°€ ì›ë¬¸ ë§í¬ë¡œ í´ë¦­ë˜ë„ë¡ êµ¬ì„±
    """
    title_en = article.get("title_en", "") or ""
    date = article.get("date", "") or ""
    summary = article.get("summary", "") or ""
    url = article.get("url", "") or ""

    journal_name = article.get("journal_name", "") or ""
    source_label = format_journal_source_label(journal_name)

    return f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:18px;">
  <tr>
    <!-- ì—¬ê¸° tdì—ëŠ” íŒ¨ë”©/ë§ˆì§„ë§Œ ë‘ê³ , ì‹¤ì œ ì¹´ë“œ ìŠ¤íƒ€ì¼ì€ ì•ˆìª½ divì— ì¤Œ -->
    <td style="padding:0; margin:0;">

      <!-- âœ… ì¹´ë“œ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë§í¬ -->
      <a href="{h(url)}" target="_blank"
         style="display:block; text-decoration:none; color:inherit;">

        <!-- ì‹¤ì œ ì¹´ë“œ ëª¨ì–‘(ë°°ê²½, í…Œë‘ë¦¬, íŒ¨ë”©)ì€ ì´ divì— ì ìš© -->
        <div style="background:#ffffff; border-radius:12px;
                    border:1px solid #e5e7eb;
                    padding:16px 18px; box-sizing:border-box;
                    box-shadow:0 10px 24px rgba(0,0,0,0.14);">

          <!-- ì˜ë¬¸ ì›ì œëª© (êµµê²Œ) -->
          <div style="font-size:17px; font-weight:700; color:#111827;
                      line-height:1.4; margin-bottom:4px;">
            {h(title_en)}
          </div>

          <!-- ì†Œ -->
          <div style="font-size:12px; font-weight:500; color:#374151; margin-bottom:2px;">
            {h(source_label)}
          </div>

          <!-- ë°œí–‰ì¼ -->
          <div style="font-size:12px; color:#6b7280; margin-bottom:8px;">
            {h(date)}
          </div>

          <!-- ìš”ì•½ (ë³¸ë¬¸/ìš”ì•½ ë‚´ìš©) -->
          <div style="font-size:14px; color:#374151; line-height:1.7;">
            {h(summary)}
          </div>

        </div>
      </a>
      <!-- ë§í¬ ë -->

    </td>
  </tr>
</table>
"""




def build_sections_html(topic_main_articles, topic_extra_articles):
    """
    ë©”ì¼ ë³¸ë¬¸ìš© ì„¹ì…˜:
    - ê° í† í”½ì˜ ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œë“¤ë§Œ ë³´ì—¬ì¤Œ
    - ê° í† í”½ ë§ˆì§€ë§‰ì— 'ì¶”ê°€ ê¸°ì‚¬' ê´€ë ¨ UI í‘œì‹œ
        â†’ extra ìˆìœ¼ë©´ : 'ì´ ì£¼ì œ ì¶”ê°€ ê¸°ì‚¬ ë³´ê¸° â†’'
        â†’ extra ì—†ìœ¼ë©´ : 'ì´ë²ˆ ì£¼ëŠ” ì¶”ê°€ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤'
    """
    html_parts = []

    for topic_num in [4, 3, 2, 1]:
        main_list = topic_main_articles.get(topic_num, [])
        extras = topic_extra_articles.get(topic_num, [])
        topic_title = f"{TOPIC_ICON[topic_num]} {TOPIC_DESC[topic_num]}"

        if not main_list:
            continue

        topic_inner = []

        # í† í”½ ì œëª©
        topic_inner.append(f"""
<div style="font-size:26px; font-weight:800; color:#111827;
            margin-bottom:24px; line-height:1.3;">
  {h(topic_title)}
</div>
""")

        # ë©”ì¸ ê¸°ì‚¬ ì¹´ë“œ
        for art in main_list:
            topic_inner.append(make_card(art))

        # â­ ì¶”ê°€ ê¸°ì‚¬ ë²„íŠ¼ / ë˜ëŠ” ì¶”ê°€ ê¸°ì‚¬ ì—†ìŒ ë©”ì‹œì§€
        if extras:
            # ì •ìƒ ë²„íŠ¼ ë²„ì „
            topic_inner.append(f"""
<div style="margin-top:12px; text-align:right;">
  <a href="{TOPIC_MORE_URLS[topic_num]}"
     style="display:inline-block; font-size:13px; font-weight:600;
            color:#111827; text-decoration:none;
            padding:6px 12px; border-radius:999px;
            border:1px solid #d1d5db; background:#ffffff;">
    ì´ ì£¼ì œ ì¶”ê°€ ê¸°ì‚¬ ë³´ê¸° â†’
  </a>
</div>
""")
        else:
            # â—ì¶”ê°€ ê¸°ì‚¬ ì—†ìŒ ë©”ì‹œì§€ ë²„íŠ¼
            topic_inner.append(f"""
<div style="margin-top:12px; text-align:right;">
  <div style="display:inline-block; font-size:13px; font-weight:600;
              color:#9ca3af;
              padding:6px 12px; border-radius:999px;
              border:1px solid #e5e7eb; background:#f9fafb;">
    ì´ë²ˆ ì£¼ëŠ” ì¶”ê°€ ë‰´ìŠ¤ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤
  </div>
</div>
""")

        topic_html = "".join(topic_inner)

        html_parts.append(f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:36px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#f9fafb;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;
                    box-shadow:0 10px 24px rgba(0,0,0,0.14);">
        <tr>
          <td>
            {topic_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
""")

    return "".join(html_parts)

def build_research_section_html(main_articles, extra_articles, more_url):
    """
    ë©”ì¸ ë‰´ìŠ¤ë ˆí„° ë³¸ë¬¸ì—ì„œ ì‚¬ìš©í•˜ëŠ” 'ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥' ì„¹ì…˜ HTML
    - ì¹´ë“œ UIëŠ” ê¸°ì¡´ ë‰´ìŠ¤ ì¹´ë“œì™€ ë™ì¼í•˜ê²Œ make_card() ì¬ì‚¬ìš©
    - 'ìµœì‹  ì—°êµ¬ ë” ë³´ê¸° â†’' ë²„íŠ¼ë§Œ í‘œì‹œ
    """
    if not main_articles:
        return ""

    inner = []

    # ì„¹ì…˜ ì œëª©
    inner.append(f"""
<div style="font-size:26px; font-weight:800; color:#111827;
            margin-bottom:24px; line-height:1.3;">
  {h(RESEARCH_SECTION_TITLE)}
</div>
""")

    # 1) ë©”ì¸ ì—°êµ¬ë™í–¥ ì¹´ë“œë“¤ â†’ 'í…ìŠ¤íŠ¸-only' ì—°êµ¬ ì¹´ë“œ ì‚¬ìš©
    for art in main_articles:
        converted = {
            "title_en": art.get("original_title", ""),
            "date": art.get("published_at", ""),
            "summary": art.get("summary_en", ""),
            "url": art.get("url", ""),
            "journal_name": art.get("journal_name", ""),  # âœ… ì¶”ê°€
        }


        inner.append(make_research_card_text_only(converted))



    # 2) 'ìµœì‹  ì—°êµ¬ ë” ë³´ê¸° â†’' ë²„íŠ¼ë§Œ í‘œì‹œ
    if extra_articles:
        inner.append(f"""
<div style="margin-top:12px; text-align:right;">
  <a href="{h(more_url)}"
     style="display:inline-block; font-size:13px; font-weight:600;
            color:#111827; text-decoration:none;
            padding:6px 12px; border-radius:999px;
            border:1px solid #d1d5db; background:#ffffff;">
    ìµœì‹  ì—°êµ¬ ë” ë³´ê¸° â†’
  </a>
</div>
""")

    # 3) ì „ì²´ ë°•ìŠ¤ ë ˆì´ì•„ì›ƒ ê°ì‹¸ê¸°
    body_html = "".join(inner)

    return f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:36px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#f9fafb;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            {body_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
"""


def build_more_page_html(topic_extra_articles, date_range, newsletter_date, header_title):
    """
    í† í”½ë³„ 'ì¶”ê°€ ê¸°ì‚¬' í˜ì´ì§€ HTML ìƒì„±
    - topic_extra_articles: {í† í”½ë²ˆí˜¸: [ê¸°ì‚¬ ë¦¬ìŠ¤íŠ¸]} í˜•íƒœ (ë³´í†µ 1ê°œ í† í”½ë§Œ ë“¤ì–´ì˜¤ê²Œ ì‚¬ìš©)
    - header_title: ìƒë‹¨ í—¤ë”ì— ë“¤ì–´ê°ˆ íƒ€ì´í‹€ (ì˜ˆ: "GeoINT - ì¶”ê°€ ê¸°ì‚¬")
    """
    sections = []

    for topic_num in [1, 2, 3, 4]:
        extras = topic_extra_articles.get(topic_num, [])
        if not extras:
            continue

        topic_title = f"{TOPIC_ICON[topic_num]} {TOPIC_DESC[topic_num]}"

        rows = []
        for idx, art in enumerate(extras):
            ko = art.get("ko_title", "") or ""
            en = art.get("orig_title", "") or ""
            url = art.get("url", "") or ""
            date = art.get("date", "") or ""
            thumb = art.get("thumbnail_url") or DEFAULT_THUMB

            main_title = ko or en
            sub_title = en if ko else ""

            sub_block = ""
            if sub_title:
                sub_block = f"""
        <div style="font-size:12px; color:#6b7280; margin-top:2px; line-height:1.4;">
          {h(sub_title)}
        </div>"""

            # âœ… í…Œì´ë¸” êµ¬ì¡°ë¥¼ ì˜¬ë°”ë¥´ê²Œ: <tr> ì•ˆì— ë‹¤ì‹œ <tr> ë„£ì§€ ì•Šê¸°
            rows.append(f"""
              <tr class="topic-article-row" data-index="{idx}">
                <td style="padding:10px 0; border-bottom:1px solid #e5e7eb;">
                  <table width="100%" cellpadding="0" cellspacing="0" border="0">
                    <tr>
                      <!-- ì¸ë„¤ì¼ -->
                      <td valign="top" width="96" style="padding-right:8px;">
                        <a href="{h(url)}" target="_blank" style="text-decoration:none;">
                          <img src="{h(thumb)}" width="80" height="80"
                               style="display:block; border-radius:10px; object-fit:cover;"
                               onerror="this.onerror=null; this.src='{DEFAULT_THUMB}';">
                        </a>
                      </td>

                      <!-- ì œëª©/ë¶€ì œëª©/ë‚ ì§œ -->
                      <td valign="top">
                        <div style="font-size:14px; line-height:1.5; margin-bottom:2px;">
                          <a href="{h(url)}" target="_blank"
                             style="color:#111827; text-decoration:none;">
                            {h(main_title)}
                          </a>
                        </div>
                        {sub_block}
                        <div style="font-size:11px; color:#9ca3af; margin-top:2px;">
                          {h(date)}
                        </div>
                      </td>
                    </tr>
                  </table>
                </td>
              </tr>
            """)


        table_html = f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0">
  {''.join(rows)}
</table>
"""

        # Gmail ìŠ¤íƒ€ì¼ í˜ì´ì§€ ì •ë³´ + ì¢Œìš° ë²„íŠ¼ ì˜ì—­
        pager_html = f"""
<div id="topic-pagination"
     style="display:flex;
            justify-content:flex-end;
            align-items:center;
            font-size:14px;
            font-weight:500;
            color:#374151;
            margin-top:-10px;
            margin-bottom:10px;">

  <span id="topic-page-info"
        style="margin-right:14px;"></span>

  <!-- ì´ì „ ë²„íŠ¼ -->
  <a href="#" id="topic-prev"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       margin-right:6px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x2039;
    </span>
  </a>

  <!-- ë‹¤ìŒ ë²„íŠ¼ -->
  <a href="#" id="topic-next"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x203A;
    </span>
  </a>
</div>
"""

        sections.append(f"""
<!-- í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ ë°•ìŠ¤ -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#ffffff;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            <div style="font-size:24px; font-weight:700; color:#111827;
                        margin-bottom:28px;">
              {h(topic_title)}
            </div>
            {pager_html}
            {table_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
""")

    if not sections:
        body_inner = """
<p style="font-size:14px; color:#4b5563;">
  ì´ë²ˆ ì£¼ëŠ” ë³„ë„ì˜ ì¶”ê°€ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤.
</p>
"""
    else:
        body_inner = "".join(sections)

    more_html = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>{header_title}</title>

<style>
  /* ëª¨ë°”ì¼ ìµœì í™” */
  @media (max-width: 768px) {{
    .hero-bg {{
      background-position:center 0 !important;
    }}
    .hero-header-cell {{
      padding:32px 0 28px 0 !important;
    }}
  }}

  /* í‘¸í„°ë¥¼ í•­ìƒ ì•„ë˜ë¡œ ë°€ì–´ì£¼ëŠ” ë ˆì´ì•„ì›ƒ */
  html, body {{
    height: 100%;
    margin: 0;
  }}

  body {{
    display: flex;
    flex-direction: column;
    min-height: 100vh;
    background:#f3f4f6;
    font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;
  }}


  .content-wrapper {{
    flex: 1;
  }}
</style>

</head>

<body>

  <!-- í—¤ë” (ë©”ì¼ê³¼ ìœ ì‚¬í•œ ìŠ¤íƒ€ì¼) -->
  <table class="hero-bg" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background-image:url('{HEADER_BACKGROUND}');
              background-size:cover;
              background-position:center -60px;
              background-repeat:no-repeat;">
    <tr>
      <td align="center" class="hero-header-cell"
          bgcolor="#000000"
          style="padding:0 24px 14px 24px;
                 background: linear-gradient(to bottom right,
                             rgba(0,0,40,0.55),
                             rgba(0,0,0,0.55));
                 color:#ffffff; ;">

        <table cellpadding="0" cellspacing="0" border="0"
               style="max-width:{CONTENT_WIDTH}px; width:100%;
                      color:#ffffff; margin:0 auto;">

          <tr>
            <td style="padding:16px 24px 8px 24px;">
              <table width="100%">
                <tr>
                  <td align="left">
                    <img src="{LOGO_URL}" style="max-width:110px; display:block;">
                  </td>
                  <td align="right"
                      style="text-transform:uppercase; font-size:13px;
                             color:#ffffff; ;">
                    WWW.INSPACE.CO.KR
                  </td>
                </tr>
              </table>
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0px 24px 12px 24px;
                       font-size:28px; font-weight:600;
                       color:#ffffff; ;">
              {h(header_title)}
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0 24px 48px 24px;
                       font-size:14px; font-weight:300; opacity:0.9; line-height:1.5;
                       color:#ffffff; ;">
              {date_range}<br>
              {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°
            </td>
          </tr>

        </table>

      </td>
    </tr>
  </table>


  <div class="content-wrapper">
  <!-- ë³¸ë¬¸ -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0" style="padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td style="padding:0 16px 0 16px;">

              <!-- ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼ (ì¹´ë“œ ìœ„ ì™¼ìª½ ì •ë ¬) -->
              <div style="padding-bottom:20px; text-align:left;">
                <a href="{MAIN_PAGE_URL}"
                  style="display:inline-block;
                          font-size:13px;
                          padding:6px 12px;
                          border-radius:999px;
                          background:#111827;
                          color:#ffffff;
                          text-decoration:none;">
                  â† ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸°
                </a>
              </div>

              {body_inner}

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
  </div> <!-- content-wrapper ë -->

  <!-- í‘¸í„° -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0"
        style="background:#1e293b; padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td class="inner-padding"
                style="padding:12px 16px;
                      font-size:12px;
                      color:#e2e8f0;
                      text-align:center;
                      line-height:1.6;">

              ë³¸ ë©”ì¼ì€ í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ì‚¬ë‚´ êµ¬ì„±ì›ì„ ìœ„í•œ ì£¼ê°„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì…ë‹ˆë‹¤.<br>
              ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ì „ì¬ ë° ê³µìœ ëŠ” ì§€ì–‘í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br><br>
              &copy; {now_kst.year} Hancom InSpace. All Rights Reserved.

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>

  <script>
  (function() {{
    // âœ… í† í”½ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ë„¤ì´ì…˜ (GeoINT/ë“œë¡ /AI/ìœ„ì„± ê³µí†µ)
    var PAGE_SIZE = 5;  // í•œ í˜ì´ì§€ì— 5ê°œì”© ë³´ì—¬ì¤Œ (10ê°œë¡œ ë°”ê¾¸ê³  ì‹¶ìœ¼ë©´ ì—¬ê¸° ìˆ«ìë§Œ 10ìœ¼ë¡œ ë³€ê²½)

    var rows = Array.prototype.slice.call(
      document.querySelectorAll('.topic-article-row')
    );
    var total = rows.length;
    var totalPages = Math.max(1, Math.ceil(total / PAGE_SIZE));
    var currentPage = 1;

    var infoEl = document.getElementById('topic-page-info');
    var prevEl = document.getElementById('topic-prev');
    var nextEl = document.getElementById('topic-next');

    function render(page) {{
      if (page < 1 || page > totalPages) return;
      currentPage = page;

      var start = (currentPage - 1) * PAGE_SIZE;
      var end = start + PAGE_SIZE;

      rows.forEach(function(row, idx) {{
        row.style.display = (idx >= start && idx < end) ? '' : 'none';
      }});

      if (infoEl) {{
        var from = total === 0 ? 0 : start + 1;
        var to = Math.min(end, total);
        infoEl.textContent = from + 'â€“' + to + ' of ' + total;
      }}

      if (prevEl) {{
        if (currentPage === 1) {{
          prevEl.style.opacity = '0.3';
          prevEl.style.pointerEvents = 'none';
        }} else {{
          prevEl.style.opacity = '1';
          prevEl.style.pointerEvents = 'auto';
        }}
      }}

      if (nextEl) {{
        if (currentPage === totalPages) {{
          nextEl.style.opacity = '0.3';
          nextEl.style.pointerEvents = 'none';
        }} else {{
          nextEl.style.opacity = '1';
          nextEl.style.pointerEvents = 'auto';
        }}
      }}
    }}

    if (prevEl) prevEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage - 1);
    }});

    if (nextEl) nextEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage + 1);
    }});

    render(1);
  }})();
  </script>


</body>
</html>
"""
    return more_html


def build_research_more_page_html(extra_articles, date_range, newsletter_date):
    """
    ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€ í˜ì´ì§€ HTML
    - ë ˆì´ì•„ì›ƒ: build_more_page_html(GeoINT ë“±)ê³¼ ë™ì¼
      * ê°™ì€ í—¤ë”(íˆì–´ë¡œ ì˜ì—­)
      * ê°™ì€ 'ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸°' ë²„íŠ¼
      * ê°™ì€ í‘¸í„°
      * ë³¸ë¬¸ì€ í°ìƒ‰ ì¹´ë“œ ë°•ìŠ¤ ì•ˆì— 'ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥' ì œëª© + ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸
    """
    # 1) ë³¸ë¬¸(ë…¼ë¬¸ ë¦¬ìŠ¤íŠ¸) HTML ë§Œë“¤ê¸°
    if not extra_articles:
        # ì¶”ê°€ í•™ìˆ ì§€ê°€ ì—†ì„ ë•Œ: ì¹´ë“œ ì•ˆì— ì•ˆë‚´ ë¬¸êµ¬ë§Œ
        topic_title = "ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€"
        body_inner = f"""
<!-- ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ ë°•ìŠ¤ (ë°ì´í„° ì—†ìŒ) -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#ffffff;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            <div style="font-size:24px; font-weight:700; color:#111827;
                        margin-bottom:16px;">
              {h(topic_title)}
            </div>
            <p style="font-size:14px; color:#4b5563; margin:0;">
              ì´ë²ˆ ì£¼ì—ëŠ” ì¶”ê°€ í•™ìˆ ì§€ê°€ ì—†ìŠµë‹ˆë‹¤.
            </p>
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
"""


    else:
        # ë…¼ë¬¸ë“¤ ë¦¬ìŠ¤íŠ¸ë¥¼ rowsë¡œ ìŒ“ê¸°
        rows = []
        for idx, art in enumerate(extra_articles):
            title_en = art.get("original_title", "") or ""
            date = art.get("published_at", "") or ""
            url = art.get("url", "") or ""
            journal_name = art.get("journal_name", "") or ""

            # 'Remote Sensing (MDPI)' -> 'MDPI - Remote Sensing' ë³€í™˜
            source_label = format_journal_source_label(journal_name)

            rows.append(f"""
              <tr class="research-article-row" data-index="{idx}">
                <td style="padding:8px 0; border-bottom:1px solid #e5e7eb;">
                  <!-- ì˜ë¬¸ ì›ì œëª© -->
                  <div style="font-size:14px; font-weight:600;
                              line-height:1.5; margin-bottom:4px;">
                    <a href="{h(url)}" target="_blank"
                      style="color:#111827; text-decoration:none;">
                      {h(title_en)}
                    </a>
                  </div>

                  <!-- ë‚ ì§œ + ì†ŒìŠ¤ ì›¹ì‚¬ì´íŠ¸/ì €ë„ í‘œì‹œ -->
                  <div style="font-size:12px; font-weight:500; color:#374151; margin-bottom:2px;">
                    {h(source_label)}
                  </div>

                  <!-- ë‚ ì§œ -->
                  <div style="font-size:12px; color:#9ca3af; margin-bottom:4px;">
                    {h(date)}
                  </div>
                </td>
              </tr>
            """)



        table_html = f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0">
  {''.join(rows)}
</table>
"""

        # Gmail ìŠ¤íƒ€ì¼ í˜ì´ì§€ ì •ë³´ + ì¢Œìš° ë²„íŠ¼ ì˜ì—­
        pager_html = f"""
<div id="research-pagination"
     style="display:flex;
            justify-content:flex-end;
            align-items:center;
            font-size:14px;
            font-weight:500;
            color:#374151;
            margin-top:-10px;
            margin-bottom:10px;">

  <span id="research-page-info"
        style="margin-right:14px;"></span>

  <!-- ì´ì „ ë²„íŠ¼ -->
  <a href="#" id="research-prev"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       margin-right:6px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x2039;
    </span>
  </a>

  <!-- ë‹¤ìŒ ë²„íŠ¼ -->
  <a href="#" id="research-next"
     style="
       display:inline-flex;
       align-items:center;
       justify-content:center;
       width:32px;
       height:32px;
       border:1px solid #d1d5db;
       border-radius:8px;
       text-decoration:none;
       background-color:#ffffff;
       cursor:pointer;
     ">
    <span style="
       display:inline-block;
       font-size:18px;
       font-weight:600;
       color:#374151;
       transform: translateY(-3px);
    ">
      &#x203A;
    </span>
  </a>
</div>
"""



        topic_title = "ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥"

        # GeoINTìš© more í˜ì´ì§€ì™€ ë™ì¼í•œ ì¹´ë“œ ë°•ìŠ¤ êµ¬ì¡°ë¡œ ê°ì‹¸ê¸°
        body_inner = f"""
<!-- ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ ë°•ìŠ¤ -->
<table width="100%" cellpadding="0" cellspacing="0" border="0"
       style="margin-bottom:28px;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="width:100%;
                    max-width:{CONTENT_WIDTH}px;
                    background:#ffffff;
                    border:1px solid #e5e7eb;
                    border-radius:12px;
                    padding:20px;
                    box-sizing:border-box;">
        <tr>
          <td>
            <div style="font-size:24px; font-weight:700; color:#111827;
                        margin-bottom:28px;">
              {h(topic_title)}
            </div>
              {pager_html}
              {table_html}
          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>
"""

    # ìƒë‹¨ íˆì–´ë¡œ í—¤ë”ì— ë“¤ì–´ê°ˆ íƒ€ì´í‹€ (íƒ­ ì œëª©ë„ ê°™ì´ ì‚¬ìš©)
    header_title = "ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€"

    more_html = f"""
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">



<title>{header_title}</title>

<style>
  /* ëª¨ë°”ì¼ ìµœì í™” */
  @media (max-width: 768px) {{
    .hero-bg {{
      background-position:center 0 !important;
    }}
    .hero-header-cell {{
      padding:32px 0 28px 0 !important;
    }}
  }}

  /* í‘¸í„°ë¥¼ í•­ìƒ ì•„ë˜ë¡œ ë°€ì–´ì£¼ëŠ” ë ˆì´ì•„ì›ƒ */
  html, body {{
    height: 100%;
    margin: 0;
  }}

  body {{
    display: flex;
    flex-direction: column;
    min-height: 100vh;
    background:#f3f4f6;
    font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;
  }}

  .content-wrapper {{
    flex: 1;
  }}
</style>

</head>

<body>

  <!-- í—¤ë” (more_geointì™€ ë™ì¼ êµ¬ì¡°) -->
  <table class="hero-bg" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background-image:url('{HEADER_BACKGROUND}');
              background-size:cover;
              background-position:center -60px;
              background-repeat:no-repeat;">
    <tr>
      <td align="center" class="hero-header-cell"
          bgcolor="#000000"
          style="padding:0 24px 14px 24px;
                 background: linear-gradient(to bottom right,
                             rgba(0,0,40,0.55),
                             rgba(0,0,0,0.55));
                 color:#ffffff; ;">

        <table cellpadding="0" cellspacing="0" border="0"
               style="max-width:{CONTENT_WIDTH}px; width:100%;
                      color:#ffffff; margin:0 auto;">

          <tr>
            <td style="padding:16px 24px 8px 24px;">
              <table width="100%">
                <tr>
                  <td align="left">
                    <img src="{LOGO_URL}" style="max-width:110px; display:block;">
                  </td>
                  <td align="right"
                      style="text-transform:uppercase; font-size:13px;
                             color:#ffffff; ;">
                    WWW.INSPACE.CO.KR
                  </td>
                </tr>
              </table>
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0px 24px 12px 24px;
                       font-size:28px; font-weight:600;
                       color:#ffffff; ;">
              {h(header_title)}
            </td>
          </tr>

          <tr>
            <td align="center"
                style="padding:0 24px 48px 24px;
                       font-size:14px; font-weight:300; opacity:0.9; line-height:1.5;
                       color:#ffffff; ;">
              {date_range}<br>
              {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°
            </td>
          </tr>

        </table>

      </td>
    </tr>
  </table>


  <div class="content-wrapper">
  <!-- ë³¸ë¬¸ -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0" style="padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td style="padding:0 16px 0 16px;">

              <!-- ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸° ë²„íŠ¼ -->
              <div style="padding-bottom:20px; text-align:left;">
                <a href="{MAIN_PAGE_URL}"
                  style="display:inline-block;
                          font-size:13px;
                          padding:6px 12px;
                          border-radius:999px;
                          background:#111827;
                          color:#ffffff;
                          text-decoration:none;">
                  â† ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ë¡œ ëŒì•„ê°€ê¸°
                </a>
              </div>

              {body_inner}

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>
  </div> <!-- content-wrapper ë -->


  <!-- í‘¸í„° (more_geointì™€ ë™ì¼) -->
  <table width="100%" cellpadding="0" cellspacing="0" border="0"
        style="background:#1e293b; padding:24px 0 32px 0;">
    <tr>
      <td align="center">
        <table cellpadding="0" cellspacing="0" border="0"
              style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
          <tr>
            <td class="inner-padding"
                style="padding:12px 16px;
                      font-size:12px;
                      color:#e2e8f0;
                      text-align:center;
                      line-height:1.6;">

              ë³¸ ë©”ì¼ì€ í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ì‚¬ë‚´ êµ¬ì„±ì›ì„ ìœ„í•œ ì£¼ê°„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì…ë‹ˆë‹¤.<br>
              ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ì „ì¬ ë° ê³µìœ ëŠ” ì§€ì–‘í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br><br>
              &copy; {now_kst.year} Hancom InSpace. All Rights Reserved.

            </td>
          </tr>
        </table>
      </td>
    </tr>
  </table>


  <script>
  (function() {{
    var PAGE_SIZE = 5; // âœ… 5ê°œì”© ë³´ì—¬ì£¼ê¸°

    var rows = Array.prototype.slice.call(
      document.querySelectorAll('.research-article-row')
    );
    var total = rows.length;
    var totalPages = Math.max(1, Math.ceil(total / PAGE_SIZE));
    var currentPage = 1;

    var infoEl = document.getElementById('research-page-info');
    var prevEl = document.getElementById('research-prev');
    var nextEl = document.getElementById('research-next');

    function render(page) {{
      if (page < 1 || page > totalPages) return;
      currentPage = page;

      var start = (currentPage - 1) * PAGE_SIZE;
      var end = start + PAGE_SIZE;

      rows.forEach(function(row, idx) {{
        row.style.display = (idx >= start && idx < end) ? '' : 'none';
      }});

      if (infoEl) {{
        var from = total === 0 ? 0 : start + 1;
        var to = Math.min(end, total);
        infoEl.textContent = from + 'â€“' + to + ' of ' + total;
      }}

      if (prevEl) {{
        if (currentPage === 1) {{
          prevEl.style.opacity = '0.3';
          prevEl.style.pointerEvents = 'none';
        }} else {{
          prevEl.style.opacity = '1';
          prevEl.style.pointerEvents = 'auto';
        }}
      }}

      if (nextEl) {{
        if (currentPage === totalPages) {{
          nextEl.style.opacity = '0.3';
          nextEl.style.pointerEvents = 'none';
        }} else {{
          nextEl.style.opacity = '1';
          nextEl.style.pointerEvents = 'auto';
        }}
      }}

    }}

    if (prevEl) prevEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage - 1);
    }});

    if (nextEl) nextEl.addEventListener('click', function(e) {{
      e.preventDefault();
      render(currentPage + 1);
    }});

    render(1);
  }})();
  </script>


</body>
</html>
"""
    return more_html


# ============================================================
# Weekly Focus Insight (ì£¼ê°„ í¬ì»¤ìŠ¤ ì¸ì‚¬ì´íŠ¸)
# - ì…ë ¥: ì£¼ì œë³„ ë‰´ìŠ¤(ìš°ì„ ìˆœìœ„ ìƒìœ„ 10ê°œ) + ì—°êµ¬ë™í–¥(ìƒìœ„ 10ê°œ)
# - ì¶œë ¥: 5~8ì¤„ í•œêµ­ì–´ ì¡°ì–¸(ë¬¸ì¥í˜•)
# ============================================================

WEEKLY_FOCUS_TITLE = "ğŸ” Weekly Focus Insight"
MAX_INSIGHT_ITEMS_PER_TOPIC = 10
MAX_INSIGHT_ITEMS_RESEARCH = 10

def _take_top_n(items, n):
    return (items or [])[:n]


def generate_weekly_focus_insight(
    topic_main_articles,
    topic_extra_articles,
    research_main_articles,
    research_extra_articles,
    top_k_per_topic=10
):
    """
    Weekly Focus Insight:
    - í† í”½(1~4) ë‰´ìŠ¤ + ìµœì‹  ì—°êµ¬ë™í–¥ ìš”ì•½(ìƒìœ„ í•­ëª©ë“¤)ì„ ì½ê³ 
      í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ì—ê²Œ 3~5ì¤„ í•œêµ­ì–´ ì¡°ì–¸ì„ ìƒì„±.
    """

    def _pick_top_k(article_list, k):
        items = list(article_list or [])
        # priorityê°€ ë†’ì€ ê²ƒì´ ë¨¼ì € ì˜¤ë„ë¡ (Noneì€ 0 ì·¨ê¸‰)
        items.sort(key=lambda x: (x.get("priority") or 0), reverse=True)
        return items[:k]


    def _news_payload(a: dict):
        # âœ… (ì¤‘ìš”) ê¸°ì¡´ íŒŒì´í”„ë¼ì¸ í‚¤(ko_title/summary) + ê³¼ê±° í‚¤(title_ko/summary_ko) ëª¨ë‘ í˜¸í™˜
        return {
            "title_ko": (a.get("ko_title") or a.get("title_ko") or "").strip(),
            "original_title": (a.get("orig_title") or a.get("original_title") or "").strip(),
            "summary_ko": (a.get("summary") or a.get("summary_ko") or "").strip(),
            "source": (a.get("source") or "").strip(),
            "date": (a.get("date") or "").strip(),
            "url": (a.get("url") or "").strip(),
            "priority": a.get("priority", None),
        }

    def _research_payload(a: dict):
        return {
            "title_ko": (a.get("title_ko") or a.get("ko_title") or "").strip(),
            "original_title": (a.get("original_title") or a.get("orig_title") or "").strip(),
            "summary_ko": (a.get("summary_ko") or a.get("summary") or "").strip(),
            "journal": (a.get("journal_name") or a.get("journal") or "").strip(),
            "published_at": (a.get("published_at") or a.get("date") or "").strip(),
            "url": (a.get("url") or "").strip(),
            "priority": a.get("priority", None),
        }

    ctx = {
        "topics": {},
        "research": {
            "main": [],
            "extra": []
        }
    }

    for topic_num in [1, 2, 3, 4]:
        main_top = _pick_top_k(topic_main_articles.get(topic_num, []), top_k_per_topic)
        extra_top = _pick_top_k(topic_extra_articles.get(topic_num, []), top_k_per_topic)

        ctx["topics"][str(topic_num)] = {
            "main": [_news_payload(a) for a in main_top],
            "extra": [_news_payload(a) for a in extra_top],
        }

    research_main_top = _pick_top_k(research_main_articles or [], top_k_per_topic)
    research_extra_top = _pick_top_k(research_extra_articles or [], top_k_per_topic)

    ctx["research"]["main"] = [_research_payload(a) for a in research_main_top]
    ctx["research"]["extra"] = [_research_payload(a) for a in research_extra_top]

    system = (
        "ë„ˆëŠ” ê¸€ë¡œë²Œ ì‚°ì—…Â·ê¸°ìˆ  ë³€í™”ë¥¼ êµ¬ì¡°ì ìœ¼ë¡œ í•´ì„í•˜ëŠ” ì‹œë‹ˆì–´ ë¦¬ì„œì¹˜ ì• ë„ë¦¬ìŠ¤íŠ¸ì…ë‹ˆë‹¤. "
        "ì…ë ¥ìœ¼ë¡œ í•´ë‹¹ ì£¼ì˜ ë‰´ìŠ¤ì™€ ì—°êµ¬ë™í–¥ ìš”ì•½ì´ ì£¼ì–´ì§‘ë‹ˆë‹¤. "
        "ì¶œë ¥ì€ ë©”ì¸ ë‰´ìŠ¤ë ˆí„°ì— ë“¤ì–´ê°ˆ 'Weekly Focus Insight'ì…ë‹ˆë‹¤. "

        "ì´ ì¸ì‚¬ì´íŠ¸ëŠ” ë‹¨ìˆœ ìš”ì•½ì´ë‚˜ í˜„ìƒ ë‚˜ì—´ì´ ì•„ë‹ˆë¼, "
        "ì—¬ëŸ¬ ì‚¬ê±´ê³¼ ê¸°ìˆ  ë³€í™”ë¥¼ í•˜ë‚˜ì˜ ê´€ì ìœ¼ë¡œ ë¬¶ì–´ í•´ì„í•˜ëŠ” ë¶„ì„ ë¬¸ë‹¨ì´ì–´ì•¼ í•©ë‹ˆë‹¤. "

        "ë‹¤ìŒ ê¸°ì¤€ì„ ë°˜ë“œì‹œ ì§€í‚¤ì„¸ìš”: "
        "1) ê¸€ ì „ì²´ëŠ” í•˜ë‚˜ì˜ ì¤‘ì‹¬ ë…¼ì§€(throughline)ë¥¼ ê°€ì ¸ì•¼ í•©ë‹ˆë‹¤. "
        "   (ì˜ˆ: 'ì§€ë¦¬ê³µê°„ ì •ë³´ê°€ ê¸°ìˆ ì„ ë„˜ì–´ ì „ëµ ìì‚°ìœ¼ë¡œ ì „í™˜ë˜ê³  ìˆë‹¤') "
        "2) ê° ë¬¸ì¥ì€ ì• ë¬¸ì¥ì˜ ì›ì¸Â·ê²°ê³¼Â·ì˜ë¯¸ í™•ì¥ ê´€ê³„ë¡œ ì—°ê²°ë˜ì–´ì•¼ í•˜ë©°, "
        "   ë…ë¦½ì ì¸ ìš”ì•½ ë¬¸ì¥ì„ ë‚˜ì—´í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "3) 'Aë„ ì¼ì–´ë‚˜ê³  Bë„ ì¼ì–´ë‚˜ê³  Cë„ ì¼ì–´ë‚œë‹¤' ì‹ì˜ ë³‘ë ¬ ë‚˜ì—´ì€ ê¸ˆì§€í•©ë‹ˆë‹¤. "
        "4) ìµœì†Œ í•œ ë²ˆ ì´ìƒ 'ì´ë¡œ ì¸í•´ / ê·¸ ê²°ê³¼ / ì´ì— ë”°ë¼ / ì´ëŸ¬í•œ íë¦„ì€'ê³¼ ê°™ì€ "
        "   ì¸ê³¼ ë˜ëŠ” êµ¬ì¡°ì  ì—°ê²° í‘œí˜„ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. "
        "5) ë³€í™”ì˜ ì˜ë¯¸ê°€ ë“œëŸ¬ë‚˜ë„ë¡ "
        "   'ê¸°ìˆ  â†’ ì „ëµ', 'ë„êµ¬ â†’ ì¸í”„ë¼', 'ìš´ì˜ â†’ ê±°ë²„ë„ŒìŠ¤'ì™€ ê°™ì€ ì „í™˜ ê´€ì ì„ í¬í•¨í•©ë‹ˆë‹¤. "
        "6) íŠ¹ì • ê¸°ì—…ì´ë‚˜ ì¡°ì§(í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ë“±)ì„ ì§ì ‘ ì§€ì¹­í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "
        "7) ì¡´ëŒ“ë§ ì„œìˆ í˜•ìœ¼ë¡œ 5~8ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤. ê° ë¬¸ì¥ì€ ì¤„ë°”ê¿ˆìœ¼ë¡œ ë¶„ë¦¬í•´ í•œ ì¤„ì— í•œ ë¬¸ì¥ë§Œ ì”ë‹ˆë‹¤. "
        "   (ë¶ˆë¦¿/ë²ˆí˜¸/ë¼ë²¨/ì½œë¡  ì‚¬ìš© ê¸ˆì§€, ë¹ˆ ì¤„ ê¸ˆì§€) "
        "8) ë¬¸ì¥ ì—­í• (ìˆœì„œ)ì„ ë‹¤ìŒ íë¦„ìœ¼ë¡œ êµ¬ì„±í•©ë‹ˆë‹¤: "
        "   (1) ì´ë²ˆ ì£¼ í•µì‹¬ ë³€í™”(í˜„ìƒ/ê²°ë¡ ) "
        "â†’ (2) êµ¬ì¡°ì  ì›ì¸ ë˜ëŠ” ë©”ì»¤ë‹ˆì¦˜(ì™œ ì§€ê¸ˆ ì´ëŸ° ë³€í™”ê°€ ë‚˜íƒ€ë‚˜ëŠ”ì§€) "
        "â†’ (3) ê·¼ê±° ì•µì»¤ 1(ì…ë ¥ì˜ ì„œë¡œ ë‹¤ë¥¸ í•­ëª© ì¤‘ 1ê°œë¥¼ í™œìš©í•´ ë©”ì»¤ë‹ˆì¦˜ì„ ë’·ë°›ì¹¨) "
        "â†’ (4) ê·¼ê±° ì•µì»¤ 2(ë‹¤ë¥¸ í•­ëª© 1ê°œë¥¼ ì¶”ê°€ë¡œ ì—°ê²°í•´ ê´€ì ì„ ê°•í™”) "
        "â†’ (5) ì „í™˜ì˜ ì˜ë¯¸(ì˜ˆ: ê¸°ìˆ â†’ì „ëµ, ë„êµ¬â†’ì¸í”„ë¼ ë“±) "
        "â†’ (6) ì‹¤ë¬´ì  ì¡°ì–¸/í–‰ë™(ì´ë²ˆ ì£¼ ë…ìê°€ ë¬´ì—‡ì„ ì ê²€Â·ì¤€ë¹„Â·ì‹¤í—˜í•´ì•¼ í•˜ëŠ”ì§€) "
        "â†’ (ì„ íƒ) (7) ë¦¬ìŠ¤í¬/í•œê³„ "
        "â†’ (ì„ íƒ) (8) ë‹¤ìŒ ì£¼ ê´€ì¸¡ í¬ì¸íŠ¸. "
        "9) ì…ë ¥ì— í¬í•¨ëœ ì„œë¡œ ë‹¤ë¥¸ í•­ëª©(ê¸°ì‚¬/ì—°êµ¬) ìµœì†Œ 2ê°œë¥¼ ê·¼ê±° ì•µì»¤ë¡œ ì‚¼ë˜, "
        "   'ìš”ì•½'ì´ ì•„ë‹ˆë¼ 'ë©”ì»¤ë‹ˆì¦˜'ì´ ë“œëŸ¬ë‚˜ë„ë¡ ë¬¸ì¥ ì†ì— ë…¹ì—¬ ì”ë‹ˆë‹¤. "
        "10) ì…ë ¥ì— ì—†ëŠ” ì‚¬ê±´Â·ê¸°ìˆ ì„ ìƒˆë¡œ ë‹¨ì •í•´ ì¶”ê°€í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. "

        "ê³¼ì¥ë˜ê±°ë‚˜ ë‹¨ì •ì ì¸ ì˜ˆì¸¡ì€ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤."
    )





    def _trim(s, n=220):
        s = (s or "").replace("\n", " ").strip()
        return s if len(s) <= n else s[:n-1].rstrip() + "â€¦"

    lines = []
    for topic_num in [1, 2, 3, 4]:
        lines.append(f"[Topic {topic_num}]")
        items = ctx["topics"][str(topic_num)]["main"] + ctx["topics"][str(topic_num)]["extra"]
        # ì•ˆì „í•˜ê²Œ í•œ ë²ˆ ë” priority ì •ë ¬(ìˆìœ¼ë©´)
        items = sorted(items, key=lambda x: (x.get("priority") or 0), reverse=True)
        for a in items[:top_k_per_topic]:
            title = a.get("title_ko") or a.get("original_title")
            summ = a.get("summary_ko")
            if title and summ:
                lines.append(f"- { _trim(title, 120) } :: { _trim(summ, 240) }")

    lines.append("[Research]")
    r_items = ctx["research"]["main"] + ctx["research"]["extra"]
    r_items = sorted(r_items, key=lambda x: (x.get("priority") or 0), reverse=True)
    for r in r_items[:top_k_per_topic]:
        title = r.get("title_ko") or r.get("original_title")
        summ = r.get("summary_ko")
        if title and summ:
            lines.append(f"- { _trim(title, 120) } :: { _trim(summ, 240) }")

    user = "\n".join(lines)


    try:
        resp = client.responses.create(
            model=MODEL_NAME,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = (resp.output[0].content[0].text or "").strip()
        lines = [ln.strip() for ln in text.splitlines() if ln.strip()]
        return "\n".join(lines[:8]) if len(lines) > 8 else "\n".join(lines)
    except Exception as e:
        print(f"[WARN] Weekly Focus Insight ìƒì„± ì‹¤íŒ¨: {e}")
        return ""

def to_paragraph_html_auto(text: str, target_paragraphs: int = 3) -> str:
    text = (text or "").strip()
    if not text:
        return "<p style='margin:0;'>ì´ë²ˆ ì£¼ í¬ì»¤ìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.</p>"

    # 1) ì´ë¯¸ ë¹ˆ ì¤„(ë¬¸ë‹¨ êµ¬ë¶„)ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ìš°ì„  ì‚¬ìš©
    if "\n\n" in text:
        parts = [p.strip() for p in text.split("\n\n") if p.strip()]
    else:
        # 2) ë¹ˆ ì¤„ì´ ì—†ìœ¼ë©´ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ëŒ€ì¶© ìª¼ê°œì„œ 2~3ë¬¸ë‹¨ìœ¼ë¡œ ë¬¶ê¸°
        # - ë§ˆì¹¨í‘œ/ë¬¼ìŒí‘œ/ëŠë‚Œí‘œ ë’¤ ê³µë°± ê¸°ì¤€
        sents = [s.strip() for s in re.split(r"(?<=[\.!?])\s+", text) if s.strip()]

        # ë¬¸ì¥ì´ ë„ˆë¬´ ì ìœ¼ë©´ ê·¸ëƒ¥ 1ë¬¸ë‹¨
        if len(sents) <= 2:
            parts = [text]
        else:
            # ëª©í‘œ ë¬¸ë‹¨ ìˆ˜ì— ë§ì¶° ê· ë“± ë¶„ë°°
            chunk = max(2, (len(sents) + target_paragraphs - 1) // target_paragraphs)
            parts = [" ".join(sents[i:i+chunk]) for i in range(0, len(sents), chunk)]

    # 3) ê° ë¬¸ë‹¨ì„ <p>ë¡œ ê°ì‹¸ê¸° (escape ì²˜ë¦¬ í¬í•¨)
    return "".join(
        f"<p style='margin:0 0 10px 0; line-height:1.75;'>{h(p)}</p>"
        for p in parts
    )

# =========================
# âœ… Archive ì¹´ë“œìš© 1ì¤„ Insight ìƒì„±
# =========================
def summarize_insight_for_archive(one_to_three_lines: str) -> str:
    """
    ë©”ì¸ Weekly Focus Insight(1~3ì¤„)ë¥¼ ì…ë ¥ìœ¼ë¡œ ë°›ì•„,
    ì•„ì¹´ì´ë¸Œ ì¹´ë“œì— ë„£ì„ 'í•œ ì¤„' ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
    - ë„ˆë¬´ ê¸¸ë©´ ìë™ìœ¼ë¡œ ì§§ê²Œ
    - ì‹¤íŒ¨ ì‹œ: ì²« ë¬¸ì¥/ì²« ì¤„ì„ ì˜ë¼ì„œë¼ë„ ë°˜í™˜
    """
    src = (one_to_three_lines or "").strip()
    if not src:
        return ""

    # 1) ê°€ë²¼ìš´ fallback(ëª¨ë¸ ì‹¤íŒ¨ ëŒ€ë¹„)
    fallback = src.splitlines()[0].strip()
    if len(fallback) > 120:
        fallback = fallback[:117].rstrip() + "â€¦"

    # 2) GPTë¡œ "í•œ ë¬¸ì¥" ì••ì¶•
    try:
        system = (
            "ë„ˆëŠ” ì£¼ê°„ ì‚°ì—…/ê¸°ìˆ  íë¦„ì„ ì•„ì¹´ì´ë¸Œ ì¹´ë“œìš© 'í•œ ë¬¸ì¥'ìœ¼ë¡œ ë§Œë“œëŠ” ì—ë””í„°ì…ë‹ˆë‹¤. "
            "ì…ë ¥ì€ ì£¼ê°„ ì¸ì‚¬ì´íŠ¸(1~3ì¤„)ì´ë©°, ì¶œë ¥ì€ í•œêµ­ì–´ ì¡´ëŒ“ë§ 1ë¬¸ì¥(ë§ˆì¹¨í‘œ 1ê°œ)ì…ë‹ˆë‹¤. "
            "\n\n"
            "[í•„ìˆ˜ ê·œì¹™]\n"
            "1) ë°˜ë“œì‹œ ë™í–¥ ë™ì‚¬(ê°•í™”/ê°€ì†/í™•ì‚°/ë¶€ìƒ/ì „í™˜/ì¬í¸ ì¤‘ 1ê°œ ì´ìƒ)ë¥¼ í¬í•¨í•´, "
            "â€˜ë¬´ì—‡ì´ ì–´ë–»ê²Œ ë³€í•˜ê³  ìˆëŠ”ì§€â€™ë¥¼ íë¦„ ì¤‘ì‹¬ìœ¼ë¡œ ë§í•©ë‹ˆë‹¤.\n"
            "2) ì£¼ì œ ë‚˜ì—´(ì˜ˆ: â€˜Aì™€ Bì™€ Câ€™)ì´ë‚˜ ë©”íƒ€ ì„¤ëª…(ì˜ˆ: â€˜~ë¥¼ ë‹¤ë£¹ë‹ˆë‹¤/ì†Œê°œí•©ë‹ˆë‹¤/ìš”ì•½í•©ë‹ˆë‹¤/ì „ë°˜ì ìœ¼ë¡œâ€™)ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "3) íŠ¹ì • ê¸°ì—…/ì¡°ì§/ë¸Œëœë“œ(í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ë“±) ì§ì ‘ ì§€ì¹­ì€ ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "4) ê³¼ì¥, ë‹¨ì •ì  ì˜ˆì¸¡, ê³¼ë„í•œ ê²°ë¡ , ë¶ˆë¦¿/ë²ˆí˜¸ëŠ” ê¸ˆì§€í•©ë‹ˆë‹¤.\n"
            "5) 80ì ì´ë‚´ì˜ í•œ ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•©ë‹ˆë‹¤.\n"
            "\n"
            "[ê¶Œì¥ êµ¬ì„±]\n"
            "â€˜ì´ë²ˆ ì£¼ì—ëŠ” [í•µì‹¬ íë¦„]ì´ [ê°•í™”/ê°€ì†/í™•ì‚°/ë¶€ìƒ]ë˜ê³  ìˆìœ¼ë©°, [ì´ìœ /ë°©í–¥]ì´ í•¨ê»˜ ë‚˜íƒ€ë‚˜ê³  ìˆìŠµë‹ˆë‹¤.â€™\n"
            "\n"
            "[ì¢‹ì€ ì˜ˆ]\n"
            "â€˜ì´ë²ˆ ì£¼ì—ëŠ” ì‹¤ì‹œê°„ ê³µê°„ì •ë³´ì™€ ë¬´ì¸ì²´ê³„ ê²°í•© íë¦„ì´ ê°•í™”ë˜ë©°, êµ°Â·ìƒì—… í™œìš© í™•ì‚°ì´ ë‘ë“œëŸ¬ì§€ê³  ìˆìŠµë‹ˆë‹¤.â€™\n"
            "\n"
            "[ë‚˜ìœ ì˜ˆ]\n"
            "â€˜ì´ë²ˆ ì£¼ì—ëŠ” ìœ„ì„±, ë“œë¡ , AIë¥¼ ë‹¤ë£¹ë‹ˆë‹¤.â€™\n"
            "â€˜Aì™€ Bê°€ ì†Œê°œë˜ì—ˆìŠµë‹ˆë‹¤.â€™"
        )
        user = src

        resp = client.responses.create(
            model=MODEL_NAME,
            input=[
                {"role": "system", "content": system},
                {"role": "user", "content": user},
            ],
            temperature=0.3,
        )
        text = (resp.output[0].content[0].text or "").strip()

        MAX_ARCHIVE_INSIGHT_CHARS = 150

        # í•œ ë¬¸ì¥ë§Œ ì‚¬ìš©
        text = text.replace("\n", " ").strip()
        if not text:
            return fallback
        if len(text) > MAX_ARCHIVE_INSIGHT_CHARS:
            text = text[:MAX_ARCHIVE_INSIGHT_CHARS-1].rstrip() + "â€¦"
        return text
    except Exception as e:
        print(f"[WARN] Archive 1ì¤„ Insight ìƒì„± ì‹¤íŒ¨: {e}")
        return fallback



def build_archive_page_html(archive_items):
    rows = []

    for item in archive_items:
        date_str = item.get("date_str", "")
        insight = (item.get("insight") or item.get("insight_full") or "").strip()

        year_attr = ""
        month_attr = ""

        # "YYYY.MM.DD" í˜•ì‹ ê°€ì •
        if len(date_str) >= 7:
            year = date_str[:4]
            month_part = date_str[5:7]
            if year.isdigit() and month_part.isdigit():
                year_attr = year
                month_attr = str(int(month_part))  # "03" â†’ "3"

        insight_html = f"""
          <div class="archive-card-insight">
            {h(insight)}
          </div>
        """ if insight else ""

        rows.append(f"""
        <tr class="archive-row" data-year="{year_attr}" data-month="{month_attr}">
          <td style="padding-bottom:16px;">
            <a href="{item['url']}" class="archive-card">
              <div class="archive-card-label">
                {item['label']}
              </div>

              {insight_html}

              <div class="archive-card-date">
                ì—…ë¡œë“œ: {item.get('date_str','')}
              </div>
            </a>
          </td>
        </tr>
        """)

    card_list_html = "\n".join(rows)

    return f"""
<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="UTF-8">
<title>ì•„ì¹´ì´ë¸Œ</title>
<meta name="viewport" content="width=device-width, initial-scale=1.0">

<style>
  html, body {{
    margin: 0;
    padding: 0;
  }}

  body {{
    font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;
    background:#0b1120;
    color:#e5e7eb;
  }}

  .bg-video {{
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    object-fit: cover;
    z-index: -1;
    opacity: 0.5;
  }}

  .bg-image {{
    position: fixed;
    top: 0;
    left: 0;
    width: 100%;
    height: 100%;
    z-index: -2;              /* video(-1)ë³´ë‹¤ ë’¤ë¡œ */
    background-image: url('https://hancom-inspace.github.io/Weekly-Newsletter/assets/archive_bg_fallback.png');
    background-size: cover;
    background-position: center top;
    background-repeat: no-repeat;
    display: none;   /* ë°ìŠ¤í¬í†± ê¸°ë³¸ê°’ */
  }}


  .archive-wrap {{
    min-height: 100vh;
    padding: 40px 16px 64px 16px;
    background:
      radial-gradient(circle at top,
        rgba(148,163,184,0.25) 0,
        transparent 55%);
  }}


  .filter-wrap {{
    margin: 0 16px 12px 16px;
  }}

  .filter-group-label {{
    font-size:13px;
    color:#cbd5f5;
    margin-bottom:6px;
    opacity:0.8;
  }}

  .chips-row {{
    display:flex;
    flex-wrap:wrap;
    gap:8px;
    margin-bottom:6px;
  }}

  .chip-base {{
    border-radius:9999px;
    border:1px solid rgba(148,163,184,0.6);
    background:rgba(15,23,42,0.6);
    color:#e5e7eb;
    font-size:13px;
    padding:6px 12px;
    cursor:pointer;
    outline:none;
    white-space:nowrap;
    transition:background 0.18s ease, border-color 0.18s ease, transform 0.12s ease, box-shadow 0.12s ease;
  }}


  /* âœ… Weekly Focus Insight(ì•„ì¹´ì´ë¸Œ ì¹´ë“œìš©) */
  .archive-card-insight{{
    font-size:13px;
    line-height:1.45;
    color:#cbd5e1;
    margin:6px 0 8px 0;
    opacity:0.92;

    display:-webkit-box;
    -webkit-line-clamp:2;
    -webkit-box-orient:vertical;
    overflow:hidden;

    /* âœ… ë‹¨ì–´ ì¤‘ê°„ ì¤„ë°”ê¿ˆ ë°©ì§€ */
    word-break: keep-all;
    overflow-wrap: break-word;
  }}


  .chip-base:hover {{
    background:rgba(30,64,175,0.7);
    border-color:#60a5fa;
    box-shadow:0 0 0 1px rgba(15,23,42,0.4);
    transform:translateY(-1px);
  }}

  .chip-base.active {{
    background:#2563eb;
    border-color:#2563eb;
    color:#ffffff;
    box-shadow:0 0 0 1px rgba(15,23,42,0.4);
  }}

  .archive-card {{
      display:block;
      text-decoration:none;
      border-radius:14px;
      border:1px solid rgba(255,255,255,0.55);
      padding:18px 20px;
      background:rgba(255,255,255,0.18);
      backdrop-filter: blur(12px);
      -webkit-backdrop-filter: blur(12px);
      box-shadow:0 10px 28px rgba(15,23,42,0.6);
      transition:transform 0.22s ease, box-shadow 0.22s ease, border-color 0.22s ease, background 0.22s ease;
  }}

  .archive-card:hover {{
      transform:translateY(-4px);
      background:rgba(255,255,255,0.32);
      box-shadow:0 18px 40px rgba(15,23,42,0.85);
      border-color:rgba(255,255,255,0.95);
  }}

  .archive-card-label {{
    font-size:17px;
    font-weight:600;
    color:#ffffff;
    margin-bottom:4px;
  }}

  .archive-card-date {{
    font-size:13px;
    color:#e5e7eb;
  }}

  @media (max-width: 768px) {{
    .archive-card {{
      padding:14px;
    }}
  }}

  @media (max-width: 768px) {{
    .bg-video {{
      display: none;
    }}

    /* ëª¨ë°”ì¼ì—ì„  ê³ ì • ì´ë¯¸ì§€ ë ˆì´ì–´ë¥¼ ì‚¬ìš© */
    .bg-image {{
      display: block;
    }}

    /* ëª¨ë°”ì¼ì—ì„œ body ë°°ê²½ì€ ì“°ì§€ ì•ŠìŒ(ì¤‘ë³µ/ìŠ¤í¬ë¡¤ ì´ìŠˆ ë°©ì§€) */
    body {{
      background: none;
    }}
  }}


    .archive-wrap {{
      background: radial-gradient(
        circle at top,
        rgba(148,163,184,0.18) 0,
        transparent 55%
      );
    }}
  }}
</style>
</head>

<body>
  <div class="bg-image"></div>

  <video id="bgVideo" class="bg-video" preload="auto" muted playsinline>
    <source src="{ARCHIVE_VIDEO_URL}" type="video/mp4" />
  </video>

  <div class="archive-wrap">
    <center>
      <table width="100%" cellpadding="0" cellspacing="0" border="0" style="max-width:700px; margin:0 auto;">
        <tr>
          <td style="padding:60px 16px 30px 16px; font-size:28px; font-weight:700; color:#f9fafb;">
            ğŸ“š ì´ì „ ë‰´ìŠ¤ë ˆí„° ë‹¤ì‹œë³´ê¸°
          </td>
        </tr>

        <tr>
          <td>
            <div class="filter-wrap">
              <div id="year-chips" class="chips-row"></div>
              <div id="month-chips" class="chips-row"></div>
            </div>
          </td>
        </tr>

        <tr>
          <td style="padding:4px 16px 8px 16px;">
            <table width="100%" cellpadding="0" cellspacing="0" border="0">
              <tbody>
                {card_list_html}
              </tbody>
            </table>
          </td>
        </tr>
      </table>
    </center>
  </div>

<script>
  document.addEventListener('DOMContentLoaded', function() {{

    // =========================================================
    // 0) ë¹„ë””ì˜¤ ìŠ¤í¬ë¡¤ ìŠ¤í¬ëŸ½(ìŠ¤ë¬´ë”©) â€” í•„í„° returnê³¼ ë¬´ê´€í•˜ê²Œ í•­ìƒ ë™ì‘
    // =========================================================
    var video = document.getElementById('bgVideo');

    if (video) {{
      try {{ video.pause(); }} catch (e) {{}}

      var duration = 0;
      var ready = false;

      // âœ… ì˜µì…˜ A íŒŒë¼ë¯¸í„°
      var SCRUB_PORTION = 0.25;
      var EASE_GAMMA = 1.6;

      // âœ… ìŠ¤ë¬´ë”© íŒŒë¼ë¯¸í„°
      var SMOOTHING = 0.12; // 0.08~0.2 ì¶”ì²œ

      var targetTime = 0;
      var smoothTime = 0;
      var animating = false;

      function calcTargetTimeFromScroll() {{
        if (!ready || !duration) return;

        var doc = document.documentElement;
        var scrollTop = window.pageYOffset || doc.scrollTop || 0;
        var maxScroll = (doc.scrollHeight - window.innerHeight);

        // 0~1 ìŠ¤í¬ë¡¤ ì§„í–‰ë¥ 
        var p = (maxScroll > 0) ? (scrollTop / maxScroll) : 0;
        if (p < 0) p = 0;
        if (p > 1) p = 1;

        // ìŠ¤í¬ë¡¤ ìµœìƒë‹¨ = ì˜ìƒ ì²˜ìŒ / ìµœí•˜ë‹¨ = ì˜ìƒ ë
        var desired = p * duration;

        // ë í”„ë ˆì„ ì•ˆì •í™” (ì„ íƒ)
        var END_MARGIN = 0.03;
        if (desired > duration - END_MARGIN) desired = duration - END_MARGIN;
        if (desired < 0) desired = 0;

        targetTime = desired;

        // ìŠ¤í¬ë¡¤ ì†ë„ ê¸°ë°˜ ìŠ¤ë¬´ë”© ì¡°ì ˆ
        if (!window.__bgScrubState) {{
          window.__bgScrubState = {{ lastScrollTop: scrollTop, lastTs: performance.now() }};
        }}

        var st = window.__bgScrubState;
        var now = performance.now();
        var dt = Math.max(1, now - st.lastTs);
        var ds = Math.abs(scrollTop - st.lastScrollTop);

        var v = ds / dt; // px/ms

        var minS = 0.06;
        var maxS = 0.28;
        var dynamic = minS + (v * 0.10);
        if (dynamic > maxS) dynamic = maxS;

        SMOOTHING = dynamic;

        st.lastScrollTop = scrollTop;
        st.lastTs = now;
      }}

      function applyTime(t) {{
        // ë„ˆë¬´ ì¦ì€ seek ë°©ì§€(ë””ì½”ë” ë¶€ë‹´â†“)
        var EPS = 0.02;
        if (Math.abs((video.currentTime || 0) - t) < EPS) return;

        try {{
          if (typeof video.fastSeek === 'function') {{
            video.fastSeek(t);
          }} else {{
            video.currentTime = t;
          }}
        }} catch (e) {{}}
      }}

      function animate() {{
        if (!animating) return;

        smoothTime = smoothTime + (targetTime - smoothTime) * SMOOTHING;
        applyTime(smoothTime);

        window.requestAnimationFrame(animate);
      }}

      function startAnimLoop() {{
        if (animating) return;
        animating = true;
        smoothTime = video.currentTime || 0;
        window.requestAnimationFrame(animate);
      }}

      function onMetaReady() {{
        duration = video.duration || 0;
        ready = duration > 0;

        // ë©”íƒ€ë°ì´í„° ì¤€ë¹„ í›„ ì²« ë™ê¸°í™” + ë£¨í”„ ì‹œì‘
        calcTargetTimeFromScroll();
        startAnimLoop();
      }}

      video.addEventListener('loadedmetadata', onMetaReady);

      // ì´ë¯¸ ì¤€ë¹„ëœ ê²½ìš° ëŒ€ë¹„
      if (video.readyState >= 1) {{
        onMetaReady();
      }} else {{
        // ì¼ë¶€ í™˜ê²½ì—ì„œ ì•ˆì „í•˜ê²Œ metadataë¥¼ ë‹¹ê¸°ê¸°
        try {{ video.load(); }} catch (e) {{}}
      }}

      window.addEventListener(
        'scroll',
        function() {{
          calcTargetTimeFromScroll();
          startAnimLoop();
        }},
        {{ passive: true }}
      );

      window.addEventListener('resize', function() {{
        calcTargetTimeFromScroll();
      }});
    }}

    // =========================================================
    // 1) ì—°/ì›” í•„í„°(ê¸°ì¡´ ì½”ë“œ) â€” ì—†ìœ¼ë©´ í•„í„°ë§Œ ìŠ¤í‚µ
    // =========================================================
    var rows = Array.prototype.slice.call(document.querySelectorAll('.archive-row'));
    var yearContainer = document.getElementById('year-chips');
    var monthContainer = document.getElementById('month-chips');

    // âœ… ì—¬ê¸°ì„œ return í•˜ì§€ ë§ê³ , í•„í„°ë§Œ ì¢…ë£Œ
    if (!rows.length || !yearContainer || !monthContainer) {{
      return;
    }}

    var yearSet = new Set();
    var yearMonthMap = {{}};

    rows.forEach(function(row) {{
      var y = row.getAttribute('data-year');
      var m = parseInt(row.getAttribute('data-month') || '0', 10);
      if (!y) return;

      yearSet.add(y);

      if (!yearMonthMap[y]) {{
        yearMonthMap[y] = new Set();
      }}
      if (m > 0) {{
        yearMonthMap[y].add(m);
      }}
    }});

    var years = Array.from(yearSet).sort();

    var today = new Date();
    var currentYear = String(today.getFullYear());

    var state = {{
      year: yearSet.has(currentYear) ? currentYear : 'all',
      month: null
    }};

    monthContainer.style.display = 'none';

    function createYearChip(value, label) {{
      var btn = document.createElement('button');
      btn.type = 'button';
      btn.className = 'chip-base year-chip';
      btn.setAttribute('data-year', value);
      btn.textContent = label;

      btn.addEventListener('click', function() {{
        state.year = value;
        state.month = null;

        if (value === 'all') {{
          clearMonthChips();
        }} else {{
          renderMonthChipsForYear(value);
        }}

        updateView();
      }});

      return btn;
    }}

    function createMonthChip(monthNumber) {{
      var btn = document.createElement('button');
      btn.type = 'button';
      btn.className = 'chip-base month-chip';
      btn.setAttribute('data-month', String(monthNumber));
      btn.textContent = monthNumber + 'ì›”';

      btn.addEventListener('click', function() {{
        state.month = monthNumber;
        updateView();
      }});

      return btn;
    }}

    function clearMonthChips() {{
      monthContainer.innerHTML = '';
      monthContainer.style.display = 'none';
    }}

    function renderMonthChipsForYear(year) {{
      monthContainer.innerHTML = '';

      var monthSet = yearMonthMap[year];
      if (!monthSet || monthSet.size === 0) {{
        monthContainer.style.display = 'none';
        return;
      }}

      var months = Array.from(monthSet);
      months.sort(function(a, b) {{ return a - b; }});

      months.forEach(function(m) {{
        monthContainer.appendChild(createMonthChip(m));
      }});

      monthContainer.style.display = 'flex';
    }}

    function updateView() {{
      rows.forEach(function(row) {{
        var rowYear = row.getAttribute('data-year');
        var rowMonth = parseInt(row.getAttribute('data-month') || '0', 10);

        var yearMatch = (state.year === 'all') || (rowYear === state.year);
        var monthMatch;

        if (state.year === 'all') {{
          monthMatch = true;
        }} else if (!state.month) {{
          monthMatch = true;
        }} else {{
          monthMatch = (rowMonth === state.month);
        }}

        row.style.display = (yearMatch && monthMatch) ? '' : 'none';
      }});

      var yearChips = document.querySelectorAll('.year-chip');
      yearChips.forEach(function(chip) {{
        var y = chip.getAttribute('data-year');
        chip.classList.toggle('active', y === state.year);
      }});

      var monthChips = document.querySelectorAll('.month-chip');
      monthChips.forEach(function(chip) {{
        var m = parseInt(chip.getAttribute('data-month') || '0', 10);
        var isActive = (state.year !== 'all') && (state.month === m);
        chip.classList.toggle('active', isActive);
      }});
    }}

    var allYearChip = createYearChip('all', 'ì „ì²´');
    yearContainer.appendChild(allYearChip);

    years.forEach(function(y) {{
      yearContainer.appendChild(createYearChip(y, y + 'ë…„'));
    }});

    if (state.year !== 'all') {{
      renderMonthChipsForYear(state.year);
    }} else {{
      clearMonthChips();
    }}

    updateView();
  }});
</script>



</body>
</html>
"""





# ============================================================
# â–¼ GitHub Pagesìš© ë‚ ì§œ í´ë” ë° URL ì„¤ì •
# ============================================================

# í•œêµ­ ì‹œê°„ ê¸°ì¤€ ì˜¤ëŠ˜ ë‚ ì§œë¡œ í´ë” ìƒì„± (ì˜ˆ: 2025/11/26)
KST = timezone(timedelta(hours=9))
now_kst = datetime.now(KST)

FOLDER_PATH = f"{now_kst.year}/{now_kst.month:02d}/{now_kst.day:02d}"
# ì˜ˆ: "2025/11/26"

# ë©”ì¸ í˜ì´ì§€ ë° í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ URL
MAIN_PAGE_URL = f"{BASE_URL}/{FOLDER_PATH}/index.html"

TOPIC_MORE_URLS = {
    t: f"{BASE_URL}/{FOLDER_PATH}/{TOPIC_MORE_FILENAMES[t]}"
    for t in TOPIC_MORE_FILENAMES
}

RESEARCH_MORE_URL = f"{BASE_URL}/{FOLDER_PATH}/{RESEARCH_MORE_FILENAME}"

# â–¼ ì´ì „ ë‰´ìŠ¤ë ˆí„° ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ ì„¤ì •
ARCHIVE_PAGE_PATH = "docs/archive.html"
ARCHIVE_PAGE_URL = f"{BASE_URL}/archive.html"

# â–¼ ì•„ì¹´ì´ë¸Œ ìƒë‹¨ ìŠ¤í¬ë¡¤ ë¹„ë””ì˜¤(mp4) ê²½ë¡œ (ì—¬ê¸°ì— ë„¤ ì˜ìƒ URL ë„£ê¸°)
ARCHIVE_VIDEO_URL = "https://hancom-inspace.github.io/Weekly-Newsletter/assets/archivebg1.mp4"

# ============================================================
# â–¼ (NEW) ë©”ì¸ ë‰´ìŠ¤ë ˆí„° ë°°ê²½ ìŠ¤í¬ë¡¤ ë¹„ë””ì˜¤/ëŒ€ì²´ ì´ë¯¸ì§€ ì„¤ì •
# ============================================================
MAIN_BG_VIDEO_URL = "https://hancom-inspace.github.io/Weekly-Newsletter/assets/archivebg1.mp4"  # â† ë©”ì¸ì— ê¹” ì˜ìƒ
MAIN_BG_FALLBACK_IMAGE_URL = "https://hancom-inspace.github.io/Weekly-Newsletter/assets/archive_bg_fallback.png"  # â† ëª¨ë°”ì¼ ëŒ€ì²´ ì´ë¯¸ì§€

# fade/ì¸í„°ë™ì…˜ ì‹œì‘ ìŠ¤í¬ë¡¤ ìœ„ì¹˜(px) (2~3ë²ˆ ìŠ¤í¬ë¡¤ ëŠë‚Œìœ¼ë¡œ ì¡°ì •)
MAIN_BG_FADE_START_PX = 200
MAIN_BG_FADE_END_PX   = 800


# â–¼ ì•„ì¹´ì´ë¸Œ ë°ì´í„°ë¥¼ JSON íŒŒì¼ë¡œë„ ê´€ë¦¬ (ë§¤ë²ˆ ê¹ƒí—ˆë¸Œ í´ë”ë¥¼ ì½ì–´ì„œ ê°±ì‹ )
ARCHIVE_JSON_PATH = "docs/archive.json"


def list_github_directory(path_in_repo: str):
    """
    GitHub REST APIë¥¼ ì‚¬ìš©í•´ì„œ ì£¼ì–´ì§„ ê²½ë¡œì˜ ë””ë ‰í„°ë¦¬/íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¨ë‹¤.
    path_in_repo ì˜ˆ: 'docs', 'docs/2025', 'docs/2025/11' ë“±
    """
    if not GITHUB_TOKEN:
        print("[ê²½ê³ ] GITHUB_TOKEN ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ë””ë ‰í„°ë¦¬ ëª©ë¡ ì¡°íšŒë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return []

    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{path_in_repo}"

    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    resp = requests.get(api_url, headers=headers)
    if resp.status_code == 200:
        return resp.json()
    else:
        print(f"[GitHub ë””ë ‰í„°ë¦¬ ì¡°íšŒ ì‹¤íŒ¨] {path_in_repo} status={resp.status_code}")
        try:
            print(resp.text)
        except Exception:
            pass
        return []


import base64

def get_github_file_text(path_in_repo: str):
    """
    GitHub repo ì•ˆì˜ íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜
    (ì˜ˆ: docs/archive.json)
    """
    if not GITHUB_TOKEN:
        return None

    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{path_in_repo}"
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    resp = requests.get(api_url, headers=headers)
    if resp.status_code != 200:
        return None

    data = resp.json()
    if data.get("encoding") != "base64":
        return None

    try:
        return base64.b64decode(data["content"]).decode("utf-8")
    except Exception:
        return None


def load_existing_archive():
    """
    âœ… 1ìˆœìœ„: docs/archive.json ì´ ìˆìœ¼ë©´ ê·¸ê±¸ ê·¸ëŒ€ë¡œ ì‚¬ìš© (insight ìœ ì§€)
    âœ… 2ìˆœìœ„: ì—†ìœ¼ë©´ ê¸°ì¡´ ë°©ì‹ëŒ€ë¡œ í´ë” ìŠ¤ìº”
    """

    # --------------------------------------------------
    # 1) archive.json ë¨¼ì € ì‹œë„
    # --------------------------------------------------
    json_text = get_github_file_text("docs/archive.json")
    if json_text:
        try:
            items = json.loads(json_text)
            if isinstance(items, list):
                return items
        except Exception:
            pass  # ì‹¤íŒ¨í•˜ë©´ fallback

    # --------------------------------------------------
    # 2) fallback: ê¸°ì¡´ í´ë” ìŠ¤ìº” ë°©ì‹
    # --------------------------------------------------
    archive_items = []

    docs_children = list_github_directory("docs")
    year_dirs = [
        item["name"]
        for item in docs_children
        if item.get("type") == "dir" and item.get("name", "").isdigit()
    ]

    for year_name in year_dirs:
        year_path = f"docs/{year_name}"
        month_children = list_github_directory(year_path)
        month_dirs = [
            m["name"]
            for m in month_children
            if m.get("type") == "dir" and m.get("name", "").isdigit()
        ]

        for month_name in month_dirs:
            month_path = f"docs/{year_name}/{month_name}"
            day_children = list_github_directory(month_path)
            day_dirs = [
                d["name"]
                for d in day_children
                if d.get("type") == "dir" and d.get("name", "").isdigit()
            ]

            for day_name in day_dirs:
                day_path = f"docs/{year_name}/{month_name}/{day_name}"
                files = list_github_directory(day_path)

                has_index = any(
                    f.get("type") == "file" and f.get("name") == "index.html"
                    for f in files
                )
                if not has_index:
                    continue

                try:
                    date_obj = datetime(int(year_name), int(month_name), int(day_name)).date()
                except ValueError:
                    continue

                date_str = date_obj.strftime("%Y.%m.%d")
                week_no = ((date_obj.day - 1) // 7) + 1
                label = f"{date_obj.month}ì›” {week_no}ì£¼ì°¨ ë‰´ìŠ¤ë ˆí„°"
                url = f"{BASE_URL}/{year_name}/{month_name}/{day_name}/index.html"

                archive_items.append({
                    "label": label,
                    "date_str": date_str,
                    "url": url,
                    "insight": "",   # fallbackì—ëŠ” insight ì—†ìŒ
                })

    return archive_items



# ì‹¤í–‰ ì‹œì ì— ì´ì „ ì•„ì¹´ì´ë¸Œ ëª©ë¡ ë¡œë”©
NEWSLETTER_ARCHIVE_BASE = load_existing_archive()


# --- ì›” ê¸°ì¤€ ì£¼ì°¨ ê³„ì‚° (í•´ë‹¹ ë‹¬ì—ì„œ ëª‡ ë²ˆì§¸ ì£¼ì¸ì§€) ---
today_date = now_kst.date()
day_in_month = today_date.day

# 1~7ì¼: 1ì£¼ì°¨, 8~14ì¼: 2ì£¼ì°¨, 15~21ì¼: 3ì£¼ì°¨, 22~28ì¼: 4ì£¼ì°¨, 29~31ì¼: 5ì£¼ì°¨
week_no = ((day_in_month - 1) // 7) + 1

WEEK_LABEL = f"{today_date.month}ì›” {week_no}ì£¼ì°¨"


# ë‰´ìŠ¤ë ˆí„° ì—…ë°ì´íŠ¸ ë‚ ì§œë„ í•œêµ­ ì‹œê°„ ê¸°ì¤€ìœ¼ë¡œ
NEWSLETTER_DATE = now_kst.strftime("%Y.%m.%d")


# ============================================================
# â–¼ Section HTML ìƒì„±
# ============================================================

sections_html = build_sections_html(topic_main_articles, topic_extra_articles)

# ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ ì„¹ì…˜ ì¶”ê°€ (ìœ„ì„±ì˜ìƒ ì„¹ì…˜ ì•„ë˜ì— ë¶™ëŠ” íš¨ê³¼)
research_section_html = build_research_section_html(
    research_main_articles,
    research_extra_articles,
    RESEARCH_MORE_URL,
)

sections_html = sections_html + research_section_html


weekly_focus_insight = generate_weekly_focus_insight(
    topic_main_articles, topic_extra_articles,
    research_main_articles, research_extra_articles
)

weekly_focus_body_html = to_paragraph_html_auto(weekly_focus_insight, target_paragraphs=4)

weekly_focus_html = f"""
<table width="100%" cellpadding="0" cellspacing="0" border="0" style="margin-bottom:24px;">
  <tr><td align="center">
    <table cellpadding="0" cellspacing="0" border="0"
           style="width:100%; max-width: {CONTENT_WIDTH}px;
                  border:1px solid #e5e7eb;
                  border-radius:12px;
                  overflow:hidden;
                  box-sizing:border-box;
                  box-shadow:0 10px 24px rgba(0,0,0,0.14);">
      <tr>
        <td style="
            background-image:url('https://hancom-inspace.github.io/Weekly-Newsletter/assets/insightcard2.png');
            background-size: cover;
            background-position: center;
            background-repeat:no-repeat;
            padding:0;">

          <!-- ê°€ë…ì„± ì˜¤ë²„ë ˆì´ -->
          <div style="background:rgba(255,255,255,0.65);
                      padding:15px;">

            <div style="font-size:20px; font-weight:800; color:#000000;">
              {WEEKLY_FOCUS_TITLE}
            </div>

            <div style="font-size:14px; color:#000000; line-height:1.7; font-weight:500;
                        padding-top:10px; word-break: keep-all; overflow-wrap: break-word;">
              {weekly_focus_body_html if weekly_focus_insight else "ì´ë²ˆ ì£¼ í¬ì»¤ìŠ¤ ì¸ì‚¬ì´íŠ¸ë¥¼ ìƒì„±í•˜ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."}
            </div>

          </div>

        </td>
      </tr>
    </table>
  </td></tr>
</table>
"""



# ë‚ ì§œ ë²”ìœ„ ê³„ì‚° (ê¸°ì¡´ ë¡œì§ ìœ ì§€)
dates = []
for topic_dict in (topic_main_articles, topic_extra_articles):
    for t in [1, 2, 3, 4]:
        for art in topic_dict.get(t, []):
            raw = (art.get("date") or "").strip()
            try:
                d = datetime.strptime(raw, "%Y-%m-%d")
                dates.append(d)
            except Exception:
                pass

if dates:
    start_date = min(dates)
    end_date = max(dates)
    date_range = f"{start_date.year}ë…„ {start_date.month}ì›” {start_date.day}ì¼ ~ {end_date.year}ë…„ {end_date.month}ì›” {end_date.day}ì¼"
else:
    date_range = ""

# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ HTML ìƒì„±
more_pages_html = {}
for topic_num in [1, 2, 3, 4]:
    extras = topic_extra_articles.get(topic_num, [])
    if not extras:
        continue

    header_title = TOPIC_MORE_TITLES.get(topic_num, f"Topic {topic_num} - ì¶”ê°€ ê¸°ì‚¬")
    more_pages_html[topic_num] = build_more_page_html(
        {topic_num: extras},
        date_range,
        NEWSLETTER_DATE,
        header_title,
    )

# ğŸ’¡ ìµœì‹  ì—°êµ¬ë™í–¥ - ì¶”ê°€ í•™ìˆ ì§€ í˜ì´ì§€ HTML ìƒì„±
# - ì´ í˜ì´ì§€ì—ëŠ” 'ì •í•œ ê¸°ê°„ ë‚´ ëª¨ë“  í•™ìˆ ì§€'ë¥¼ ë³´ì—¬ì£¼ê¸° ìœ„í•´ research_more_articles ì‚¬ìš©
research_more_html = build_research_more_page_html(
    research_more_articles,
    date_range,
    NEWSLETTER_DATE,
)



# ============================================================
# â–¼ ìµœì¢… ë‰´ìŠ¤ë ˆí„° HTML
# ============================================================

import base64  # íŒŒì¼ ìƒë‹¨ì— ì´ë¯¸ ì—†ìœ¼ë©´, ë§¨ ìœ„ import ë¶€ë¶„ì— ì¶”ê°€ë˜ì–´ ìˆì–´ì•¼ í•¨.

def upload_file_to_github(path_in_repo: str, content_str: str, commit_message: str):
    """
    GitHub REST APIë¥¼ ì‚¬ìš©í•´ì„œ íŒŒì¼ 1ê°œë¥¼ docs/ ì´í•˜ì— ì—…ë¡œë“œ/ì—…ë°ì´íŠ¸.
    path_in_repo ì˜ˆ: 'docs/2025/11/26/index.html'
    """
    if not GITHUB_TOKEN:
        print("[ê²½ê³ ] GITHUB_TOKEN ì´ ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None

    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{path_in_repo}"

    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    # 1) ê¸°ì¡´ íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸ (ìˆìœ¼ë©´ sha í•„ìš”)
    sha = None
    get_resp = requests.get(api_url, headers=headers)
    if get_resp.status_code == 200:
        sha = get_resp.json().get("sha")

    # 2) ë‚´ìš© base64 ì¸ì½”ë”©
    encoded = base64.b64encode(content_str.encode("utf-8")).decode("utf-8")

    payload = {
        "message": commit_message,
        "content": encoded,
        "branch": "main",
    }
    if sha:
        payload["sha"] = sha  # ì—…ë°ì´íŠ¸

    put_resp = requests.put(api_url, headers=headers, json=payload)

    if put_resp.status_code in (200, 201):
        print(f"[GitHub ì—…ë¡œë“œ ì„±ê³µ] {path_in_repo}")
        return put_resp.json()
    else:
        print(f"[GitHub ì—…ë¡œë“œ ì‹¤íŒ¨] {path_in_repo} status={put_resp.status_code}")
        print(put_resp.text)
        return None


newsletter_html = f"""

<!DOCTYPE html>
<html lang="ko">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1">


<title>í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°</title>
<link rel="preconnect" href="https://cdn.jsdelivr.net" crossorigin>
<link rel="stylesheet" as="style" href="https://cdn.jsdelivr.net/gh/orioncactus/pretendard/dist/web/static/pretendard.css">

<style>
  @media (max-width: 768px) {{
    .hero-bg {{
      background-position: center 0 !important;
    }}
    .hero-header-cell {{
      padding: 32px 0 28px 0 !important;
    }}
    .inner-padding {{
      padding-left: 16px !important;
      padding-right: 16px !important;
    }}
    .main-title {{
      font-size: 32px !important;
    }}
    .sub-title {{
      font-size: 14px !important;
    }}
  }}


  /* ===============================
     ë©”ì¸ ë°°ê²½ ìŠ¤í¬ë¡¤ ë¹„ë””ì˜¤
     =============================== */
  .main-bg-layer {{
    position: fixed;
    inset: 0;
    z-index: -1;
    pointer-events: none;
  }}

  .main-bg-video,
  .main-bg-image,
  .main-bg-whitewash {{
    position: absolute;
    inset: 0;
    width: 100%;
    height: 100%;
  }}

  .main-bg-video {{
    object-fit: cover;
    opacity: 0;
  }}

  .main-bg-image {{
    background-image: url('{MAIN_BG_FALLBACK_IMAGE_URL}');
    background-size: cover;
    background-position: center top;
    background-repeat: no-repeat;
    display: none;
    opacity: 0;
  }}

  .main-bg-whitewash {{
    background: #f3f4f6;
    opacity: 1;
  }}

  @media (max-width: 768px) {{
    .main-bg-video {{ display: none !important; }}
    .main-bg-image {{ display: block !important; }}
  }}

    /* í‘¸í„°ë¥¼ í•­ìƒ ì•„ë˜ë¡œ ë°€ì–´ì£¼ëŠ” ë ˆì´ì•„ì›ƒ (iOS 100vh ì´ìŠˆ ëŒ€ì‘) */
  html, body {{
    margin: 0;
    padding: 0;
  }}

  body {{
    display: flex;
    flex-direction: column;

    min-height: 100vh;
    min-height: 100svh;
    min-height: 100dvh;

    background: #f3f4f6;

    font-family: "Pretendard", -apple-system, BlinkMacSystemFont,
                "Apple SD Gothic Neo", "ë§‘ì€ ê³ ë”•", system-ui, sans-serif;
  }}


  .content-wrapper {{
    flex: 1 0 auto;
  }}

  .footer-wrap {{
    flex-shrink: 0;
  }}


</style>



</head>

<body style="margin:0; padding:0; background:#f3f4f6;
             font-family:-apple-system,BlinkMacSystemFont,"Apple SD Gothic Neo","ë§‘ì€ ê³ ë”•",system-ui,sans-serif;">


<!-- (NEW) ë©”ì¸ ë°°ê²½ ë ˆì´ì–´: ì´ë©”ì¼ì—ì„œëŠ” JSê°€ ì•ˆ ëŒì•„ display:none ê·¸ëŒ€ë¡œë¼ ì˜ìƒ/ì´ë¯¸ì§€ê°€ â€œì•ˆ ë³´ì„(í°ìƒ‰ ìœ ì§€)â€ -->
<div class="main-bg-layer" id="main-bg-layer" style="display:none;">
  <video
    class="main-bg-video"
    id="main-bg-video"
    playsinline
    muted
    preload="auto"
  >
    <source src="{MAIN_BG_VIDEO_URL}" type="video/mp4">
  </video>

  <div class="main-bg-image" id="main-bg-image"></div>
  <div class="main-bg-whitewash" id="main-bg-whitewash"></div>
</div>



<!-- í—¤ë” -->

<table class="hero-bg" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background-image:url('{HEADER_BACKGROUND}');
              background-size:cover;
              background-position:center -60px;
              background-repeat:no-repeat;">
  <tr>
    <td align="center" class="hero-header-cell"
        bgcolor="#000000"
        style="padding:12px 24px 10px 24px;
               background: linear-gradient(to bottom right,
                           rgba(0,0,40,0.55),
                           rgba(0,0,0,0.55));
               color:#ffffff; ;">

      <table cellpadding="0" cellspacing="0" border="0"
             style="max-width:{CONTENT_WIDTH}px; width:100%;
                    color:#ffffff; margin:0 auto;">

        <tr>
          <td class="inner-padding" style="padding:16px 24px 8px 24px;">
            <table width="100%">
              <tr>
                <td align="left">
                  <img src="{LOGO_URL}" style="max-width:110px; display:block;">
                </td>
                <td align="right"
                    style="text-transform:uppercase; font-size:13px;
                           color:#ffffff; ;">
                  WWW.INSPACE.CO.KR
                </td>
              </tr>
            </table>
          </td>
        </tr>

        <tr>
          <td align="center" class="inner-padding"
              style="padding:12px 24px 6px 24px;
                     font-size:40px; font-weight:600;
                     color:#ffffff; ;">
            <div class="main-title"
                 style="color:#ffffff; ;">
              InSpace Weekly
            </div>
          </td>
        </tr>

        <tr>
          <td align="center" class="inner-padding sub-title"
              style="padding:0 24px 8px 24px;
                     font-size:15px; font-weight:300; opacity:0.85;
                     color:#ffffff; ;">
            {date_range}
          </td>
        </tr>

        <tr>
          <td class="inner-padding" style="padding:8px 24px 12px 24px;">
            <table width="100%">
              <tr>
                <td></td>
                <td align="right"
                    style="font-size:13px; line-height:1.6;
                           color:#ffffff; ;">
                  í•œì»´ ì¸ìŠ¤í˜ì´ìŠ¤<br>{WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°
                  <div style="margin-top:4px; letter-spacing:0.14em;
                              color:#ffffff; ;">
                    ì—…ë°ì´íŠ¸: {NEWSLETTER_DATE}
                  </div>
                </td>
              </tr>
            </table>
          </td>
        </tr>

      </table>

    </td>
  </tr>
</table>


<!-- ë³¸ë¬¸ -->

<table width="100%" cellpadding="0" cellspacing="0" border="0" style="padding:24px 0 16px 0;">
  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
        <tr>
          <td class="inner-padding" style="padding:0 16px 0 16px;">

            <!-- ì´ì „ ë‰´ìŠ¤ë ˆí„° ë³´ê¸° ë²„íŠ¼ -->
            <table width="100%" cellpadding="0" cellspacing="0" border="0" style="margin-bottom:24px;">
              <tr>
                <td
                  style="border-radius:12px;
                        overflow:hidden;
                        border:1px solid #e5e7eb;
                        background-image:url('https://hancom-inspace.github.io/Weekly-Newsletter/assets/archivebutton.png');
                        background-size:cover;
                        background-position:center;
                        background-repeat:no-repeat;"
                        box-shadow:0 10px 24px rgba(0,0,0,0.14);">

                  <a href="{ARCHIVE_PAGE_URL}"
                    style="display:block; text-decoration:none; color:inherit;">

                    <div style="background:rgba(255,255,255,0.70); padding:20px 18px;">
                      <div style="font-size:14px; font-weight:700; color:#000000; margin-bottom:4px;">
                        ì•„ì¹´ì´ë¸Œ
                      </div>
                      <div style="font-size:18px; font-weight:700; color:#000000;">
                        ì´ì „ ë‰´ìŠ¤ë ˆí„° ë‹¤ì‹œ ë³´ê¸° â†’
                      </div>
                    </div>

                  </a>
                </td>
              </tr>
            </table>

            {weekly_focus_html}
            {sections_html}

          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>

<!-- í‘¸í„° -->
<table class="footer-wrap" width="100%" cellpadding="0" cellspacing="0" border="0"
       style="background:#1e293b; padding:24px 0 32px 0;">

  <tr>
    <td align="center">
      <table cellpadding="0" cellspacing="0" border="0"
             style="max-width:{CONTENT_WIDTH}px; width:100%; margin:0 auto;">
        <tr>
          <td class="inner-padding"
              style="padding:12px 16px;
                     font-size:12px;
                     color:#e2e8f0;
                     text-align:center;
                     line-height:1.6;">

            ë³¸ ë©”ì¼ì€ í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ ì‚¬ë‚´ êµ¬ì„±ì›ì„ ìœ„í•œ ì£¼ê°„ ë‰´ìŠ¤ ë¸Œë¦¬í•‘ì…ë‹ˆë‹¤.<br>
            ì™¸ë¶€ë¡œì˜ ë¬´ë‹¨ ì „ì¬ ë° ê³µìœ ëŠ” ì§€ì–‘í•´ ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.<br><br>
            &copy; {now_kst.year} Hancom InSpace. All Rights Reserved.

          </td>
        </tr>
      </table>
    </td>
  </tr>
</table>


<script>
(function () {{
  var layer = document.getElementById('main-bg-layer');
  var video = document.getElementById('main-bg-video');
  var img   = document.getElementById('main-bg-image');
  var wash  = document.getElementById('main-bg-whitewash');

  if (!layer || !wash) return;

  layer.style.display = 'block';

  var FADE_START = {MAIN_BG_FADE_START_PX};
  var FADE_END   = {MAIN_BG_FADE_END_PX};

  var duration = 0;
  var ready = false;

  if (video) {{
    video.addEventListener('loadedmetadata', function () {{
      duration = video.duration || 0;
      ready = true;
    }});
  }}

  function clamp01(x) {{
    return Math.max(0, Math.min(1, x));
  }}

  function getDocScrollMax() {{
    var doc = document.documentElement;
    return Math.max(0, (doc.scrollHeight || 0) - window.innerHeight);
  }}

  function updateFade(scrollY) {{
    var t = clamp01((scrollY - FADE_START) / Math.max(1, (FADE_END - FADE_START)));

    wash.style.opacity = String(1 - t);

    if (video) video.style.opacity = String(t);
    if (img)   img.style.opacity   = String(t);

    return t;
  }}

  var targetTime = 0;
  var currentTime = 0;
  var easing = 0.08;

  function computeTargetTime(scrollY) {{
    if (!video || !ready || !duration) return 0;

    var scrollMax = getDocScrollMax();
    var usable = Math.max(1, scrollMax);   // ì „ì²´ ìŠ¤í¬ë¡¤ êµ¬ê°„ ì‚¬ìš©
    var p = clamp01(scrollY / usable);     // 0ë¶€í„° ë°”ë¡œ ë§¤í•‘

    return p * duration;
  }}


  function tick() {{
    if (video && ready && duration) {{
      currentTime += (targetTime - currentTime) * easing;
      if (isFinite(currentTime)) {{
        try {{ video.currentTime = currentTime; }} catch(e) {{}}
      }}
    }}
    requestAnimationFrame(tick);
  }}

  function onScroll() {{
    var y = window.scrollY || window.pageYOffset || 0;

    // fadeëŠ” ë…ë¦½ì ìœ¼ë¡œ ê³„ì† ê³„ì‚°
    updateFade(y);

    // (NEW) fade ì§„í–‰ ì¤‘ì—ë„ í•­ìƒ íƒ€ì„ë¼ì¸ ëª©í‘œê°’ ê°±ì‹ 
    targetTime = computeTargetTime(y);
  }}


  window.addEventListener('scroll', onScroll, {{ passive: true }});
  window.addEventListener('resize', onScroll);

  onScroll();
  requestAnimationFrame(tick);
}})();
</script>



</body>
</html>
"""


# íŒŒì¼ ì €ì¥
with open("newsletter.html", "w", encoding="utf-8") as f:
    f.write(newsletter_html)

for topic_num, filename in TOPIC_MORE_FILENAMES.items():
    html = more_pages_html.get(topic_num)
    if not html:
        continue
    with open(filename, "w", encoding="utf-8") as f:
        f.write(html)

# ìµœì‹  ì—°êµ¬ë™í–¥ ì¶”ê°€ í•™ìˆ ì§€ í˜ì´ì§€ ì €ì¥
with open("more_research.html", "w", encoding="utf-8") as f:
    f.write(research_more_html)

print("more_research.html ì €ì¥ ì™„ë£Œ")

print("newsletter.html ì €ì¥ ì™„ë£Œ")
for topic_num, filename in TOPIC_MORE_FILENAMES.items():
    if topic_num in more_pages_html:
        print(f"{filename} ì €ì¥ ì™„ë£Œ")

if IN_COLAB:
    display(HTML(newsletter_html))

# ============================================================
# â–¼ GitHub Pages ì—…ë¡œë“œ
# ============================================================

main_repo_path = f"docs/{FOLDER_PATH}/index.html"

commit_msg_main = f"Add newsletter main: {FOLDER_PATH}"
upload_file_to_github(main_repo_path, newsletter_html, commit_msg_main)

# â–¼ ì´ì „ ë‰´ìŠ¤ë ˆí„° ì•„ì¹´ì´ë¸Œ í˜ì´ì§€ ìƒì„±
archive_items = list(NEWSLETTER_ARCHIVE_BASE)

# âœ… ë©”ì¸ í˜ì´ì§€ìš©(1~3ì¤„) insightëŠ” ê·¸ëŒ€ë¡œ ìœ ì§€
weekly_focus_insight_full = weekly_focus_insight

# âœ… ì•„ì¹´ì´ë¸Œ ì¹´ë“œì—ëŠ” 1ì¤„ë¡œ ì¬ìš”ì•½í•´ì„œ ì €ì¥
weekly_focus_insight_card = summarize_insight_for_archive(weekly_focus_insight_full)

today_item = {
    "label": f"{WEEK_LABEL} ë‰´ìŠ¤ë ˆí„°",
    "date_str": NEWSLETTER_DATE,
    "url": MAIN_PAGE_URL,
    # âœ… ì•„ì¹´ì´ë¸Œ ì¹´ë“œì—ì„œ ë°”ë¡œ ì“°ëŠ” ê°’(1ì¤„)
    "insight": weekly_focus_insight_card,
}

# âœ… ê°™ì€ URLì´ ìˆìœ¼ë©´ "ìŠ¤í‚µ"ì´ ì•„ë‹ˆë¼ "ì—…ë°ì´íŠ¸" (ê¸°ì¡´ ì¹´ë“œì— insightë¥¼ ì±„ì›Œ ë„£ê¸°)
found = None
for item in archive_items:
    if item.get("url") == today_item["url"]:
        found = item
        break

if found:
    # ê¸°ì¡´ í•­ëª©ì„ ìµœì‹  ê°’ìœ¼ë¡œ ê°±ì‹  (insight í¬í•¨)
    found.update(today_item)

    # (ì„ íƒ) ìµœì‹  í•­ëª©ì´ ë¦¬ìŠ¤íŠ¸ ë§¨ ì•ì— ì˜¤ë„ë¡ ì •ë ¬ ìœ ì§€
    archive_items = [found] + [x for x in archive_items if x is not found]
else:
    archive_items.insert(0, today_item)

# archive.html ìƒì„±
archive_html = build_archive_page_html(archive_items)

# ë¡œì»¬ ì €ì¥ (HTML)
with open("archive.html", "w", encoding="utf-8") as f:
    f.write(archive_html)

# GitHub ì—…ë¡œë“œ (HTML)
upload_file_to_github(
    ARCHIVE_PAGE_PATH,
    archive_html,
    f"Update archive page: {NEWSLETTER_DATE}"
)

# JSON ë¬¸ìì—´ë¡œ ì§ë ¬í™”
archive_json_str = json.dumps(archive_items, ensure_ascii=False, indent=2)

# ë¡œì»¬ ì €ì¥ (JSON)
with open("archive.json", "w", encoding="utf-8") as f:
    f.write(archive_json_str)

# GitHub ì—…ë¡œë“œ (JSON)
upload_file_to_github(
    ARCHIVE_JSON_PATH,
    archive_json_str,
    f"Update archive data: {NEWSLETTER_DATE}"
)



# í† í”½ë³„ ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ ì—…ë¡œë“œ
for topic_num, filename in TOPIC_MORE_FILENAMES.items():
    html = more_pages_html.get(topic_num)
    if not html:
        continue

    more_repo_path = f"docs/{FOLDER_PATH}/{filename}"
    commit_msg_more = f"Add newsletter more (topic{topic_num}): {FOLDER_PATH}"
    upload_file_to_github(more_repo_path, html, commit_msg_more)

# ğŸ’¡ ì—°êµ¬ë™í–¥ more í˜ì´ì§€ ì—…ë¡œë“œ
if research_more_html:
    research_repo_path = f"docs/{FOLDER_PATH}/{RESEARCH_MORE_FILENAME}"
    commit_msg_research = f"Add newsletter research more: {FOLDER_PATH}"
    upload_file_to_github(research_repo_path, research_more_html, commit_msg_research)


print("GitHub Pages ì—…ë¡œë“œ ìš”ì²­ ì™„ë£Œ")
print("ë©”ì¸ í˜ì´ì§€ URL:", MAIN_PAGE_URL)
for topic_num, url in TOPIC_MORE_URLS.items():
    if topic_num in more_pages_html:
        print(f"ì¶”ê°€ ê¸°ì‚¬ í˜ì´ì§€ URL (topic{topic_num}):", url)


# # **09 ì´ë©”ì¼ ìë™ ë°œì†¡**
# ### **(Colabì—ì„œ ì‹¤í–‰í•˜ë©´ í…ŒìŠ¤íŠ¸ ì´ë©”ì¼ë¡œ, Github ì‹¤í–‰ ì‹œ, ì‹¤ì œ ìˆ˜ì‹ ìì—ê²Œ)**

# In[68]:


SEND_EMAIL = os.environ.get("SEND_EMAIL", "true").lower() == "true"

GMAIL_USER = os.environ.get("GMAIL_USER")
GMAIL_APP_PASSWORD = os.environ.get("GMAIL_APP_PASSWORD")
TO_EMAIL = None  # ê¸°ë³¸ê°’

if SEND_EMAIL:
    # ğŸ”¹ Colabì—ì„œëŠ” í…ŒìŠ¤íŠ¸ ë©”ì¼ë¡œë§Œ ì „ì†¡
    if IN_COLAB:
        TO_EMAIL = os.environ.get("TO_EMAIL_TEST")
        if not TO_EMAIL:
            print("[ë©”ì¼ ì „ì†¡ X] Colab í…ŒìŠ¤íŠ¸ìš© ì´ë©”ì¼(TO_EMAIL_TEST)ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì¼ì€ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            SEND_EMAIL = False  # ë©”ì¼ ì „ì†¡ ê°•ì œ OFF
        else:
            print(f"[ë©”ì¼ ì „ì†¡] Colab í…ŒìŠ¤íŠ¸ ëª¨ë“œ â†’ {TO_EMAIL}")
    else:
        # ğŸ”¹ GitHub Actions / ë¡œì»¬ì—ì„œëŠ” ì‹¤ì œ ìˆ˜ì‹ ì
        TO_EMAIL = os.environ.get("TO_EMAIL")
        if not TO_EMAIL:
            print("[ë©”ì¼ ì „ì†¡ X] í”„ë¡œë•ì…˜ ì´ë©”ì¼(TO_EMAIL)ì´ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ë©”ì¼ì€ ì „ì†¡í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.")
            SEND_EMAIL = False
        else:
            print(f"[ë©”ì¼ ì „ì†¡] í”„ë¡œë•ì…˜ ëª¨ë“œ â†’ {TO_EMAIL}")

# ğŸ”¹ ì‹¤ì œ ë©”ì¼ ì „ì†¡ì€ ì—¬ê¸°ì„œ ìµœì¢…ì ìœ¼ë¡œ í•œ ë²ˆ ë” ì²´í¬
if SEND_EMAIL and TO_EMAIL:
    SUBJECT = f"í•œì»´ì¸ìŠ¤í˜ì´ìŠ¤ {WEEK_LABEL} ë‰´ìŠ¤ë ˆí„° | {NEWSLETTER_DATE}"

    with open("newsletter.html", "r", encoding="utf-8") as f:
        html_content = f.read()

    msg = MIMEMultipart("alternative")
    msg["From"] = GMAIL_USER
    msg["To"] = TO_EMAIL
    msg["Subject"] = SUBJECT
    msg.attach(MIMEText(html_content, "html", _charset="utf-8"))

    with smtplib.SMTP_SSL("smtp.gmail.com", 465) as server:
        server.login(GMAIL_USER, GMAIL_APP_PASSWORD)
        server.send_message(msg)

    print("ë©”ì¼ ë³´ëƒˆìŠµë‹ˆë‹¤!")
elif not SEND_EMAIL:
    print("SEND_EMAIL=False ìƒíƒœì´ê±°ë‚˜, ìœ„ ì¡°ê±´ì— ì˜í•´ ë©”ì¼ ì „ì†¡ì´ ë¹„í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.")
else:
    print("TO_EMAILì´ ì„¤ì •ë˜ì§€ ì•Šì•„ ë©”ì¼ì„ ë³´ë‚´ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")


# # **10. ìµœì¢… í†µê³„ ì¶œë ¥**

# In[69]:


# ============================
# 10. ìµœì¢… í†µê³„ ì¶œë ¥
# ============================
print("\n" + "="*70)
print("ğŸ“Š ë‰´ìŠ¤ë ˆí„° ìƒì„± ìµœì¢… í†µê³„")
print("="*70)

# ============================
# 1. OpenAI í† í° ì‚¬ìš©ëŸ‰
# ============================
print("\nğŸ’° OpenAI API ì‚¬ìš©ëŸ‰:")
print("-" * 50)

try:
    # ì‹¤ì œ ì‚¬ìš©í•œ ê¸°ì‚¬ ìˆ˜ ê¸°ì¤€ ì¶”ì •
    total_articles = len(df_final) if 'df_final' in globals() else 0
    research_articles = len(research_processed_articles) if 'research_processed_articles' in globals() else 0

    # GPT-4.1-mini í‰ê·  í† í° ì‚¬ìš©ëŸ‰ ì¶”ì •
    # - ì¼ë°˜ ê¸°ì‚¬ ìš”ì•½: ì•½ 800 í† í°/ê¸°ì‚¬ (ì…ë ¥ 500 + ì¶œë ¥ 300)
    # - ì—°êµ¬ë…¼ë¬¸ ìš”ì•½: ì•½ 1200 í† í°/ë…¼ë¬¸ (ì…ë ¥ 800 + ì¶œë ¥ 400)

    estimated_article_tokens = total_articles * 800
    estimated_research_tokens = research_articles * 1200
    estimated_total_tokens = estimated_article_tokens + estimated_research_tokens

    print(f"  â€¢ ì¼ë°˜ ê¸°ì‚¬ ì²˜ë¦¬: ì•½ {total_articles}ê±´ Ã— 800 í† í° = {estimated_article_tokens:,} í† í°")
    print(f"  â€¢ ì—°êµ¬ë…¼ë¬¸ ì²˜ë¦¬: ì•½ {research_articles}ê±´ Ã— 1,200 í† í° = {estimated_research_tokens:,} í† í°")
    print(f"  â€¢ ì´ ì˜ˆìƒ í† í°: ì•½ {estimated_total_tokens:,} í† í°")

    # ë¹„ìš© ê³„ì‚° (GPT-4.1-mini ê¸°ì¤€: $0.15/1M input tokens, $0.60/1M output tokens)
    # í‰ê· ì ìœ¼ë¡œ input:output = 65:35 ë¹„ìœ¨ë¡œ ê°€ì •
    input_tokens = int(estimated_total_tokens * 0.65)
    output_tokens = int(estimated_total_tokens * 0.35)

    input_cost = (input_tokens / 1_000_000) * 0.15
    output_cost = (output_tokens / 1_000_000) * 0.60
    total_cost = input_cost + output_cost

    print(f"\n  ğŸ’µ ì˜ˆìƒ ë¹„ìš©:")
    print(f"     - Input í† í°: {input_tokens:,} Ã— $0.15/1M = ${input_cost:.4f}")
    print(f"     - Output í† í°: {output_tokens:,} Ã— $0.60/1M = ${output_cost:.4f}")
    print(f"     - ì´ ë¹„ìš©: ${total_cost:.4f} (ì•½ â‚©{int(total_cost * 1300):,}ì›)")

except Exception as e:
    print(f"  âš ï¸ í† í° ì‚¬ìš©ëŸ‰ ê³„ì‚° ì‹¤íŒ¨: {e}")
    total_cost = 0

# ============================
# 2. ë©”ì¸ vs ì¶”ê°€ ê¸°ì‚¬ ë¶„í¬
# ============================
print("\nğŸ“‘ ë©”ì¸ vs ì¶”ê°€ ê¸°ì‚¬:")
print("-" * 50)

if 'topic_main_articles' in globals() and 'topic_extra_articles' in globals():
    for topic_num in [4, 3, 2, 1]:  # ì—­ìˆœìœ¼ë¡œ ì¶œë ¥
        main_count = len(topic_main_articles.get(topic_num, []))
        extra_count = len(topic_extra_articles.get(topic_num, []))

        topic_icon = TOPIC_ICON.get(topic_num, "")
        print(f"{topic_icon} í† í”½ {topic_num}: ë©”ì¸ {main_count}ê°œ | ì¶”ê°€ {extra_count}ê°œ")

# ============================
# 3. í† í”½ë³„ í•œê¸€/ì˜ë¬¸ ê¸°ì‚¬ ìˆ˜  (âœ… ìµœì¢… ì„ íƒ ê²°ê³¼ ê¸°ì¤€ìœ¼ë¡œ ìˆ˜ì •)
# ============================
print("\nğŸ“° í† í”½ë³„ ê¸°ì‚¬ ë¶„í¬:")
print("-" * 50)

total_korean_count = 0
total_english_count = 0
total_news = 0

if 'topic_main_articles' in globals() and 'topic_extra_articles' in globals():
    for topic_num in [4, 3, 2, 1]:  # ì—­ìˆœìœ¼ë¡œ ì¶œë ¥
        final_articles = (topic_main_articles.get(topic_num, []) or []) + (topic_extra_articles.get(topic_num, []) or [])

        if not final_articles:
            continue

        ko_count, en_count, ko_pct, en_pct = calculate_language_ratio(final_articles)
        total_korean_count += ko_count
        total_english_count += en_count
        total_news += (ko_count + en_count)

        topic_icon = TOPIC_ICON.get(topic_num, "")
        topic_desc = TOPIC_DESC.get(topic_num, "")

        print(f"\n{topic_icon} í† í”½ {topic_num}: {topic_desc}")
        print(f"  â”œâ”€ ğŸ‡°ğŸ‡· í•œê¸€ ê¸°ì‚¬: {ko_count}ê°œ ({ko_pct:.1f}%)")
        print(f"  â”œâ”€ ğŸ‡ºğŸ‡¸ ì˜ë¬¸ ê¸°ì‚¬: {en_count}ê°œ ({en_pct:.1f}%)")
        print(f"  â””â”€ ğŸ“Š ì´ ê¸°ì‚¬: {ko_count + en_count}ê°œ")

else:
    # fallback: topic_main/extraê°€ ì—†ìœ¼ë©´ df_final ê¸°ì¤€(ê¸°ì¡´ ë™ì‘)ìœ¼ë¡œ ìœ ì§€
    if 'df_final' in globals() and not df_final.empty:
        for topic_num in [4, 3, 2, 1]:
            topic_articles = df_final[df_final["topic_final"] == topic_num].to_dict("records")
            if not topic_articles:
                continue

            ko_count, en_count, ko_pct, en_pct = calculate_language_ratio(topic_articles)
            total_korean_count += ko_count
            total_english_count += en_count
            total_news += (ko_count + en_count)

            topic_icon = TOPIC_ICON.get(topic_num, "")
            topic_desc = TOPIC_DESC.get(topic_num, "")

            print(f"\n{topic_icon} í† í”½ {topic_num}: {topic_desc}")
            print(f"  â”œâ”€ ğŸ‡°ğŸ‡· í•œê¸€ ê¸°ì‚¬: {ko_count}ê°œ ({ko_pct:.1f}%)")
            print(f"  â”œâ”€ ğŸ‡ºğŸ‡¸ ì˜ë¬¸ ê¸°ì‚¬: {en_count}ê°œ ({en_pct:.1f}%)")
            print(f"  â””â”€ ğŸ“Š ì´ ê¸°ì‚¬: {ko_count + en_count}ê°œ")


# ============================
# 4. ì—°êµ¬ë™í–¥ í†µê³„
# ============================
print("\nğŸ”¬ ì—°êµ¬ë™í–¥ ë…¼ë¬¸:")
print("-" * 50)

if 'research_processed_articles' in globals():
    print(f"  â€¢ ìˆ˜ì§‘ëœ ë…¼ë¬¸ ìˆ˜: {len(research_processed_articles)}ê°œ")

    # ì €ë„ë³„ ë¶„í¬
    if research_processed_articles:
        from collections import Counter
        journal_counts = Counter(
            article.get('journal_name', 'Unknown')
            for article in research_processed_articles
        )

        print(f"  â€¢ ì €ë„ë³„ ë¶„í¬:")
        for journal, count in journal_counts.most_common():
            print(f"     - {journal}: {count}ê°œ")

# ============================
# 5. ìµœì¢… ìš”ì•½ (ìˆ˜ì •ë¨)
# ============================
print()  # ğŸ”¥ ë¹ˆ ì¤„ ì¶”ê°€
print()  # ğŸ”¥ ë¹ˆ ì¤„ ì¶”ê°€
print()  # ğŸ”¥ ë¹ˆ ì¤„ ì¶”ê°€

print("="*70)

total_news = total_news if 'total_news' in globals() else (len(df_final) if 'df_final' in globals() else 0)
total_research = len(research_processed_articles) if 'research_processed_articles' in globals() else 0

# ğŸ”¥ ìˆ˜ì •: ì¼ë°˜ ë‰´ìŠ¤ì— í•œê¸€/ì˜ë¬¸ ê°œìˆ˜ í‘œì‹œ
print(f"ğŸ“° ì¼ë°˜ ë‰´ìŠ¤: {total_news}ê°œ (ì˜ë¬¸: {total_english_count}ê°œ, í•œê¸€: {total_korean_count}ê°œ)")
print(f"ğŸ”¬ ì—°êµ¬ë™í–¥: {total_research}ê°œ")
print(f"ğŸ’° ì˜ˆìƒ ë¹„ìš©: ${total_cost:.4f} (ì•½ â‚©{int(total_cost * 1300):,}ì›)" if total_cost > 0 else "ğŸ’° ì˜ˆìƒ ë¹„ìš©: ê³„ì‚° ë¶ˆê°€")
print("="*70)
print("âœ… ë‰´ìŠ¤ë ˆí„° ìƒì„± ì™„ë£Œ!")
print("="*70 + "\n")


# # **11. Colabì—ì„œë§Œ ì‹¤í–‰: í˜„ì¬ ìŠ¤í¬ë¦½íŠ¸ë¥¼ GitHubì— ìë™ ì—…ë¡œë“œ**

# In[ ]:


# ============================================================
# Colab ì „ìš©: í˜„ì¬ ë…¸íŠ¸ë¶ ì „ì²´ë¥¼ .pyë¡œ ë³€í™˜í•´ GitHubì˜ newsletter.pyë¥¼ êµì²´
# + (ì¶”ê°€) newsletter_history/ í´ë”ì— íƒ€ì„ìŠ¤íƒ¬í”„ íŒŒì¼ë¡œ ìŠ¤ëƒ…ìƒ· ì €ì¥
# ============================================================

def _in_colab() -> bool:
    try:
        import google.colab  # noqa: F401
        return True
    except Exception:
        return False

if _in_colab():
    import os
    import json
    import base64
    import requests

    # 1) GitHub ì„¤ì • (í•„ìˆ˜)
    GITHUB_OWNER = os.environ.get("GITHUB_OWNER", "HANCOM-InSpace")
    GITHUB_REPO  = os.environ.get("GITHUB_REPO", "Weekly-Newsletter")
    BRANCH       = os.environ.get("GITHUB_BRANCH", "main")
    TARGET_PATH  = "newsletter.py"

    # Colab Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ í† í° ì œê³µ (ë‘˜ ì¤‘ í•˜ë‚˜)
    GITHUB_TOKEN = os.environ.get("GITHUB_TOKEN")
    if not GITHUB_TOKEN:
        try:
            from google.colab import userdata
            GITHUB_TOKEN = userdata.get("GITHUB_TOKEN")
        except Exception:
            GITHUB_TOKEN = None

    if not GITHUB_TOKEN:
        raise RuntimeError("GITHUB_TOKENì´ ì—†ìŠµë‹ˆë‹¤. Colab Secrets ë˜ëŠ” í™˜ê²½ë³€ìˆ˜ë¡œ ì„¤ì •í•˜ì„¸ìš”.")

    # 2) í˜„ì¬ ë…¸íŠ¸ë¶(.ipynb)ì„ 'íŒŒì¼ ì—†ì´' ì§ì ‘ ê°€ì ¸ì˜¤ê¸° (Colab API)
    from google.colab import _message
    nb = _message.blocking_request("get_ipynb", timeout_sec=30)
    nb_json = nb["ipynb"]

    # 3) ë…¸íŠ¸ë¶ JSON -> Python(.py) ë³€í™˜ (nbconvert)
    import nbformat
    from nbconvert.exporters import PythonExporter

    nb_node = nbformat.reads(json.dumps(nb_json), as_version=4)
    exporter = PythonExporter()
    py_text, _ = exporter.from_notebook_node(nb_node)

    # 4) GitHub APIë¡œ ê¸°ì¡´ newsletter.py SHA ì¡°íšŒ í›„ ì—…ë°ì´íŠ¸(PUT)
    api_url = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{TARGET_PATH}"
    headers = {
        "Authorization": f"Bearer {GITHUB_TOKEN}",
        "Accept": "application/vnd.github+json",
    }

    # ê¸°ì¡´ íŒŒì¼ ì¡°íšŒ(sha í•„ìš”)
    r = requests.get(api_url, headers=headers, params={"ref": BRANCH})
    if r.status_code == 200:
        sha = r.json()["sha"]
    elif r.status_code == 404:
        sha = None
    else:
        raise RuntimeError(f"GitHub GET ì‹¤íŒ¨: {r.status_code} / {r.text}")

    content_b64 = base64.b64encode(py_text.encode("utf-8")).decode("utf-8")

    # ì»¤ë°‹ ë©”ì‹œì§€ (ì›í•˜ë©´ ìˆ˜ì •)
    commit_msg = "Update newsletter.py from Colab notebook"

    payload = {
        "message": commit_msg,
        "content": content_b64,
        "branch": BRANCH,
    }
    if sha:
        payload["sha"] = sha

    u = requests.put(api_url, headers=headers, json=payload)
    if u.status_code not in (200, 201):
        raise RuntimeError(f"GitHub PUT ì‹¤íŒ¨: {u.status_code} / {u.text}")

    print(f"âœ… GitHubì— {TARGET_PATH} ì—…ë°ì´íŠ¸ ì™„ë£Œ: {GITHUB_OWNER}/{GITHUB_REPO}@{BRANCH}")

    # ============================================================
    # (ì¶”ê°€) íˆìŠ¤í† ë¦¬ íŒŒì¼ ì €ì¥:
    # newsletter_history/newsletter_YYYY_MM_DD_HH_MM.py
    # ============================================================
    HISTORY_DIR = "newsletter_history"

    # KST íƒ€ì„ìŠ¤íƒ¬í”„ ìƒì„± (ê°™ì€ ë¶„ì— 2ë²ˆ ì €ì¥í•˜ë©´ ì¶©ëŒí•  ìˆ˜ ìˆì–´ ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    from datetime import datetime, timezone, timedelta
    KST = timezone(timedelta(hours=9))
    ts = datetime.now(KST).strftime("%Y_%m_%d_%H_%M")

    history_path = f"{HISTORY_DIR}/newsletter_{ts}.py"
    api_url_hist = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{history_path}"

    payload_hist = {
        "message": f"Archive snapshot: {history_path}",
        "content": content_b64,
        "branch": BRANCH,
    }

    resp = requests.put(api_url_hist, headers=headers, json=payload_hist)

    if resp.status_code in (200, 201):
        print(f"âœ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ: {history_path}")
    else:
        # ê°™ì€ ë¶„ì— ì¤‘ë³µ ì €ì¥ ë“±ìœ¼ë¡œ ì‹¤íŒ¨ ì‹œ -> ì´ˆê¹Œì§€ ë¶™ì—¬ ì¬ì‹œë„
        ts2 = datetime.now(KST).strftime("%Y_%m_%d_%H_%M_%S")
        history_path2 = f"{HISTORY_DIR}/newsletter_{ts2}.py"
        api_url_hist2 = f"https://api.github.com/repos/{GITHUB_OWNER}/{GITHUB_REPO}/contents/{history_path2}"

        payload_hist["message"] = f"Archive snapshot: {history_path2}"
        resp2 = requests.put(api_url_hist2, headers=headers, json=payload_hist)

        if resp2.status_code in (200, 201):
            print(f"âœ… íˆìŠ¤í† ë¦¬ ì €ì¥ ì™„ë£Œ(ì´ˆ í¬í•¨): {history_path2}")
        else:
            raise RuntimeError(f"íˆìŠ¤í† ë¦¬ ì €ì¥ ì‹¤íŒ¨: {resp2.status_code} / {resp2.text}")

else:
    # Colabì´ ì•„ë‹Œ í™˜ê²½(ì˜ˆ: GitHub Actions)ì—ì„œëŠ” ì•„ë¬´ ê²ƒë„ í•˜ì§€ ì•ŠìŒ
    pass

